http://ieeexplore.ieee.org/search/searchresult.jsp?bulkSetSize=2000&sortType%3Ddesc_p_Publication_Year%26queryText%3DHigh+Performance+Computing%2C+Networking%2C+Storage+And+Analysis+.LB.SC.RB.,4/24/17 16:45,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document Title,Authors,Author Affiliations,Publication Title,Date Added To Xplore,Year,Volume,Issue,Start Page,End Page,Abstract,ISSN,ISBNs,DOI,PDF Link,Author Keywords,IEEE Terms,INSPEC Controlled Terms,INSPEC Non-Controlled Terms,MeSH Terms,Article Citation Count,Patent Citation Count,Reference Count,Copyright Year,Online Date,Issue Date,Meeting Date,Publisher,Document Identifier
Exploring the Potentials of Parallel Garbage Collection in SSDs for Enterprise Storage Systems,N. Shahidi; M. T. Kandemir; M. Arjomand; C. R. Das; M. Jung; A. Sivasubramaniam,"Pennsylvania State Univ., University Park, PA, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,561,572,"In the last decade, NAND flash-based SSDs have been widely adopted for high-end enterprise systems in an attempt to provide a high-performance and reliable storage. However, inferior performance is frequently attained mainly due to the need for Garbage Collection (GC). GC in flash memory is the process of identifying and clearing the blocks of unneeded data to create space for the new data to be allocated. GC is a high-latency operation and once it is scheduled for service to a block of a plane in a flash chip (each flash chip consists of multiple planes), it can increase latency for later arriving I/O requests to the same plane. Apart from that, the consequent high latency also keep other planes of the same chip, that are not involved in this GC, idle for a long time. We show that for the baseline SSD with modern FTL, GC considerably reduces the plane-level parallelism, causing significant performance degradation. There are several circuit-level constraints that make it difficult to allow subsequent I/O operations and/or GCs to be served concurrently from the same chip, but different planes, during the long latency GC. This paper proposes a novel GC strategy, called Parallel GC (PaGC), whose goal is to proactively run GC on the remaining planes of a flash chip whenever any of its planes needs to execute on-demand GC. The resulting PaGC system boosts the response time of I/O requests by up to 45% (32% on average) for different GC settings and across a wide spectrum of enterprise I/O workloads.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.47,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877126,,Computer architecture;Optimization;Organizations;Parallel processing;Quality of service;Reliability;Time factors,business data processing;flash memories;parallel processing;storage management,I/O operations;I/O requests;PaGC;SSD;circuit-level constraints;enterprise I/O workloads;enterprise storage systems;flash chip;flash memory;high-latency operation;on-demand GC;parallel GC;parallel garbage collection;plane-level parallelism;solid-state disk,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Caliper: Performance Introspection for HPC Software Stacks,D. Boehme; T. Gamblin; D. Beckingsale; P. T. Bremer; A. Gimenez; M. LeGendre; O. Pearce; M. Schulz,"Lawrence Livermore Nat. Lab., Livermore, CA, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,550,560,"Many performance engineering tasks, from long-term performance monitoring to post-mortem analysis and online tuning, require efficient runtime methods for introspection and performance data collection. To understand interactions between components in increasingly modular HPC software, performance introspection hooks must be integrated into runtime systems, libraries, and application codes across the software stack. This requires an interoperable, cross-stack, general-purpose approach to performance data collection, which neither application-specific performance measurement nor traditional profile or trace analysis tools provide. With Caliper, we have developed a general abstraction layer to provide performance data collection as a service to applications, runtime systems, libraries, and tools. Individual software components connect to Caliper in independent data producer, data consumer, and measurement control roles, which allows them to share performance data across software stack boundaries. We demonstrate Caliper's performance analysis capbilities with two case studies of production scenarios.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.46,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877125,Computer performance;High performance computing;Parallel processing;Performance analysis;Software performance;Software reusability;Software tools,Context;Data models;Instruments;Libraries;Runtime;Semantics;Software,data acquisition;open systems;parallel processing;software libraries;software performance evaluation;software tools,Caliper performance analysis capbilities;HPC software stacks;application codes;data consumer;data producer;general abstraction layer;interoperable cross-stack general- purpose approach;long-term performance monitoring;measurement control roles;online tuning;performance data collection;performance engineering;performance introspection hooks;post-mortem analysis;runtime systems;software components;software consumer;software libraries;software stack;software stack boundaries;software tools,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Enabling Efficient Preemption for SIMT Architectures with Lightweight Context Switching,Z. Lin; L. Nyland; H. Zhou,"Dept. of Electr. & Comput. Eng., North Carolina State Univ., Raleigh, NC, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,898,908,"Context switching is a key technique enabling preemption and time-multiplexing for CPUs. However, for single-instruction multiple-thread (SIMT) processors such as high-end graphics processing units (GPUs), it is challenging to support context switching due to the massive number of threads, which leads to a huge amount of architectural states to be swapped during context switching. The architectural state of SIMT processors includes registers, shared memory, SIMT stacks and barrier states. Recent works present thread-block-level preemption on SIMT processors to avoid context switching overhead. However, because the execution time of a thread block (TB) is highly dependent on the kernel program. The response time of preemption cannot be guaranteed and some TB-level preemption techniques cannot be applied to all kernel functions. In this paper, we propose three complementary ways to reduce and compress the architectural states to achieve lightweight context switching on SIMT processors. Experiments show that our approaches can reduce the register context size by 91.5% on average. Based on lightweight context switching, we enable instruction-level preemption on SIMT processors with compiler and hardware co-design. With our proposed schemes, the preemption latency is reduced by 59.7% on average compared to the naive approach.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.76,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877155,,Context;Instruction sets;Kernel;Registers;Resource management;Switches,computer architecture;flip-flops;graphics processing units;microprocessor chips;operating system kernels;parallel processing;program compilers;shared memory systems;switching circuits,CPU preemption;CPU time-multiplexing;GPU;SIMT architecture efficient preemption;SIMT processor architectural states;SIMT stacks;TB level preemption;barrier states;compiler;hardware codesign;high-end graphics processing units;instruction-level preemption;kernel program;lightweight context switching;preemption response time;registers;shared memory;single-instruction multiple-thread processors;thread-block-level preemption,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
A Machine Learning Framework for Performance Coverage Analysis of Proxy Applications,T. Z. Islam; J. J. Thiagarajan; A. Bhatele; M. Schulz; T. Gamblin,"Center for Appl. Sci. Comput., Lawrence Livermore Nat. Lab., Livermore, CA, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,538,549,"Proxy applications are written to represent subsets of performance behaviors of larger, and more complex applications that often have distribution restrictions. They enable easy evaluation of these behaviors across systems, e.g., for procurement or co-design purposes. However, the intended correlation between the performance behaviors of proxy applications and their parent codes is often based solely on the developer's intuition. In this paper, we present novel machine learning techniques to methodically quantify the coverage of performance behaviors of parent codes by their proxy applications. We have developed a framework, VERITAS, to answer these questions in the context of on-node performance: (a) which hardware resources are covered by a proxy application and how well, and (b) which resources are important, but not covered. We present our techniques in the context of two benchmarks, STREAM and DGEMM, and two production applications, OpenMC and CMTnek, and their respective proxy applications.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.45,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877124,Machine learning;Performance analysis;Scalability;Unsupervised learning,Computer architecture;Correlation;Hardware;Loss measurement;Production;Radiation detectors,Monte Carlo methods;learning (artificial intelligence);parallel processing,CMTnek production application;DGEMM benchmark;OpenMC production application;STREAM benchmark;VERITAS;machine learning;performance coverage analysis;proxy applications,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
DCA: A DRAM-cache-Aware DRAM Controller,C. C. Huang; V. Nagarajan; A. Joshi,"Inst. of Comput. Syst. Archit., Univ. of Edinburgh, Edinburgh, UK","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,887,897,"3D-stacking technology has enabled the option of embedding a large DRAM cache onto the processor. Since the DRAM cache can be orders of magnitude larger than a conventional SRAM cache, the size of its cache tags can also be large. Recent works have proposed storing these tags in the stacked DRAM array itself. However, this increases the complexity of a DRAM cache request, which now translates into multiple DRAM cache accesses (tag/data). In this work, we address how to schedule these DRAM cache accesses. We start by exploring whether or not a conventional DRAM controller will work well. We introduce two potential baseline designs and study their limitations. We then derive a set of design principles that a DRAM cache controller must ideally satisfy. Our DRAM-cache-aware (DCA) DRAM controller, that is based on these principles, consistently improves performance over various DRAM cache organizations.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.75,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877154,,Arrays;Complexity theory;Delays;Organizations;Random access memory;Schedules;System performance,DRAM chips;cache storage,3D-stacking technology;DCA;DRAM cache accesses;DRAM cache organizations;DRAM cache request complexity;DRAM-cache-aware DRAM controller;cache tags;design principles;stacked DRAM array,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
MUSA: A Multi-level Simulation Approach for Next-Generation HPC Machines,T. Grass; C. Allande; A. Armejach; A. Rico; E. AyguadÌ©; J. Labarta; M. Valero; M. Casas; M. Moreto,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,526,537,"The complexity of High Performance Computing (HPC) systems is increasing in the number of components and their heterogeneity. Interactions between software and hardware involve many different aspects which are typically not transparent to scientific programmers and system architects. Therefore, predicting the behavior of current scientific applications on future HPC infrastructures is a challenging task. In this paper we present MUSA, an end-to-end methodology that employs a multi-level simulation infrastructure. By combining different levels of abstraction, MUSA is able to model the communication network, microarchitectural details and system software interactions, providing different trade-offs in terms of simulation cost and accuracy. We compare detailed MUSA simulations with native executions of up to 2,048 cores and find relative errors that are within 10% in the common case. In addition, we use MUSA to simulate up to 16,384 cores and successfully identify scalability bottlenecks due to different factors, e.g. memory contention or load imbalance. We also compare different system configurations, showing how MUSA can help system designers to assess the usefulness of future technologies in next-generation HPC machines.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.44,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877123,,Computational modeling;Microarchitecture;Next generation networking;Parallel processing;Programming;Runtime;System software,digital simulation;parallel architectures;parallel machines,MUSA;communication network;high performance computing;microarchitectural details;multilevel simulation approach;next-generation HPC machines;system software interactions,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Improving Application Resilience to Memory Errors with Lightweight Compression,S. Levy; K. B. Ferreira; P. G. Bridges,"Dept. of Comput. Sci., Univ. of New Mexico, Albuquerque, NM, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,323,334,"In next-generation extreme-scale systems, application performance will be limited by memory performance characteristics. The first exascale system is projected to contain many petabytes of memory. In addition to the sheer volume of the memory required, device trends, such as shrinking feature sizes and reduced supply voltages, have the potential to increase the frequency of memory errors. As a result, resilience to memory errors is a key challenge. In this paper, we evaluate the viability of using memory compression to repair detectable uncorrectable errors (DUEs) in memory. We develop a software library, evaluate its performance and demonstrate that it is able to significantly compress memory of HPC applications. Further, we show that exploiting compressed memory pages to correct memory errors can significantly improve application performance on next-generation systems.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.27,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877106,,Hardware;Memory management;Next generation networking;Performance evaluation;Reliability;Runtime;Scalability,parallel processing;storage management,DUE;HPC applications;application resilience;detectable uncorrectable errors;high performance computing;lightweight compression;memory compression;memory errors;memory performance;next-generation extreme-scale systems;reduced supply voltages;sheer volume,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Elastic Multi-resource Fairness: Balancing Fairness and Efficiency in Coupled CPU-GPU Architectures,S. Tang; B. He; S. Zhang; Z. Niu,"Sch. of Comput. Sci. & Technol., Tianjin Univ., Tianjin, China","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,875,886,"Fairness and efficiency are two important concerns for users in a shared computer system, and there tends to be a tradeoff between them. Heterogeneous computing poses new challenging issues on the fair allocation of computational resources among users due to the availability of different kinds of computing devices (e.g., CPU and GPU). Prior work either considers the fair resource allocation separately for each computing device or is unable to balance flexibly the tradeoff between the fairness and system utilization. In this work, we consider an emerging heterogeneous computing system with coupled CPU and GPU into a single chip. We first show that it is essential to have a new fair policy for coupled CPU-GPU architectures that is capable of considering both the CPU and the GPU as a whole in fair resource allocation and being aware of the system utilization maximization. We then propose a fair policy called Elastic Multi-Resource Fairness (EMRF) for coupled CPU-GPU architectures, by modeling CPU and GPU as two resource types and viewing the resource fairness problem as a multi-resource fairness problem. It extends DRF by adding a knob that allows users to tune and balance fairness and performance flexibly, and considers the fair allocation of computational resources as a whole for CPU and GPU devices. We show that EMRF satisfies fairness properties of sharing incentive, envy-freeness and pareto efficiency. Finally, we evaluate EMRF using real experiments, and the results show that EMRF can achieve better performance and fairness.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.74,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877153,APU;Coupled CPU-GPU Architecture;EMRF;Fairness;Performance,Computational modeling;Computer architecture;Graphics processing units;Performance evaluation;Programming;Quality of service;Resource management,graphics processing units;microprocessor chips;resource allocation,DRF;EMRF;Pareto efficiency property;coupled CPU-GPU architectures;elastic multiresource fairness;envy-freeness property;heterogeneous computing system;incentive sharing property;multiresource fairness problem;resource allocation;system utilization maximization,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
A Parallel Arbitrary-Order Accurate AMR Algorithm for the Scalar Advection-Diffusion Equation,A. Bakhtiari; D. Malhotra; A. Raoofy; M. Mehl; H. J. Bungartz; G. Biros,"Tech. Univ. of Munich, Munich, Germany","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,514,525,"We present a numerical method for solving the scalar advection-diffusion equation using adaptive mesh refinement. Our solver has three unique characteristics: (1) it supports arbitrary-order accuracy in space; (2) it allows different discretizations for the velocity and scalar advected quantity; (3) it combines the method of characteristics with an integral equation formulation; and (4) it supports shared and distributed memory architectures. In particular, our solver is based on a second-order accurate, unconditionally stable, semi-Lagrangian scheme combined with a spatially-adaptive Chebyshev octree for discretization. We study the convergence, single-node performance, strong scaling, and weak scaling of our scheme for several challenging flows that cannot be resolved efficiently without using high-order accurate discretizations. For example, we consider problems for which switching from 4th order to 14th order approximation results in two orders of magnitude speedups for a computation in which we keep the target accuracy in the solution fixed. For our largest run, we solve a problem with one billion unknowns on a tree with maximum depth equal to 10 and using 14th-order elements on 16,384 x86 cores on the ‰ÛÏSTAMPEDE‰ÛÏ system at the Texas Advanced Computing Center.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.43,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877122,,Boundary conditions;Chebyshev approximation;Convergence;Integral equations;Interpolation;Method of moments;Octrees,approximation theory;computational fluid dynamics;convergence of numerical methods;diffusion;integral equations;mesh generation;parallel algorithms,14th order approximation;STAMPEDE system;Texas Advanced Computing Center;adaptive mesh refinement;arbitrary-order accuracy;convergence;distributed memory architectures;high-order accurate discretizations;integral equation formulation;orders of magnitude;parallel arbitrary-order accurate AMR algorithm;scalar advected quantity;scalar advection-diffusion equation;second-order accurate scheme;semiLagrangian scheme;shared memory architectures;single-node performance;spatially-adaptive Chebyshev octree;strong scaling;unconditionally stable scheme;velocity field;weak scaling,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Failure Detection and Propagation in HPC systems,G. Bosilca; A. Bouteiller; A. Guermouche; T. Herault; Y. Robert; P. Sens; J. Dongarra,"ICL, Univ. of Tennessee, Knoxville, TN, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,312,322,"Building an infrastructure for Exascale applications requires, in addition to many other key components, a stable and efficient failure detector. This paper describes the design and evaluation of a robust failure detector, able to maintain and distribute the correct list of alive resources within proven and scalable bounds. The detection and distribution of the fault information follow different overlay topologies that together guarantee minimal disturbance to the applications. A virtual observation ring minimizes the overhead by allowing each node to be observed by another single node, providing an unobtrusive behavior. The propagation stage is using a non-uniform variant of a reliable broadcast over a circulant graph overlay network, and guarantees a logarithmic fault propagation. Extensive simulations, together with experiments on the Titan ORNL supercomputer, show that the algorithm performs extremely well, and exhibits all the desired properties of an Exascale-ready algorithm.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.26,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877105,Failure Detection;Fault-Tolerance;MPI,Algorithm design and analysis;Computer crashes;Fault tolerance;Fault tolerant systems;Heart beat;Observers;Protocols,fault diagnosis;fault tolerant computing;parallel processing,HPC systems;Titan ORNL supercomputer;circulant graph overlay network;exascale-ready algorithm;failure detection;failure propagation;fault information distribution;fault tolerance;logarithmic fault propagation;no-uniform variant;reliable broadcast;virtual observation ring,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
High Performance Emulation of Quantum Circuits,T. HÌ_ner; D. S. Steiger; M. Smelyanskiy; M. Troyer,"Inst. for Theor. Phys., ETH Zurich, Zurich, Switzerland","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,866,874,"As quantum computers of non-trivial size become available in the near future, it is imperative to develop tools to emulate small quantum computers. This allows for validation and debugging of algorithms as well as exploring hardware-software co-design to guide the development of quantum hardware and architectures. The simulation of quantum computers entails multiplications of sparse matrices with very large dense vectors of dimension 2n, where n denotes the number of qubits, making this a memory-bound and network bandwidth-limited application. We introduce the concept of a quantum computer emulator as a component of a software framework for quantum computing, enabling a significant performance advantage over simulators by emulating quantum algorithms at a high level rather than simulating individual gate operations. We describe various optimization approaches and present benchmarking results, establishing the superiority of quantum computer emulators in terms of performance.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.73,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877152,,Computational modeling;Computers;Emulation;Logic gates;Optimization;Quantum computing;Sparse matrices,computer debugging;hardware-software codesign;parallel processing;quantum computing,debugging;hardware-software codesign;high performance emulation;network bandwidth-limited application;quantum algorithms;quantum circuits;quantum computer emulators;quantum computing;quantum hardware;software framework;sparse matrices,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
GreenLA: Green Linear Algebra Software for GPU-accelerated Heterogeneous Computing,J. Chen; L. Tan; P. Wu; D. Tao; H. Li; X. Liang; S. Li; R. Ge; L. Bhuyan; Z. Chen,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,667,677,"While many linear algebra libraries have been developed to optimize their performance, no linear algebra library considers their energy efficiency at the library design time. In this paper, we present GreenLA - an energy efficient linear algebra software package that leverages linear algebra algorithmic characteristics to maximize energy savings with negligible overhead. GreenLA is (1) energy efficient: it saves up to several times more energy than the best existing energy saving approaches that do not modify library source codes; (2) high performance: its performance is comparable to the highly optimized linear algebra library MAGMA; and (3) transparent to applications: with the same programming interface, existing MAGMA users do not need to modify their source codes to benefit from GreenLA. Experimental results demonstrate that GreenLA is able to save up to three times more energy than the best existing energy saving approaches while delivering similar performance compared to the state-of-the-art linear algebra library MAGMA.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.56,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877135,CPU;DVFS;GPU;algorithmic slack prediction;critical path;dense matrix factorizations;energy;performance,Central Processing Unit;Graphics processing units;Libraries;Linear algebra;Prediction algorithms;Software algorithms;Software packages,green computing;linear algebra;mathematics computing;power aware computing;software libraries;software packages,GPU-accelerated heterogeneous computing;energy efficient linear algebra software package;green linear algebra software;greenLA;linear algebra library MAGMA,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
10M-Core Scalable Fully-Implicit Solver for Nonhydrostatic Atmospheric Dynamics,C. Yang; W. Xue; H. Fu; H. You; X. Wang; Y. Ao; F. Liu; L. Gan; P. Xu; L. Wang; G. Yang; W. Zheng,"State Key Lab. of Comput. Sci., Inst. of Software, Beijing, China","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,57,68,"An ultra-scalable fully-implicit solver is developed for stiff time-dependent problems arising from the hyperbolic conservation laws in nonhydrostatic atmospheric dynamics. In the solver, we propose a highly efficient hybrid domain-decomposed multigrid preconditioner that can greatly accelerate the convergence rate at the extreme scale. For solving the overlapped subdomain problems, a geometry-based pipelined incomplete LU factorization method is designed to further exploit the on-chip fine-grained concurrency. We perform systematic optimizations on different hardware levels to achieve best utilization of the heterogeneous computing units and substantial reduction of data movement cost. The fully-implicit solver successfully scales to the entire system of the Sunway TaihuLight supercomputer with over 10.5M heterogeneous cores, sustaining an aggregate performance of 7.95 PFLOPS in double-precision, and enables fast and accurate atmospheric simulations at the 488-m horizontal resolution (over 770 billion unknowns) with 0.07 simulated-years-per-day. This is, to our knowledge, the largest fully-implicit simulation to date.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.5,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877004,Sunway TaihuLight supercomputer;atmospheric modeling;fully implicit solver;heterogeneous many-core architecture,Atmospheric modeling;Computational modeling;Earth;Mathematical model;Predictive models;Supercomputers,atmospheric movements;atmospheric techniques;geophysics computing,10M-core scalable fully-implicit solver;Sunway TaihuLight supercomputer;data movement cost;heterogeneous computing units;hybrid domain-decomposed multigrid preconditioner;hyperbolic conservation law;nonhydrostatic atmospheric dynamics;on-chip fine-grained concurrency;ultrascalable fully-implicit solver,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Refactoring and Optimizing the Community Atmosphere Model (CAM) on the Sunway TaihuLight Supercomputer,H. Fu; J. Liao; W. Xue; L. Wang; D. Chen; L. Gu; J. Xu; N. Ding; X. Wang; C. He; S. Xu; Y. Liang; J. Fang; Y. Xu; W. Zheng; J. Xu; Z. Zheng; W. Wei; X. Ji; H. Zhang; B. Chen; K. Li; X. Huang; W. Chen; G. Yang,"Nat. Res. Center of Parallel Comput. Eng. & Technol, Beijing, China","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,969,980,"This paper reports our efforts on refactoring and optimizing the Community Atmosphere Model (CAM) on the Sunway TaihuLight supercomputer, which uses a many-core processor that consists of management processing elements (MPEs) and clusters of computing processing elements (CPEs). To map the large code base of CAM to the millions of cores on the Sunway system, we take OpenACC-based refactoring as the major approach, and apply source-to-source translator tools to exploit the most suitable parallelism for the CPE cluster, and to fit the intermediate variable into the limited on-chip fast buffer. For individual kernels, when comparing the original ported version using only MPEs and the refactored version using both the MPE and CPE clusters, we achieve up to 22ÌÑ speedup for the compute-intensive kernels. For the 25km resolution CAM global model, we manage to scale to 24,000 MPEs, and 1,536,000 CPEs, and achieve a simulation speed of 2.81 model years per day.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.82,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877161,OpenACC;atmospheric modeling;many-core;optimization;tool,Atmospheric modeling;Computational modeling;Computer architecture;Graphics processing units;Meteorology;Supercomputers,multiprocessing systems;program interpreters;software maintenance,CAM;CAM global model;CPE;MPE;OpenACC-based refactoring;Sunway TaihuLight supercomputer;community atmosphere model;computing processing elements;management processing elements;many-core processor;source-to-source translator tools,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Extreme Scale Plasma Turbulence Simulations on Top Supercomputers Worldwide,W. Tang; B. Wang; S. Ethier; G. Kwasniewski; T. Hoefler; K. Z. Ibrahim; K. Madduri; S. Williams; L. Oliker; C. Rosales-Fernandez; T. Williams,"Princeton Inst. for Comput. Sci. & Eng., Princeton Univ., Princeton, NJ, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,502,513,"The goal of the extreme scale plasma turbulence studies described in this paper is to expedite the delivery of reliable predictions on confinement physics in large magnetic fusion systems by using world-class supercomputers to carry out simulations with unprecedented resolution and temporal duration. This has involved architecture-dependent optimizations of performance scaling and addressing code portability and energy issues, with the metrics for multi-platform comparisons being ‰ÛÏtime-to-solution‰Û and ‰ÛÏenergy-to-solution‰Û. Realistic results addressing how confinement losses caused by plasma turbulence scale from present-day devices to the much larger $25 billion international ITER fusion facility have been enabled by innovative advances in the GTC-P code including (i) implementation of one-sided communication from MPI 3.0 standard; (ii) creative optimization techniques on Xeon Phi processors; and (iii) development of a novel performance model for the key kernels of the PIC code. Results show that modeling data movement is sufficient to predict performance on modern supercomputer platforms.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.42,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877121,,Computational modeling;Computer architecture;Interpolation;Ions;Supercomputers;Tokamaks,Tokamak devices;application program interfaces;message passing;multiprocessing systems;optimisation;parallel machines;performance evaluation;physics computing;plasma simulation;plasma toroidal confinement;plasma turbulence;power aware computing;software portability,GTC-P code;MPI 3.0 standard;PIC code;Xeon Phi processors;architecture-dependent optimizations;code portability;confinement losses;confinement physics;data movement modeling;energy issues;energy-to-solution metrics;extreme scale plasma turbulence simulation;international ITER fusion facility;large magnetic fusion systems;multiplatform comparisons;one-sided communication;performance prediction;performance scaling;supercomputers;temporal duration;time-to-solution metrics;unprecedented resolution,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
SERF: Efficient Scheduling for Fast Deep Neural Network Serving via Judicious Parallelism,F. Yan; O. Ruwase; Y. He; E. Smirni,"Univ. of Nevada, Reno, Reno, NV, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,300,311,"Deep neural networks (DNNs) has enabled a variety of artificial intelligence applications. These applications are backed by large DNN models running in serving mode on a cloud computing infrastructure. Given the compute-intensive nature of large DNN models, a key challenge for DNN serving systems is to minimize the request response latencies. This paper characterizes the behavior of different parallelism techniques for supporting scalable and responsive serving systems for large DNNs. We identify and model two important properties of DNN workloads: homogeneous request service demand, and interference among requests running concurrently due to cache/memory contention. These properties motivate the design of SERF, a dynamic scheduling framework that is powered by an interference-aware queueing-based analytical model. We evaluate SERF in the context of an image classification service using several well known benchmarks. The results demonstrate its accurate latency prediction and its ability to adapt to changing load conditions.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.25,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877104,,Analytical models;Computational modeling;Hardware;Load modeling;Neural networks;Neurons;Parallel processing,cache storage;cloud computing;dynamic scheduling;neural nets;parallel processing;queueing theory,DNN models;DNN workloads;SERF;cache contention;cloud computing;deep neural network;dynamic scheduling framework;homogeneous request service demand;image classification service;interference-aware queueing-based analytical model;judicious parallelism;memory contention;request response latency minimization;responsive serving systems;scalable serving systems,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
ZNNi: Maximizing the Inference Throughput of 3D Convolutional Networks on CPUs and GPUs,A. Zlateski; K. Lee; H. S. Seung,"Electr. Eng. & Comput. Sci. Dept., Massachusetts Inst. of Technol. Cambridge, Cambridge, MA, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,854,865,"Sliding window convolutional networks (ConvNets) have become a popular approach to computer vision problems such as image segmentation and object detection and localization. Here we consider the parallelization of inference, i.e., the application of a previously trained ConvNet, with emphasis on 3D images. Our goal is to maximize throughput, defined as the number of output voxels computed per unit time. We propose CPU and GPU primitives for convolutional and pooling layers, which are combined to create CPU, GPU, and CPU-GPU inference algorithms. The primitives include convolution based on highly efficient padded and pruned FFTs. Our theoretical analyses and empirical tests reveal a number of interesting findings. For example, adding host RAM can be a more efficient way of increasing throughput than adding another GPU or more CPUs. Furthermore, our CPU-GPU algorithm can achieve greater throughput than the sum of CPU-only and GPU-only throughputs.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.72,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877151,,Convolution;Graphics processing units;Kernel;Random access memory;Three-dimensional displays;Throughput,computer vision;fast Fourier transforms;feedforward neural nets;graphics processing units;parallel processing,3D convolutional networks;3D images;CPU-GPU inference algorithms;ConvNets;ZNNi;computer vision problems;convolutional layers;inference parallelization;inference throughput maximization;padded FFT;pooling layers;pruned FFT;sliding window convolutional networks,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
A Data Driven Scheduling Approach for Power Management on HPC Systems,S. Wallace; X. Yang; V. Vishwanath; W. E. Allcock; S. Coghlan; M. E. Papka; Z. Lan,"Illinois Inst. of Technol., Chicago, IL, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,656,666,"Modern schedulers running on HPC systems traditionally consider the number of resources and the time requested for each job that is to be executed when making scheduling decisions. Until recently this has been sufficient, however as systems get larger, other metrics like power consumption become necessary to ensure system stability. In this paper, we propose a data driven scheduling approach for controlling the power consumption of the entire system under any user defined budget. Here, ‰ÛÏdata driven‰Û means that our approach actively observes, analyzes, and assesses power behaviors of the system and user jobs to guide scheduling decisions for power management. This design is based on the key observation that HPC jobs have distinct power profiles. Our work contains an empirical analysis of workload power characteristics on a production system, dynamic learner to estimate the job power profile for scheduling, and an online power-aware scheduler for managing the overall system power. Using real workload traces, we demonstrate that our design effectively controls system power consumption while minimizing the impact on system utilization.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.55,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877134,,Control systems;Dynamic scheduling;Job shop scheduling;Monitoring;Power demand;Resource management,parallel processing;power aware computing;power consumption;processor scheduling,HPC jobs;HPC systems;data driven scheduling;dynamic learner;high performance computing;job power profile;online power-aware scheduler;power behaviors;power consumption;power management;production system;scheduling decisions;system stability;system utilization;user defined budget;user job scheduling;workload power characteristics;workload traces,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
A Highly Effective Global Surface Wave Numerical Simulation with Ultra-High Resolution,F. Qiao; W. Zhao; X. Yin; X. Huang; X. Liu; Q. Shu; G. Wang; Z. Song; X. Li; H. Liu; G. Yang; Y. Yuan,"First Inst. of Oceanogr., State Oceanic Adm., Qingdao, China","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,46,56,"Surface wave is the most energetic form of motions in the ocean and is crucially important to navigation safety and climate change. High-resolution global wave model plays a key role in accurate surface wave forecasting. However, operational forecasting systems are still not in high-resolution due to entailed high demand for large computation, as well as low parallel efficiency barrier. Here breakthroughs encompassing the design and application of irregular quasi-rectangular domain decomposition, master-slave cooperative computing workflow and pipelining scheme were applied to a global wave model, which has been used in several operational forecasting systems and earth system models. Our realistic surface wave simulations on Sunway TaihuLight Supercomputer demonstrated that our model had outstanding scalability and achieved 45.43 PFlops in ultra-high resolution of (1/100)å¡, using full-scale supercomputer with 10,649,600 cores. That provides a highly effective solution for accurate surface wave forecasting and climate change prediction.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.4,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877003,,Atmospheric modeling;Computational modeling;Forecasting;Predictive models;Sea surface;Surface waves,geophysical image processing;geophysics computing;image resolution;marine navigation;numerical analysis;ocean waves;parallel processing;pipeline processing;surface waves (fluid),Sunway TaihuLight Supercomputer;climate change prediction;earth system models;full-scale supercomputer;highly effective global surface wave numerical simulation;irregular quasi-rectangular domain decomposition application;master-slave cooperative computing workflow;navigation safety;operational forecasting system;pipelining scheme;surface wave forecasting;ultra-high resolution,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
High-Frequency Nonlinear Earthquake Simulations on Petascale Heterogeneous Supercomputers,D. Roten; Y. Cui; K. B. Olsen; S. M. Day; K. Withers; W. H. Savran; P. Wang; D. Mu,"San Diego State Univ., San Diego, CA, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,957,968,"The omission of nonlinear effects in large-scale 3D ground motion estimation, which are particularly challenging due to memory and scalability issues, can result in costly misguidance for structural design in earthquake-prone regions. We have implemented nonlinearity using a Drucker-Prager yield condition in AWP-ODC and further optimized the CUDA kernels to more efficiently utilize the GPU's memory bandwidth. The application has resulted in a significant increase in the model region and accuracy for state-of-the-art earthquake simulations in a realistic earth structure, which are now able to resolve the wavefield at frequencies relevant for the most vulnerable buildings (> 1 Hz) while maintaining the scalability and efficiency of the method. We successfully run the code on 4,200 Kepler K20X GPUs on NCSA Blue Waters and OLCF Titan to simulate a M 7.7 earthquake on the southern San Andreas fault with a spatial resolution of 25 m for frequencies up to 4 Hz.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.81,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877160,GPU;SCEC;earthquake ground motion;fault zone plasticity;nonlinear soil behavior,Computational modeling;Earthquakes;Graphics processing units;Hazards;Mathematical model;Propagation;Stress,earthquakes;faulting;graphics processing units;seismic waves,AWP-ODC;CUDA kernel;Drucker-Prager yield condition;GPU memory bandwidth;NCSA blue water;OLCF Titan;earthquake-prone region;high-frequency nonlinear earthquake simulation;large-scale 3D ground motion estimation;nonlinear effect omission;petascale heterogeneous supercomputer;realistic earth structure;scalability;southern San Andreas;state-of-the-art earthquake simulation,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Author index,,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,1026,1032,Presents an index of the authors whose articles are published in the conference proceedings record.,,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.87,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877166,,,,,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Granularity and the Cost of Error Recovery in Resilient AMR Scientific Applications,A. Dubey; H. Fujita; D. T. Graves; A. Chien; D. Tiwari,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,492,501,"Supercomputing platforms are expected to have larger failure rates in the future because of scaling and power concerns. The memory and performance impact may vary with error types and failure modes. Therefore, localized recovery schemes will be important for scientific computations, including failure modes where application intervention is suitable for recovery. We present a resiliency methodology for applications using structured adaptive mesh refinement, where failure modes map to granularities within the application for detection and correction. This approach also enables parameterization of cost for differentiated recovery. The cost model is built with tuning parameters that can be used to customize the strategy for different failure rates in different computing environments. We also show that this approach can make recovery cost proportional to the failure rate.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.41,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877120,High performance computing;Scientific computing,Arrays;Bars;Contamination;Libraries;Mathematical model;Resilience;Transient analysis,parallel processing;power aware computing;software cost estimation;system recovery,AMR scientific applications;cost parameterization;error recovery;high performance computing;localized recovery;structured adaptive mesh refinement;supercomputing platforms,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
HARP: Predictive Transfer Optimization Based on Historical Analysis and Real-Time Probing,E. Arslan; K. Guner; T. Kosar,"Dept. of Comput. Sci. & Eng., Univ. at Buffalo (SUNY), Buffalo, NY, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,288,299,"Increasingly data-intensive scientific and commercial applications require frequent movement of large datasets from one site to the other. Despite the growing capacity of the networking capacity, these data movements rarely achieve the promised data transfer rates of the underlying physical network due to the poorly tuned data transfer protocols. Accurately and efficiently tuning the data transfer protocol parameters in a dynamically changing network environment is a big challenge and still an open research problem. In this paper, we present predictive end-to-end data transfer optimization algorithms based on historical data analysis and real-time background traffic probing, dubbed HARP. Most of the existing work in this area is solely based on real time network probing, which either cause too much sampling overhead or fail to accurately predict the correct transfer parameters. Combining historical data analysis with real time sampling enables our algorithms to tune the application level data transfer parameters accurately and efficiently to achieve close-to-optimal end-to-end data transfer throughput with very low overhead. Our experimental analysis over a variety of network settings shows that HARP outperforms existing solutions by up to 50% in terms of the achieved throughput.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.24,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877103,,Concurrent computing;Data transfer;Pipeline processing;Protocols;Real-time systems;Throughput,computer networks;data analysis;data communication;optimisation;protocols,HARP;data transfer protocol parameter tuning;data transfer rates;data-intensive scientific application;dynamically changing network environment;historical analysis;historical data analysis;large dataset movement;networking capacity;physical network;predictive end-to-end data transfer optimization algorithm;predictive transfer optimization;real time network probing;real time sampling;real-time background traffic probing;real-time probing,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Distributed-Memory Large Deformation Diffeomorphic 3D Image Registration,A. Mang; A. Gholami; G. Biros,"Inst. of Comput. Eng. & Sci. The Univ. of Texas at Austin, Austin, TX, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,842,853,"We present a parallel distributed-memory algorithm for large deformation diffeomorphic registration of volumetric images that produces large isochoric deformations (locally volume preserving). Image registration is a key technology in medical image analysis. Our algorithm uses a partial differential equation constrained optimal control formulation. Finding the optimal deformation map requires the solution of a highly nonlinear problem that involves pseudo-differential operators, biharmonic operators, and pure advection operators both forward and backward in time. A key issue is the time to solution, which poses the demand for efficient optimization methods as well as an effective utilization of high performance computing resources. To address this problem we use a preconditioned, inexact, Gauss-Newton-Krylov solver. Our algorithm integrates several components: a spectral discretization in space, a semi-Lagrangian formulation in time, analytic adjoints, different regularization functionals (including volume-preserving ones), a spectral preconditioner, a highly optimized distributed Fast Fourier Transform, and a cubic interpolation scheme for the semi-Lagrangian time-stepping. We demonstrate the scalability of our algorithm on images with resolution of up to 10243 on the ‰ÛÏMaverick‰Û and ‰ÛÏStampede‰Û systems at the Texas Advanced Computing Center (TACC). The critical problem in the medical imaging application domain is strong scaling, that is, solving registration problems of a moderate size of 2563-a typical resolution for medical images. We are able to solve the registration problem for images of this size in less than five seconds on 64 x86 nodes of TACC's ‰ÛÏMaverick‰Û system.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.71,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877150,Diffeomorphic Image Registration;High Performance Computing;Newton-Krylov Methods;Optimal Control;Scientific Computing,Acceleration;Biomedical imaging;Graphics processing units;Image registration;Image resolution;Interpolation;Optimal control,Fourier transforms;Gaussian processes;biomedical MRI;computerised tomography;image registration;interpolation;medical image processing;parallel processing;partial differential equations;stereo image processing,Fourier transform;Gauss-Newton-Krylov solver;computed tomography;cubic interpolation;diffeomorphic 3D image registration;distributed memory large deformation;high performance computing;magnetic resonance imaging;medical image analysis;optimal control;parallel distributed memory;partial differential equation;semi-Lagrangian formulation,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Unprotected Computing: A Large-Scale Study of DRAM Raw Error Rate on a Supercomputer,L. Bautista-Gomez; F. Zyulkyarov; O. Unsal; S. McIntosh-Smith,"Barcelona Supercomput. Center, Barcelona, Spain","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,645,655,"Supercomputers offer new opportunities for scientific computing as they grow in size. However, their growth also poses new challenges. Resilience has been recognized as one of the most pressing issues to solve for extreme scale computing. Transistor scaling in the single-digit nanometer era and power constraints might dramatically increase the failure rate of next generation machines. DRAM errors have been analyzed in the past for different supercomputers but those studies are usually based on job scheduler logs and counters produced by hardware-level error correcting codes. Consequently, little is known about errors escaping hardware checks, which lead to silent data corruption. This work attempts to fill that gap by analyzing memory errors for over a year on a cluster with about 1000 nodes featuring low-power memory without error correction. The study gathered millions of events recording detailed information of thousands of memory errors, many of them corrupting multiple bits. Several factors are analyzed, such as temporal and spatial correlation between errors, but also the influence of temperature and even the position of the sun in the sky. The study showed that most multi-bit errors corrupted non-adjacent bits in the memory word and that most errors flipped memory bits from 1 to 0. In addition, we observed thousands of cases of multiple single-bit errors occurring simultaneously in different regions of the memory. These new observations would not be possible by simply analyzing error correction counters on classical systems. We propose several directions in which the findings of this study can help the design of more reliable systems in the future.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.54,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877133,,Error analysis;Error correction codes;Hardware;Memory management;Prototypes;Random access memory;Supercomputers,DRAM chips;error analysis;fault tolerant computing;parallel machines;software reliability,DRAM raw error rate;extreme scale computing;fault tolerance research;hardware checks;high-performance computing;memory errors;multibit errors;resilience;silent data corruption;supercomputer;transistor scaling;unprotected computing,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Enhanced MPSM3 for Applications to Quantum Biological Simulations,A. Pozdneev; V. Weber; T. Laino; C. Bekas; A. Curioni,"IBM Sci. & Technol. Center, Moscow, Russia","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,96,106,"Classical molecular dynamics simulations have been the preferred method to cope with the characteristic sizes and time scales of complex life-science systems. However, while classical methods have well known limitations, such as that their accuracy strongly depends on empirical tuning, the practical use of far more accurate methods that rely on quantum Hamiltonians, has been limited by the current efficiency and scalability of sparse matrix-matrix multiplication algorithms used in the self-consistent field equations. In this paper, we show unprecedented massive scalability of a recently presented method, called MPSM3, for sparse matrix-matrix multiplication. The algorithmic basis of the method was presented in a recent publication, while here we describe the algorithmic enhancements that allow us to claim at least one order of magnitude improvement in scalability and time to solution over the state of the art (original MPSM3). We achieve a time to solution for the multiplication of density matrices within the self-consistent field scheme that is approaching the time needed to evaluate energy and forces with classical force-field methods and that is independent from the system size, provided proportional resources. This latest development renders the application of entirely quantum Hamiltonians to systems of several millions of atoms for extended molecular dynamics investigations feasible.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.8,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877007,Biological Simulations;Scalability;Semiempirical Molecular Dynamics;Sparse Matrix-Matrix Multiplication;Time-to-solution,Biological system modeling;Computational modeling;Jacobian matrices;Kernel;Mathematical model;Scalability;Sparse matrices,biology;molecular dynamics method;quantum field theory;sparse matrices,characteristic sizes;classical force-field methods;classical molecular dynamic simulations;complex life-science systems;density matrix multiplication;empirical tuning;enhanced MPSM3;extended molecular dynamics investigations;quantum Hamiltonians;quantum biological simulations;self-consistent field equations;sparse matrix-matrix multiplication algorithms;time scales,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Extreme-Scale Phase Field Simulations of Coarsening Dynamics on the Sunway TaihuLight Supercomputer,J. Zhang; C. Zhou; Y. Wang; L. Ju; Q. Du; X. Chi; D. Xu; D. Chen; Y. Liu; Z. Liu,"Comput. Network Inf. Center, Beijing, China","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,34,45,"Many important properties of materials such as strength, ductility, hardness and conductivity are determined by the microstructures of the material. During the formation of these microstructures, grain coarsening plays an important role. The Cahn-Hilliard equation has been applied extensively to simulate the coarsening kinetics of a two-phase microstructure. It is well accepted that the limited capabilities in conducting large scale, long time simulations constitute bottlenecks in predicting microstructure evolution based on the phase field approach. We present here a scalable time integration algorithm with large stepsizes and its efficient implementation on the Sunway TaihuLight supercomputer. The highly nonlinear and severely stiff Cahn-Hilliard equations with degenerate mobility for microstructure evolution are solved at extreme scale, demonstrating that the latest advent of high performance computing platform and the new advances in algorithm design are now offering us the possibility to simulate the coarsening dynamics accurately at unprecedented spatial and time scales.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.3,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877002,,Algorithm design and analysis;Computational modeling;Electronic mail;Heuristic algorithms;Mathematical model;Microstructure;Numerical models,digital simulation;ductility;hardness;materials science computing;mechanical engineering computing;parallel machines,Cahn-Hilliard equation;Sunway TaihuLight supercomputer;coarsening dynamics;coarsening kinetics;conductivity;ductility;extreme-scale phase field simulations;grain coarsening;hardness;high performance computing platform;material properties;microstructure evolution;phase field approach;two-phase microstructure,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Perilla: Metadata-Based Optimizations of an Asynchronous Runtime for Adaptive Mesh Refinement,T. Nguyen; D. Unat; W. Zhang; A. Almgren; N. Farooqi; J. Shalf,"Lawrence Berkeley Nat. Lab., Berkeley, CA, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,945,956,"Hardware architecture is increasingly complex, urging the development of asynchronous runtime systems with advance resource and locality management supports. However, these supports may come at the cost of complicating the user interface while programming remains one of the major constraints to wide adoption of asynchronous runtimes in practice. In this paper, we propose a solution that leverages application metadata to enable challenging optimizations as well as to facilitate the task of transforming legacy code to an asynchronous representation. We develop Perilla, a task graph-based runtime system that requires only modest programming effort. Perilla utilizes metadata of an AMR software framework to enable various optimizations at the communication layer without complicating its API. Experimental results with different applications on up to 24K processor cores show that Perilla can realize up to 1.44x speedup over the synchronous code variant. The metadata enabled optimizations account for 25% to 100% of the performance improvement.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.80,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877159,,Computer architecture;Libraries;Metadata;Optimization;Parallel processing;Programming;Runtime,application program interfaces;graph theory;mesh generation;meta data;optimisation;parallel processing;programming;user interfaces,AMR software framework metadata;API;Perilla;adaptive mesh refinement;advance resource;application metadata;asynchronous representation;asynchronous runtime systems;hardware architecture;legacy code transformation;locality management supports;metadata-based optimizations;processor cores;programming effort;task graph-based runtime system;user interface,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
A Multi-faceted Approach to Job Placement for Improved Performance on Extreme-Scale Systems,C. Zimmer; S. Gupta; S. Atchley; S. S. Vazhkudai; C. Albing,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,1015,1025,"Job placement plays a pivotal role in application performance on supercomputers. We present a multi-faceted exploration to influence placement in extreme-scale systems, to improve network performance and decrease variability. In our first exploration, Scores, we developed a machine learning model that extracts features from a job's node-allocation and grades performance. This identified several important node-metrics that led to Dual-Ended scheduling, a means of reducing network contention without impacting utilization. In evaluations on the Titan supercomputer, we observed reductions in average hop-count by up to 50%. We also developed an improved node-layout strategy that targets a better balance between network latency and bandwidth, replacing the default ALPS layout on Titan that resulted in an average of 10% runtime improvement. Both of these efforts underscore the importance of a job placement strategy that is cognizant of workload mixture and network topology.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.86,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877165,,Bandwidth;Benchmark testing;Layout;Processor scheduling;Resource management;Three-dimensional displays;Visualization,feature extraction;learning (artificial intelligence);parallel machines;resource allocation;scheduling,ALPS layout;Scores;Titan supercomputer;application performance;average hop-count reduction;bandwidth;dual-ended scheduling;extreme-scale system;feature extraction;job node-allocation;job placement;machine learning model;network contention reducition;network latency;network performance;network topology;node metrics;node-layout strategy;performance grading;performance improvement;runtime improvement;workload mixture,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
[Copyright notice],,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,ii,ii,,,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.90,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7876996,,,,,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Automating Wavefront Parallelization for Sparse Matrix Computations,A. Venkat; M. S. Mohammadi; J. Park; H. Rong; R. Barik; M. M. Strout; M. Hall,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,480,491,"This paper presents a compiler and runtime framework for parallelizing sparse matrix computations that have loop-carried dependences. Our approach automatically generates a runtime inspector to collect data dependence information and achieves wavefront parallelization of the computation, where iterations within a wavefront execute in parallel, and synchronization is required across wavefronts. A key contribution of this paper involves dependence simplification, which reduces the time and space overhead of the inspector. This is implemented within a polyhedral compiler framework, extended for sparse matrix codes. Results demonstrate the feasibility of using automatically-generated inspectors and executors to optimize ILU factorization and symmetric Gauss-Seidel relaxations, which are part of the Preconditioned Conjugate Gradient (PCG) computation. Our implementation achieves a median speedup of 2.97ÌÑ on 12 cores over the reference sequential PCG implementation, significantly outperforms PCG parallelized using Intel's Math Kernel Library (MKL), and is within 6% of the median performance of manually-parallelized PCG.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.40,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877119,,Arrays;Indexes;Level set;Optimization;Runtime;Sparse matrices;Testing,conjugate gradient methods;parallelising compilers;program control structures;sparse matrices;system monitoring,ILU factorization optimization;Intel MKL;Intel Math Kernel Library;PCG computation;data dependence information collection;dependence simplification;loop-carried dependence;parallel iteration;polyhedral compiler framework;preconditioned conjugate gradient computation;runtime framework;runtime inspector generation;sequential PCG implementation;space overhead reduction;sparse matrix codes;sparse matrix computation parallelization;symmetric Gauss-Seidel relaxation;time overhead reduction;wavefront parallelization automation;wavefront synchronization,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Performance Modeling of In Situ Rendering,M. Larsen; C. Harrison; J. Kress; D. Pugmire; J. S. Meredith; H. Childs,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,276,287,"With the push to exascale, in situ visualization and analysis will continue to play an important role in high performance computing. Tightly coupling in situ visualization with simulations constrains resources for both, and these constraints force a complex balance of trade-offs. A performance model that provides an a priori answer for the cost of using an in situ approach for a given task would assist in managing the trade-offs between simulation and visualization resources. In this work, we present new statistical performance models, based on algorithmic complexity, that accurately predict the run-time cost of a set of representative rendering algorithms, an essential in situ visualization task. To train and validate the models, we conduct a performance study of an MPI+X rendering infrastructure used in situ with three HPC simulation applications. We then explore feasibility issues using the model for selected in situ rendering questions.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.23,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877102,,Computational modeling;Data models;Data visualization;Graphics processing units;Pipelines;Rendering (computer graphics);Supercomputers,computational complexity;data visualisation;parallel processing;rendering (computer graphics);resource allocation;statistical analysis,HPC simulation application;MPI+X rendering infrastructure;algorithmic complexity;high performance computing;in situ analysis;in situ rendering;in situ visualization;performance modeling;rendering algorithm;resource trade-off management;run-time cost prediction;statistical performance model,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
G-Store: High-Performance Graph Store for Trillion-Edge Processing,P. Kumar; H. H. Huang,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,830,841,"High-performance graph processing brings great benefits to a wide range of scientific applications, e.g., biology networks, recommendation systems, and social networks, where such graphs have grown to terabytes of data with billions of vertices and trillions of edges. Subsequently, storage performance plays a critical role in designing a high-performance computer system for graph analytics. In this paper, we present G-Store, a new graph store that incorporates three techniques to accelerate the I/O and computation of graph algorithms. First, G-Store develops a space-efficient tile format for graph data, which takes advantage of the symmetry present in graphs as well as a new smallest number of bits representation. Second, G-Store utilizes tile-based physical grouping on disks so that multi-core CPUs can achieve high cache and memory performance and fully utilize the throughput from an array of solid-state disks. Third, G-Store employs a novel slide-cache-rewind strategy to pipeline graph I/O and computing. With a modest amount of memory, G-Store utilizes a proactive caching strategy in the system so that all fetched graph data are fully utilized before evicted from memory. We evaluate G-Store on a number of graphs against two state-of-the-art graph engines and show that G-Store achieves 2 to 8ÌÑ saving in storage and outperforms both by 2 to 32ÌÑ. G-Store is able to run different algorithms on trillion-edge graphs within tens of minutes, setting a new milestone in semi-external graph processing system.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.70,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877149,,Algorithm design and analysis;Arrays;Memory management;Metadata;Optimization;Partitioning algorithms;Two dimensional displays,cache storage;graph theory;multiprocessing systems;parallel processing,G-Store;bits representation;cache performance;graph I/O;graph algorithms;graph analytics;graph processing system;high-performance computer system;high-performance graph processing;high-performance graph store;memory performance;multicore CPU;proactive caching;scientific applications;slide-cache-rewind strategy;solid-state disks;storage performance;tile format;tile-based physical grouping;trillion-edge processing,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Designing MPI Library with On-Demand Paging (ODP) of InfiniBand: Challenges and Benefits,M. Li; K. Hamidouche; X. Lu; H. Subramoni; J. Zhang; D. K. Panda,"Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,433,443,"Existing InfiniBand drivers require the communication buffers to be pinned in physical memory during communication. Most runtimes leave these buffers pinned until the end of the run. Such situation limits the swappable memory space for applications. To address these concerns, Mellanox has recently introduced the On-Demand Paging (ODP) feature for InfiniBand. With ODP, communication buffers are paged in when they are needed by the HCA and paged out when the OS needs to swap them. This paper presents a thorough analysis on ODP and studies its performance characteristics. With these studies, we propose novel designs of ODP-aware MPI communication protocols. To the best of our knowledge, this is the first work to study and analyze the ODP feature and design an ODP-aware MPI library. Performance evaluations with applications show that ODP-aware designs can reduce the size of pin-down buffers by 11X without performance degradation compared with the pin-down scheme.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.36,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877115,,Bandwidth;Benchmark testing;Data transfer;Libraries;Memory management;Protocols;Runtime,application program interfaces;device drivers;message passing;paged storage;software libraries,HCA;InfiniBand drivers;MPI communication protocols;MPI library;ODP;communication buffers;on-demand paging;physical memory;swappable memory space,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Optimizing Memory Efficiency for Deep Convolutional Neural Networks on GPUs,C. Li; Y. Yang; M. Feng; S. Chakradhar; H. Zhou,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,633,644,"Leveraging large data sets, deep Convolutional Neural Networks (CNNs) achieve state-of-the-art recognition accuracy. Due to the substantial compute and memory operations, however, they require significant execution time. The massive parallel computing capability of GPUs make them as one of the ideal platforms to accelerate CNNs and a number of GPU-based CNN libraries have been developed. While existing works mainly focus on the computational efficiency of CNNs, the memory efficiency of CNNs have been largely overlooked. Yet CNNs have intricate data structures and their memory behavior can have significant impact on the performance. In this work, we study the memory efficiency of various CNN layers and reveal the performance implication from both data layouts and memory access patterns. Experiments show the universal effect of our proposed optimizations on both single layers and various networks, with up to 27.9ÌÑ for a single layer and up to 5.6ÌÑ on the whole networks.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.53,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877132,Convolutional Neural Network;Data Layout;Deep Learning;GPU Acceleration;Memory Efficiency,Convolution;Feature extraction;Graphics processing units;Layout;Libraries;Memory management;Neural networks,data structures;graphics processing units;neural nets;optimisation;parallel processing;storage management,CNN;GPU;data structures;deep convolutional neural networks;large data sets;massive parallel computing;memory efficiency;optimization,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Increasing Molecular Dynamics Simulation Rates with an 8-Fold Increase in Electrical Power Efficiency,W. M. Brown; A. Semin; M. Hebenstreit; S. Khvostov; K. Raman; S. J. Plimpton,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,82,95,"Electrical power efficiency is a primary concern in designing modern HPC systems. Common strategies to improve CPU power efficiency rely on increased parallelism within a processor that is enabled both by an increase in the vector capabilities within the core and also the number of cores within a processor. Although many-core processors have been available for some time, achieving power-efficient performance has been challenging due to the offload model. Here, we evaluate performance of the molecular dynamics code LAMMPS on two new Intel<sup>å¨</sup> processors including the second generation many-core Intel<sup>å¨</sup> Xeon Phi‰ã¢ processor that is available as a bootable CPU. We describe our approach to measure power consumption out-of-band and software optimizations necessary to achieve energy efficiency. We analyze benefits from Intel<sup>å¨</sup> Advanced Vector Extensions 512 instructions and demonstrate increased simulations rates with over 9X the CPU+DRAM power efficiency when compared to the unoptimized code on previous generation processors.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.7,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877006,,Computer architecture;Graphics processing units;Instruction sets;Optimization;Registers;Sockets,multiprocessing systems;power aware computing,CPU power efficiency;HPC systems;bootable CPU;electrical power efficiency;energy efficiency;many-core Intel Xeon Phi processor;many-core processors;molecular dynamics code LAMMPS;molecular dynamics simulation rates;offload model;power consumption;power-efficient performance;software optimizations,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Low-complexity segmented CRC-aided SC stack decoder for polar codes,W. Song; H. Zhou; Y. Zhao; S. Zhang; X. You; C. Zhang,"National Mobile Communications Research Laboratory, Southeast University, Nanjing, China","2016 50th Asilomar Conference on Signals, Systems and Computers",20170306,2016,,,1189,1193,"In order to improve the decoding performance of successive cancellation (SC) decoder for polar code, SC stack (SCS) polar decoder has been proposed by existing literatures. Compared with SCL decoder, SCS decoder successfully reduces the decoding complexity especially at high SNR while keeping the same decoding performance. However in correlated channels at low SNR, SCS decoder suffers a lot from the overwhelming complexity due to frequent backtracking operations. Aim to lower such complexity as well as the required storage resources of SCS decoder, this paper proposes a low complexity SCS polar decoder based on segmented CRC scheme. Employing the segmented CRC as an early termination scheme, the proposed SCS decoder can successfully reduce the complexity especially at low SNR without any performance degradation. Numerical results have shown that for (1024, 512) polar code, the proposed approach achieves more than 34.4% complexity reduction and 29.7% resource occupation reduction compared to the conventional CRC-aided SCS (CA-SCS) decoder at SNR of 1.5 dB.",,DVD:978-1-5386-3952-8; Electronic:978-1-5386-3954-2; POD:978-1-5386-3955-9,10.1109/ACSSC.2016.7869560,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7869560,Polar codes;segmented CRC;stack decoding,Detectors;Maximum likelihood decoding;Reliability;Signal to noise ratio;Time complexity,cyclic redundancy check codes;decoding;numerical analysis,CRC-aided SC stack decoder;SC stack polar decoder;SCS polar decoder;polar codes;segmented CRC scheme;successive cancellation decoder,,,,,,,6-9 Nov. 2016,,IEEE,IEEE Conference Publications
Simulations of Below-Ground Dynamics of Fungi: 1.184 Pflops Attained by Automated Generation and Autotuning of Temporal Blocking Codes,T. Muranushi; H. Hotta; J. Makino; S. Nishizawa; H. Tomita; K. Nitadori; M. Iwasawa; N. Hosono; Y. Maruyama; H. Inoue; H. Yashiro; Y. Nakamura,"RIKEN Adv. Inst. for Comput. Sci., Kobe, Japan","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,23,33,"Stencil computation has many applications in science and engineering, thus many optimization techniques such as temporal blocking have been developed. They are, however, rarely used in real-world applications, since a large amount of careful programming is required for even the simplest of stencils. We introduce Formura, a domain specific language that provides easy access to optimized stencil computations. Higher-order integration schemes can be defined using mathematical notations. Formura generates C code with MPI calls and performs autotuning. Hence its performance is portable to most distributed-memory computers. We show the scientific applicability of Formura by performing magnetohydrodynamics (MHD) and belowground biology simulations. Ability to reach bytes-per-flops ratio only attainable by temporal blocking is demonstrated. We also demonstrate scaling up to the full nodes of the K computer, with 1.184 Pflops, 11.62% floating-pointoperation efficiency, and 31.26% memory throughput efficiency.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.2,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877001,,Computational modeling;DSL;Magnetohydrodynamics;Mathematical model;Numerical models;Optimization;Programming,C language;biology computing;cellular biophysics;distributed memory systems;floating point arithmetic;magnetohydrodynamics;message passing;microorganisms;program compilers;storage management,1.184 Pflops;Below-Ground Dynamics Simulations;C code generation;Formura;MPI calls;automated temporal blocking code autotuning;automated temporal blocking code generation;below-ground biology simulation;bytes-per-flops ratio;distributed-memory computers;domain specific language;floating-point-operation efficiency;fungi;higher-order integration;magnetohydrodynamics;mathematical notations;memory throughput efficiency;optimized stencil computations,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Compiler-Directed Lightweight Checkpointing for Fine-Grained Guaranteed Soft Error Recovery,Q. Liu; C. Jung; D. Lee; D. Tiwari,"Virginia Tech, Blacksburg, VA, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,228,239,"This paper presents Bolt, a compiler-directed soft error recovery scheme, that provides fine-grained and guaranteed recovery without excessive performance and hardware overhead. To get rid of expensive hardware support, the compiler protects the architectural inputs during their entire liveness period by safely checkpointing the last updated value in idempotent regions. To minimize the performance overhead, Bolt leverages a novel compiler analysis that eliminates those checkpoints whose value can be reconstructed by other checkpointed values without compromising the recovery guarantee. As a result, Bolt incurs only 4.7% performance overhead on average which is 57% reduction compared to the state-of-the-art scheme that requires expensive hardware support for the same recovery guarantee as Bolt.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.19,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877098,Checkpointing;Compiler;Reliability,Checkpointing;Error correction codes;Fasteners;Hardware;Radio frequency;Registers;Runtime,checkpointing;program compilers,Bolt;compiler-directed lightweight checkpointing;fine-grained guaranteed soft error recovery,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Extended Task Queuing: Active Messages for Heterogeneous Systems,M. LeBeane; B. Potter; A. Pan; A. Dutu; V. Agarwala; W. Lee; D. Majeti; B. Ghimire; E. V. Tassell; S. Wasmundt; B. Benton; M. Breternitz; M. L. Chu; M. Thottethodi; L. K. John; S. K. Reinhardt,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,933,944,"Accelerators have emerged as an important component of modern cloud, datacenter, and HPC computing environments. However, launching tasks on remote accelerators across a network remains unwieldy, forcing programmers to send data in large chunks to amortize the transfer and launch overhead. By combining advances in intra-node accelerator unification with one-sided Remote Direct Memory Access (RDMA) communication primitives, it is possible to efficiently implement lightweight tasking across distributed-memory systems. This paper introduces Extended Task Queuing (XTQ), an RDMA-based active messaging mechanism for accelerators in distributed-memory systems. XTQ's direct NIC-to-accelerator communication decreases inter-node GPU task launch latency by 10-15% for small-to-medium sized messages and ameliorates CPU message servicing overheads. These benefits are shown in the context of MPI accumulate, reduce, and allreduce operations with up to 64 nodes. Finally, we illustrate how XTQ can improve the performance of popular deep learning workloads implemented in the Computational Network Toolkit (CNTK).",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.79,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877158,Accelerator architectures;Computer architecture;Computer networks;Distributed computing;Network interfaces,Central Processing Unit;Context;Graphics processing units;Hardware;Kernel;Peer-to-peer computing;Schedules,distributed memory systems;file organisation;graphics processing units;learning (artificial intelligence);message passing;network interfaces;queueing theory,CNTK;CPU message servicing overhead;Computational Network Toolkit;HPC computing environment;MPI accumulate operation;MPI allreduce operation;MPI reduce operation;RDMA communication;RDMA-based active messaging mechanism;XTQ direct NIC-to-accelerator communication;datacenter;deep learning workload;distributed-memory system;extended task queuing;heterogeneous systems;internode GPU task launch latency;intranode accelerator unification;lightweight tasking;modern cloud;one-sided remote direct memory access communication;remote accelerator,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Optimal Execution of Co-analysis for Large-Scale Molecular Dynamics Simulations,P. Malakar; V. Vishwanath; C. Knight; T. Munson; M. E. Papka,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,702,715,"The analysis of scientific simulation data enables scientists to derive insights from their simulations. This analysis of the simulation output can be performed at the same execution site as the simulation using the same resources or can be done at a different site. The optimal output frequency is challenging to decide and is often chosen empirically. We propose a mathematical formulation for choosing the optimal frequency of data transfer for analysis and the feasibility of performing the analysis, under the given resource constraints such as network bandwidth, disk space, available memory, and computation time. We propose formulations for two cases of co-analysis - local and remote. We consider various analyses features such as computation time, input data and memory requirement, importance of the analysis and minimum frequency required for performing the analysis. We demonstrate the effectiveness of our approach using molecular dynamics applications on the Mira and Edison supercomputers.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.59,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877138,analysis;optimization;scheduling;simulation,Analytical models;Computational modeling;Data models;Data transfer;Memory management;Nonvolatile memory;Supercomputers,data handling;digital simulation;molecular dynamics method;parallel machines;scientific information systems,Edison supercomputer;Mira supercomputer;available memory;computation time;data transfer;disk space;large-scale molecular dynamics simulations;network bandwidth;optimal coanalysis execution;optimal output frequency;resource constraints;scientific simulation data,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Multi-resource Fair Sharing for Datacenter Jobs with Placement Constraints,W. Wang; B. Li; B. Liang; J. Li,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,1003,1014,"Providing quality-of-service guarantees by means of fair sharing has never been more challenging in datacenters. Due to the heterogeneity of machine configurations, datacenter jobs frequently specify placement constraints, restricting them to run on a particular class of machines meeting specific hardware/software requirements. In addition, jobs have diverse demands across multiple resource types, and may saturate any of the CPU, memory, or storage resources. Despite the rich body of recent work on datacenter scheduling, it remains unclear how multi-resource fair sharing is defined and achieved for jobs with placement constraints. In this paper, we propose a new sharing policy called Task Share Fairness (TSF). With TSF, jobs are better off sharing the datacenter, and are better off reporting demands and constraints truthfully. We have prototyped TSF on Apache Mesos and confirmed its service guarantees in a 50-node EC2 cluster. Trace-driven simulations have further revealed that TSF speeds up 60% of tasks over existing fair schedulers.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.85,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877164,Cluster schedulers;fairness;multi-resource allocation;placement constraints,Google;Kernel;Memory management;Quality of service;Resource management;Scheduling,computer centres;quality of service;resource allocation;scheduling,50-node EC2 cluster;Apache Mesos;TSF;datacenter jobs;datacenter scheduling;multiresource fair sharing;placement constraints;quality-of-service guarantees;task share fairness,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
[Title page],,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,i,i,The following topics are dealt with: high performance computing; computer networks; storage management; molecular dynamics simulation; numerical algorithms; data management; data visualization; distributed computing; graph algorithms; cloud computing; and job scheduling.,,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.89,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7876995,,,cloud computing;computer networks;data handling;distributed processing;scheduling;storage management,cloud computing;computer networks;data management;data visualization;distributed computing;graph algorithms;high performance computing;job scheduling;molecular dynamics simulation;numerical algorithms;storage management,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
"TrueNorth Ecosystem for Brain-Inspired Computing: Scalable Systems, Software, and Applications",J. Sawada; F. Akopyan; A. S. Cassidy; B. Taba; M. V. Debole; P. Datta; R. Alvarez-Icaza; A. Amir; J. V. Arthur; A. Andreopoulos; R. Appuswamy; H. Baier; D. Barch; D. J. Berg; C. d. Nolfo; S. K. Esser; M. Flickner; T. A. Horvath; B. L. Jackson; J. Kusnitz; S. Lekuch; M. Mastro; T. Melano; P. A. Merolla; S. E. Millman; T. K. Nayak; N. Pass; H. E. Penner; W. P. Risk; K. Schleupen; B. Shaw; H. Wu; B. Giera; A. T. Moody; N. Mundhenk; B. C. V. Essen; E. X. Wang; D. P. Widemann; Q. Wu; W. E. Murphy; J. K. Infantolino; J. A. Ross; D. R. Shires; M. M. Vindiola; R. Namburu; D. S. Modha,"IBM Res., Yorktown Heights, NY, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,130,141,"This paper describes the hardware and software ecosystem encompassing the brain-inspired TrueNorth processor - a 70mW reconfigurable silicon chip with 1 million neurons, 256 million synapses, and 4096 parallel and distributed neural cores. For systems, we present a scale-out system loosely coupling 16 single-chip boards and a scale-up system tightly integrating 16 chips in a 4 ÌÑ 4 configuration by exploiting TrueNorth's native tiling. For software, we present an end-to-end ecosystem consisting of a simulator, a programming language, an integrated programming environment, a library of algorithms and applications, firmware, tools for deep learning, a teaching curriculum, and cloud enablement. For the scale-up systems we summarize our approach to physical placement of neural network, to reduce intra- and inter-chip network traffic. The ecosystem is in use at over 30 universities and government/corporate labs. Our platform is a substrate for a spectrum of applications from mobile and embedded computing to cloud and supercomputers.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.11,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877010,,Biological neural networks;Computer architecture;Ecosystems;Hardware;Neurons;Software;Training,cloud computing;firmware;learning (artificial intelligence);mobile computing;multiprocessing systems;neural nets;parallel machines;programming languages;teaching,16 single-chip boards;TrueNorth ecosystem;TrueNorth processor;brain-inspired computing;cloud computing;deep learning;embedded computing;firmware;hardware ecosystem;mobile computing;neural network;programming language;software ecosystem;supercomputer;teaching curriculum,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Towards Green Aviation with Python at Petascale,P. Vincent; F. Witherden; B. Vermeire; J. S. Park; A. Iyer,"Dept. of Aeronaut., Imperial Coll. London, London, UK","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,1,11,"Accurate simulation of unsteady turbulent flow is critical for improved design of greener aircraft that are quieter and more fuel-efficient. We demonstrate application of PyFR, a Python based computational fluid dynamics solver, to petascale simulation of such flow problems. Rationale behind algorithmic choices, which offer increased levels of accuracy and enable sustained computation at up to 58% of peak DP-FLOP/s on unstructured grids, will be discussed in the context of modern hardware. A range of software innovations will also be detailed, including use of runtime code generation, which enables PyFR to efficiently target multiple platforms, including heterogeneous systems, via a single implementation. Finally, results will be presented from a fullscale simulation of flow over a low-pressure turbine blade cascade, along with weak/strong scaling statistics from the Piz Daint and Titan supercomputers, and performance data demonstrating sustained computation at up to 13.7 DP-PFLOP/s.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.1,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7876999,,Aerodynamics;Aircraft;Bandwidth;Blades;Computational modeling;Hardware;Turbines,aerospace computing;aircraft;blades;computational fluid dynamics;flow simulation;fuel economy;green computing;high level languages;parallel machines;program compilers;turbines;turbulence,DP-FLOP;Piz Daint supercomputers;PyFR;Python based computational fluid dynamics solver;Titan supercomputers;fuel-efficiency;full-scale flow simulation;green aviation;greener aircraft design;heterogeneous systems;low-pressure turbine blade cascade;performance data;petascale flow simulation;runtime code generation;software innovations;strong scaling statistics;unsteady turbulent flow simulation;unstruc- tured grids;weak scaling statistics,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
A Domain-Specific Compiler for a Parallel Multiresolution Adaptive Numerical Simulation Environment,S. Rajbhandari; J. Kim; S. Krishnamoorthy; L. N. Pouchet; F. Rastello; R. J. Harrison; P. Sadayappan,"Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,468,479,"This paper describes the design and implementation of a layered domain-specific compiler to support MADNESS-Multiresolution ADaptive Numerical Environment for Scientific Simulation. MADNESS is a high-level software environment for the solution of integral and differential equations in many dimensions, using adaptive and fast harmonic analysis methods with guaranteed precision. MADNESS uses k-d trees to represent spatial functions and implements operators like addition, multiplication, differentiation, and integration on the numerical representation of functions. The MADNESS runtime system provides global namespace support and a task-based execution model including futures. MADNESS is currently deployed on massively parallel supercomputers and has enabled many science advances. Due to the highly irregular and statically unpredictable structure of the k-d trees representing the spatial functions encountered in MADNESS applications, only purely runtime approaches to optimization have previously been implemented in the MADNESS framework. This paper describes a layered domain-specific compiler developed to address some performance bottlenecks in MADNESS. The newly developed static compile-time optimizations, in conjunction with the MADNESS runtime support, enable significant performance improvement for the MADNESS framework.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.39,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877118,,C++ languages;DSL;Mathematical model;Optimization;Runtime;Schedules;Synchronization,differential equations;digital simulation;harmonic analysis;integral equations;mathematics computing;numerical analysis;program compilers;software engineering;trees (mathematics),MADNESS;adaptive harmonic analysis;differential equations;global namespace support;high-level software environment;integral equations;k-d trees;layered domain-specific compiler;massively parallel supercomputers;multiresolution adaptive numerical environment for scientific simulation;parallel multiresolution adaptive numerical simulation environment;spatial functions;static compile-time optimizations;task-based execution model,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Real-Time Synthesis of Compression Algorithms for Scientific Data,M. Burtscher; H. Mukka; A. Yang; F. Hesaaraki,"Dept. of Comput. Sci., Texas State Univ., San Marcos, TX, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,264,275,"Many scientific programs produce large amounts of floating-point data that are saved for later use. To minimize the storage requirement, it is worthwhile to compress such data as much as possible. However, existing algorithms tend to com-press floating-point data relatively poorly. As a remedy, we have developed FPcrush, a tool that automatically synthesizes an optimized compressor for each given input. The synthesized algorithms are lossless and parallelized using OpenMP. This paper describes how FPcrush is able to perform this synthesis in real-time, i.e., even when accounting for the synthesis overhead, it compresses the 16 tested real-world single- and double-precision data files more quickly than parallel bzip2. Decompression is an order of magnitude faster and exceeds the throughput of multi-core implementations of bzip2, gzip, and FPC. On all but two of the tested files, as well as on average, the customized algorithms deliver higher compression ratios than the other three tools.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.22,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877101,data compression;real-time algorithm synthesis,Approximation algorithms;Compression algorithms;Data compression;Data models;Genetic algorithms;Throughput;Transforms,data compression;real-time systems,FPcrush;OpenMP;compression algorithms;floating-point data;real-time synthesis;scientific data;storage requirement,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
A PCIe Congestion-Aware Performance Model for Densely Populated Accelerator Servers,M. Martinasso; G. Kwasniewski; S. R. Alam; T. C. Schulthess; T. Hoefler,"Swiss Nat. Supercomput. Centre, ETH Zurich, Lugano, Switzerland","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,739,749,"MeteoSwiss, the Swiss national weather forecast institute, has selected densely populated accelerator servers as their primary system to compute weather forecast simulation. Servers with multiple accelerator devices that are primarily connected by a PCI-Express (PCIe) network achieve a significantly higher energy efficiency. Memory transfers between accelerators in such a system are subjected to PCIe arbitration policies. In this paper, we study the impact of PCIe topology and develop a congestion-aware performance model for PCIe communication. We present an algorithm for computing congestion factors of every communication in a congestion graph that characterizes the dynamic usage of network resources by an application. Our model applies to any PCIe tree topology. Our validation results on two different topologies of 8 GPU devices demonstrate that our model achieves an accuracy of over 97% within the PCIe network. We demonstrate the model on a weather forecast application to identify the best algorithms for its communication patterns among GPUs.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.62,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877141,Multiple GPUs;PCI-Express;performance model,Bandwidth;Computational modeling;Graphics processing units;Ports (Computers);Predictive models;Switches;Topology,digital simulation;energy conservation;geophysics computing;graph theory;peripheral interfaces;weather forecasting,GPU devices;MeteoSwiss;PCI-Express;PCIe arbitration policies;PCIe communication;PCIe congestion-aware performance model;PCIe network;PCIe tree topology;Swiss national weather forecast institute;accelerator devices;congestion graph;densely populated accelerator servers;energy efficiency;memory transfers;network resources;peripheral component interconnect express;weather forecast simulation,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Designing Scalable b-MATCHING Algorithms on Distributed Memory Multiprocessors by Approximation,A. Khan; A. Pothen; M. M. A. Patwary; M. Halappanavar; N. R. Satish; N. Sundaram; P. Dubey,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,773,783,"A b-MATCHING is a subset of edges M such that at most b(v) edges in M are incident on each vertex v, where b(v) is specified. We present a distributed-memory parallel algorithm, b-SUITOR, that computes a b-MATCHING with more than half the maximum weight in a graph with weights on the edges. The approximation algorithm is designed to have high concurrency and low time complexity. We organize the implementation of the algorithm in terms of asynchronous supersteps that combine computation and communication, and balance the computational work and frequency of communication to obtain high performance. Since the performance of the b-SUITOR algorithm is strongly influenced by communication, we present several strategies to reduce the communication volume. We implement the algorithm using a hybrid strategy where inter-node communication uses MPI and intra-node computation is done with OpenMP threads. We demonstrate strong and weak scaling of b-SUITOR up to 16K processors on two supercomputers at NERSC. We compute a b-MATCHING in a graph with 2 billion edges in under 4 seconds using 16K processors.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.65,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877144,,Algorithm design and analysis;Approximation algorithms;Concurrent computing;Greedy algorithms;Parallel algorithms;Program processors;Time complexity,approximation theory;graph theory;message passing;multiprocessing programs;parallel algorithms,MPI;OpenMP threads;approximation algorithm;b-suitor algorithm;distributed memory multiprocessors;distributed memory parallel algorithm;graph;high performance computing;intra-node computation;scalable b-matching algorithm,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
An Efficient and Scalable Algorithmic Method for Generating Large-Scale Random Graphs,M. Alam; M. Khan; A. Vullikanti; M. Marathe,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,372,383,"Many real-world systems and networks are modeled and analyzed using various random graph models. These models must incorporate relevant properties such as degree distribution and clustering coefficient. Many models, such as the Chung-Lu (CL), stochastic Kronecker, stochastic block model (SBM), and block two-level Erdoíös-Reínyi (BTER) models have been devised to capture those properties. However, the generative algorithms for these models are mostly sequential and take prohibitively long time to generate large-scale graphs. In this paper, we present a novel time and space efficient algorithmic method to generate random graphs using CL, BTER, and SBM models. First, we present an efficient sequential algorithm and an efficient distributed-memory parallel algorithm for the CL model. Our sequential algorithm takes O(m) time and O(ëÝ) space, where m and ëÝ are the number of edges and distinct degrees, and our parallel algorithm takes O (m/p + ëÝ + P) time w.h.p. and O(ëÝ) space using P processors. These algorithms are almost time optimal since any sequential and parallel algorithms need at least ë©(m) and ë©(m/p) time, respectively. Our algorithms outperform the best known previous algorithms by a significant margin in terms of both time and space. Experimental results on various large-scale networks show that both of our sequential and parallel algorithms require 400-15000 times less memory than the existing sequential and parallel algorithms, respectively, making our algorithms suitable for generating very large-scale networks. Moreover, both of our algorithms are about 3-4 times faster than the existing sequential and parallel algorithms. Finally, we show how our algorithmic method also leads to efficient parallel and sequential algorithms for the SBM and BTER models.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.31,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877110,distributed computing;network theory;parallel programming;random graphs,Approximation algorithms;Biological system modeling;Computational modeling;Memory management;Parallel algorithms;Program processors;Stochastic processes,graph theory;large-scale systems;parallel algorithms,distributed-memory parallel algorithm;generative algorithms;incorporate relevant properties;large-scale random graphs;scalable algorithmic method;sequential algorithm;space efficient algorithmic method,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Scalable Non-blocking Preconditioned Conjugate Gradient Methods,P. R. Eller; W. Gropp,"Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,204,215,"The preconditioned conjugate gradient method (PCG) is a popular method for solving linear systems at scale. PCG requires frequent blocking allreduce collective operations that can limit performance at scale. We investigate PCG variations designed to reduce communication costs by decreasing the number of allreduces and by overlapping communication with computation using a non-blocking allreduce. These variations include two methods we have developed, non-blocking PCG and 2-step pipelined PCG, and pipelined PCG from Ghysels and Vanroose. Performance modeling for communication and computation costs shows the expected performance of these methods. Weak and strong scaling experiments on up to 128k cores show that scalable PCG methods can outperform standard PCG at scale. We observe that the fastest method varies depending on the work per core, suggesting we need a suite of scalable solvers to obtain the best performance. Experiments with multiple preconditioners and linear systems show the robustness of these methods.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.17,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877096,,Approximation algorithms;Convergence;Gradient methods;Jacobian matrices;Kernel;Sparse matrices;Synchronization,conjugate gradient methods;digital arithmetic;parallel processing,PCG;communication costs;frequent blocking allreduce collective operations;linear systems;nonblocking allreduce;scalable nonblocking preconditioned conjugate gradient methods;scalable solvers,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Server-Side Log Data Analytics for I/O Workload Characterization and Coordination on Large Shared Storage Systems,Y. Liu; R. Gunasekaran; X. Ma; S. S. Vazhkudai,"North Carolina State Univ., Raleigh, NC, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,819,829,"Inter-application I/O contention and performance interference have been recognized as severe problems. In this work, we demonstrate, through measurement from Titan (world's No. 3 supercomputer), that high I/O variance co-exists with the fact that individual storage units remain under-utilized for the majority of the time. This motivates us to propose AID, a system that performs automatic application I/O characterization and I/O-aware job scheduling. AID analyzes existing I/O traffic and batch job history logs, without any prior knowledge on applications or user/developer involvement. It identifies the small set of I/O-intensive candidates among all applications running on a supercomputer and subsequently mines their I/O patterns, using more detailed per-I/O-node traffic logs. Based on such auto-extracted information, AID provides online I/O-aware scheduling recommendations to steer I/O-intensive applications away from heavy ongoing I/O activities. We evaluate AID on Titan, using both real applications (with extracted I/O patterns validated by contacting users) and our own pseudo-applications. Our results confirm that AID is able to (1) identify I/O-intensive applications and their detailed I/O characteristics, and (2) significantly reduce these applications' I/O performance degradation/variance by jointly evaluating outstanding applications' I/O pattern and real-time system l/O load.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.69,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877148,,Aggregates;History;Interference;Monitoring;Servers;Supercomputers;Throughput,data analysis;input-output programs;parallel processing;processor scheduling;shared memory systems,AID;IO traffic;IO workload characterization;IO-aware job scheduling;Titan;auto-extracted information;interapplication IO contention;online IO-aware scheduling recommendations;per-IO-node traffic logs;performance interference;pseudo-applications;server-side log data analytics;shared storage systems;storage units,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Enhancing InfiniBand with OpenFlow-Style SDN Capability,J. Lee; Z. Tong; K. Achalkar; X. Yuan; M. Lang,"Dept. of Comput. Sci., Florida State Univ., Tallahassee, FL, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,421,432,"InfiniBand is the de facto networking technology for commodity HPC clusters and has been widely deployed. However, most production large-scale InfiniBand clusters use simple routing schemes such as the destination-mod-k routing to route traffic, which may result in degraded communication performance. In this work, we investigate using the OpenFlow-style Software-Defined Networking (SDN) technology to overcome the routing deficiency in InfiniBand. We design an enhanced InfiniBand with OpenFlow-style SDN capability and demonstrate a use case that illustrates how the SDN capability can be exploited in HPC clusters to improve the system and application performance. Finally, we quantify the potential benefits of InfiniBand with OpenFlow-style SDN capability in balancing the network load by simulating job traces from production HPC clusters. The results indicate that InfiniBand with SDN capability can achieve much better network load balancing than traditional InfiniBand for HPC clusters.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.35,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877114,Fat Tree;High Performance Computing;InfiniBand;OpenFlow;Simulation;Software Defined Networking,Adaptive systems;Production;Protocols;Resource management;Routing;Standards;Switches,large-scale systems;parallel processing;resource allocation;software defined networking,OpenFlow-style SDN capability;commodity HPC clusters;de facto networking technology;destination-mod-k routing;network load balancing;production large-scale InfiniBand clusters;routing schemes;software-defined networking,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Daino: A High-Level Framework for Parallel and Efficient AMR on GPUs,M. Wahib; N. Maruyama; T. Aoki,"RIKEN Adv. Inst. for Comput. Sci., CREST Kobe, Kobe, Japan","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,621,632,"Adaptive Mesh Refinement methods reduce computational requirements of problems by increasing resolution for only areas of interest. However, in practice, efficient AMR implementations are difficult considering that the mesh hierarchy management must be optimized for the underlying hardware. Architecture complexity of GPUs can render efficient AMR to be particularity challenging in GPU-accelerated supercomputers. This paper presents a compiler-based high-level framework that can automatically transform serial uniform mesh code annotated by the user into parallel adaptive mesh code optimized for GPU-accelerated supercomputers. We also present a method for empirical analysis of a uniform mesh to project an upper-bound on achievable speedup of a GPU-optimized AMR code. We show experimental results on three production applications. The speedups of code generated by our framework are comparable to hand-written AMR code while achieving good and weak scaling up to 1000 GPUs.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.52,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877131,Accelerator processing;Adaptive mesh refinement;Parallel programming;Performance analysis,Adaptation models;Complexity theory;Graphics processing units;Memory management;Octrees;Programming,computational complexity;mesh generation;parallel algorithms;parallel machines;parallelising compilers,Daino high-level framework;GPU architecture complexity;GPU-accelerated supercomputer;GPU-optimized AMR code;adaptive mesh refinement method;automatic serial uniform mesh code transformation;code generation;compiler-based high-level framework;computational requirements;mesh hierarchy management optimization;parallel AMR;parallel adaptive mesh code optimization;weak scaling,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
TÌ_r: Blob Storage Meets Built-In Transactions,P. Matri; A. Costan; G. Antoniu; J. Montes; M. S. PÌ©rez,"Ontology Eng. Group, Univ. Politec. de Madrid, Madrid, Spain","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,573,584,"Concurrent Big Data applications often require high-performance storage, as well as ACID (Atomicity, Consistency, Isolation, Durability) transaction support. Although blobs (binary large objects) are an increasingly popular storage model for such applications, state-of-the-art blob storage systems offer no transaction semantics. This demands users to coordinate data access carefully in order to avoid race conditions, inconsistent writes, overwrites and other problems that cause erratic behavior. We argue there is a gap between existing storage solutions and application requirements, which limits the design of transaction-oriented applications. We introduce Tyír, the first blob storage system to provide built-in, multiblob transactions, while retaining sequential consistency and high throughput under heavy access concurrency. Tyír offers fine-grained random write access to data and in-place atomic operations. Large-scale experiments with a production application from CERN LHC show Tyír throughput outperforming state-of-the-art solutions by more than 75%.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.48,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877127,,Distributed databases;Generators;Metadata;Protocols;Servers;Synchronization;Throughput,Big Data;concurrency control;storage management;transaction processing,ACID;Tyír;atomicity consistency isolation durability transaction support;binary large objects;blob storage system;built-in transactions;concurrent Big Data;data access;high-performance storage;multiblob transactions;random write access;storage model;transaction-oriented applications,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Characterizing Parallel Scientific Applications on Commodity Clusters: An Empirical Study of a Tapered Fat-Tree,E. A. LeÌ_n; I. Karlin; A. Bhatele; S. H. Langer; C. Chambreau; L. H. Howell; T. D'Hooge; M. L. Leininger,"Lawrence Livermore Nat. Lab., Livermore, CA, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,909,920,"Understanding the characteristics and requirements of applications that run on commodity clusters is key to properly configuring current machines and, more importantly, procuring future systems effectively. There are only a few studies, however, that are current and characterize realistic workloads. For HPC practitioners and researchers, this limits our ability to design solutions that will have an impact on real systems. We present a systematic study that characterizes applications with an emphasis on communication requirements. It includes cluster utilization data, identifying a representative set of applications from a U.S. Department of Energy laboratory, and characterizing their communication requirements. The driver for this work is understanding application sensitivity to a tapered fat-tree network. These results provided key insights into the procurement of our next generation commodity systems. We believe this investigation can provide valuable input to the HPC community in terms of workload characterization and requirements from a large supercomputing center.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.77,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877156,High performance computing;computer performance;high-speed networks;network topology;scientific computing,Bandwidth;Hardware;Next generation networking;Procurement;Production;Sensitivity;Throughput,parallel processing;pattern clustering;trees (mathematics),HPC practitioners;HPC researchers;U.S. department of energy laboratory;application representative set;commodity clusters;communication requirements;next generation commodity systems;parallel scientific application characterization;supercomputing center;tapered fat-tree network;utilization data clustering;workload characterization,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
FlipBack: Automatic Targeted Protection against Silent Data Corruption,X. Ni; L. V. Kale,"T.J. Watson Res. Center, IBM, Yorktown Heights, NY, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,335,346,"The decreasing size of transistors has been critical to the increase in capacity of supercomputers. The smaller the transistors are, less energy is required to flip a bit, and thus silent data corruptions (SDCs) become more common. In this paper, we present FlipBack, an automatic software-based approach that protects applications from SDCs. FlipBack provides targeted protection for different types of data and calculations based on their characteristics. It leverages compile-time analysis and program markup to identify data critical for control flow and enables selective replication of computation by the runtime system. We evaluate FlipBack with various HPC mini-applications that capture the behavior of real scientific applications and show that FlipBack is able to protect applications from most silent data corruptions with only 6-20% performance degradation.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.28,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877107,Computer errors;Fault detection;Redundancy;Software reliability,C++ languages;Process control;Redundancy;Runtime;Software;Transient analysis,data flow analysis;data protection;parallel processing;program compilers;software reliability,FlipBack;HPC miniapplications;SDC;automatic software-based approach;automatic targeted protection;compile-time analysis;control flow;performance degradation;program markup;runtime system;scientific applications;selective computation replication;silent data corruptions;supercomputer capacity;transistors,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Merge-Based Parallel Sparse Matrix-Vector Multiplication,D. Merrill; M. Garland,"NVIDIA Corp., Santa Clara, CA, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,678,689,"We present a strictly balanced method for the parallel computation of sparse matrix-vector products (SpMV). Our algorithm operates directly upon the Compressed Sparse Row (CSR) sparse matrix format without preprocessing, inspection, reformatting, or supplemental encoding. Regardless of nonzero structure, our equitable 2D merge-based decomposition tightly bounds the workload assigned to each processing element. Furthermore, our technique is suitable for recursively partitioning CSR datasets themselves into multi-scale, distributed, NUMA, and GPU environments that are constrained by fixed-size local memories. We evaluate our method on both CPU and GPU microarchitectures across a very large corpus of diverse sparse matrix datasets. We show that traditional CsrMV methods are inconsistent performers, often subject to order-of-magnitude performance variation across similarly-sized datasets. In comparison, our method provides predictable performance that is substantially uncorrelated to the distribution of nonzeros among rows and broadly improves upon that of current CsrMV methods.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.57,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877136,GPU;SpMV;linear algebra;many-core;merge-path;parallel merge;sparse matrix,Encoding;Graphics processing units;Matrix decomposition;Parallel processing;Partitioning algorithms;Runtime;Sparse matrices,distributed memory systems;matrix decomposition;matrix multiplication;parallel algorithms;sparse matrices;vector processor systems,2D merge-based decomposition;CPU microarchitectures;CSR datasets;GPU environments;GPU microarchitectures;NUMA environments;SpMV;compressed sparse row;distributed environments;fixed-size local memories;many-core processor;merge-based parallel sparse matrix-vector multiplication;multiscale environments;parallel computation;recursive partitioning;sparse matrix-vector products,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
The Vectorization of the Tersoff Multi-body Potential: An Exercise in Performance Portability,M. HÌ¦hnerbach; A. E. Ismail; A. E. Ismail,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,69,81,"Molecular dynamics simulations, an indispensable research tool in computational chemistry and materials science, consume a significant portion of the supercomputing cycles around the world. We focus on multi-body potentials and aim at achieving performance portability. Compared with well-studied pair potentials, multibody potentials deliver increased simulation accuracy but are too complex for effective compiler optimization. Because of this, achieving cross-platform performance remains an open question. By abstracting from target architecture and computing precision, we develop a vectorization scheme applicable to both CPUs and accelerators. We present results for the Tersoff potential within the molecular dynamics code LAMMPS on several architectures, demonstrating efficiency gains not only for computational kernels, but also for large-scale simulations. On a cluster of Intel Xeon Phi's, our optimized solver is between 3 and 5 times faster than the pure MPI reference.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.6,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877005,,Architecture;Computational modeling;Computer architecture;Graphics processing units;Kernel;Nickel;Optimization,chemistry computing;program compilers,Intel Xeon Phi;LAMMPS molecular dynamics code;Tersoff multibody potential;compiler optimization;computational chemistry;materials science;molecular dynamics simulations;multibody potentials;pair potentials;performance portability,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
LIBXSMM: Accelerating Small Matrix Multiplications by Runtime Code Generation,A. Heinecke; G. Henry; M. Hutchinson; H. Pabst,"Intel Corp., Santa Clara, CA, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,981,991,"Many modern highly scalable scientific simulations packages rely on small matrix multiplications as their main computational engine. Math libraries or compilers are unlikely to provide the best possible kernel performance. To address this issue, we present a library which provides high performance small matrix multiplications targeting all recent x86 vector instruction set extensions up to Intel AVX-512. Our evaluation proves that speed-ups of more than 10ÌÑ are possible depending on the CPU and application. These speed-ups are achieved by a combination of several novel technologies. We use a code generator which has a built-in architectural model to create code which runs well without requiring an auto-tuning phase. Since such code is very specialized we leverage just-in-time compilation to only build the required kernel variant at runtime. To keep ease-of-use, overhead, and kernel management under control we accompany our library with a BLAS-compliant frontend which features a multi-level code-cache hierarchy.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.83,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877162,Block CSR;FEM;JIT compilation;SEM;Small GEMM;code generation,Algorithms;Indexes;Kernel;Libraries;Runtime;Sparse matrices,cache storage;digital simulation;instruction sets;matrix multiplication;natural sciences computing;program compilers,BLAS-compliant frontend;Intel AVX-512;LIBXSMM;auto-tuning phase;computational engine;kernel management;kernel performance;multilevel code-cache hierarchy;runtime code generation;scalable scientific simulations packages;small matrix multiplications;x86 vector instruction set extensions,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Development Effort Estimation in HPC,S. Wienke; J. Miller; M. Schulz; M. S. MÌ_ller,"IT Center, RWTH Aachen Univ., Aachen, Germany","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,107,118,"In order to cover the ever increasing demands for computational power, while meeting electrical power and budget constraints, HPC systems are continuing to increase in hardware and software complexity. As a direct consequence, this also leads to increased development efforts to parallelize, tune or port applications. For an informed decision on how to spend available budgets, we therefore need quantitative metrics to estimate the development effort in HPC. While development effort estimation is widely used in software engineering, applying it to HPC, with its strong focus on performance, is not straightforward. In this paper, we first review existing approaches of effort estimation for general computing and then derive a novel methodology to estimate development effort specifically targeted at HPC. Further, we propose a concept to identify factors impacting development effort and encapsulate it in an effort log tool to collect data on development time.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.9,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877008,,Biological system modeling;Complexity theory;Estimation;Measurement;Parallel programming;Software;Tuning,parallel processing;software engineering,HPC development effort estimation;general computing;hardware complexity;parallel applications;software complexity;software engineering cost model,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Table of contents,,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,iii,xii,Presents the table of contents/splash page of the proceedings record.,,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.92,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7876997,,,,,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
The Mont-Blanc Prototype: An Alternative Approach for HPC Systems,N. Rajovic; A. Rico; F. Mantovani; D. Ruiz; J. O. Vilarrubi; C. Gomez; L. Backes; D. Nieto; H. Servat; X. Martorell; J. Labarta; E. Ayguade; C. Adeniyi-Jones; S. Derradji; H. Gloaguen; P. Lanucara; N. Sanna; J. F. Mehaut; K. Pouget; B. Videau; E. Boyer; M. Allalen; A. Auweter; D. Brayford; D. Tafani; V. Weinberg; D. BrÌ¦mmel; R. Halver; J. H. Meinke; R. Beivide; M. Benito; E. Vallejo; M. Valero; A. Ramirez,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,444,455,"High-performance computing (HPC) is recognized as one of the pillars for further progress in science, industry, medicine, and education. Current HPC systems are being developed to overcome emerging architectural challenges in order to reach Exascale level of performance, projected for the year 2020. The much larger embedded and mobile market allows for rapid development of intellectual property (IP) blocks and provides more flexibility in designing an application-specific system-on-chip (SoC), in turn providing the possibility in balancing performance, energy-efficiency, and cost. In the Mont-Blanc project, we advocate for HPC systems being built from such commodity IP blocks, currently used in embedded and mobile SoCs. As a first demonstrator of such an approach, we present the Mont-Blanc prototype; the first HPC system built with commodity SoCs, memories, and network interface cards (NICs) from the embedded and mobile domain, and off-the-shelf HPC networking, storage, cooling, and integration solutions. We present the system's architecture and evaluate both performance and energy efficiency. Further, we compare the system's abilities against a production level supercomputer. At the end, we discuss parallel scalability and estimate the maximum scalability point of this approach across a set of applications.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.37,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877116,,Blades;Computer architecture;Graphics processing units;Mobile communication;Monitoring;Prototypes;Switches,embedded systems;industrial property;market opportunities;mobile computing;parallel processing;software architecture;system-on-chip,HPC systems;Mont-Blanc prototype;NIC;application-specific system-on-chip;architectural challenge;commodity IP blocks;embedded SoC;embedded market;high-performance computing;intellectual property;mobile SoC;mobile market;network interface cards;production level supercomputer;rapid development,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Flexfly: Enabling a Reconfigurable Dragonfly through Silicon Photonics,K. Wen; P. Samadi; S. Rumley; C. P. Chen; Y. Shen; M. Bahadori; K. Bergman; J. Wilke,"Columbia Univ., New York, NY, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,166,177,"The Dragonfly topology provides low-diameter connectivity for high-performance computing with all-to-all global links at the inter-group level. Our traffic matrix characterization of various scientific applications shows consistent mismatch between the imbalanced group-to-group traffic and the uniform global bandwidth allocation of Dragonfly. Though adaptive routing has been proposed to utilize bandwidth of non-minimal paths, increased hops and cross-group interference lower efficiency. This work presents a photonic architecture, Flexfly, which ‰ÛÏtrades‰Û global links among groups using low-radix Silicon photonic switches. With transparent optical switching, Flexfly reconfigures the inter-group topology based on traffic pattern, stealing additional direct bandwidth for communication-intensive group pairs. Simulations with applications such as GTC, Nekbone and LULESH show up to 1.8x speedup over Dragonfly paired with UGAL routing, along with halved hop count and latency for cross-group messages. We built a 32-node Flexfly prototype using a Silicon photonic switch connecting four groups and demonstrated 820 ns interconnect reconfiguration time.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.14,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877093,,Bandwidth;Computer architecture;Optical switches;Ports (Computers);Routing;Silicon photonics;Topology,bandwidth allocation;integrated optics;network topology;photonic switching systems;silicon,Flexfly;GTC;LULESH;Nekbone;adaptive routing;all-to-all global links;bandwidth utilization;cross-group interference;dragonfly topology;high-performance computing;imbalanced group-to-group traffic;inter-group topology;low-diameter connectivity;low-radix silicon photonic switches;nonminimal paths;photonic architecture;reconfigurable dragonfly;traffic matrix characterization;traffic pattern;transparent optical switching;uniform global bandwidth allocation,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Understanding Error Propagation in GPGPU Applications,G. Li; K. Pattabiraman; C. Y. Cher; P. Bose,"Univ. of British Columbia, Vancouver, BC, Canada","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,240,251,"GPUs have emerged as general-purpose accelerators in high-performance computing (HPC) and scientific applications. However, the reliability characteristics of GPU applications have not been investigated in depth. While error propagation has been extensively investigated for non-GPU applications, GPU applications have a very different programming model which can have a significant effect on error propagation in them. We perform an empirical study to understand and characterize error propagation in GPU applications. We build a compilerbased fault-injection tool for GPU applications to track error propagation, and define metrics to characterize propagation in GPU applications. We find GPU applications exhibit significant error propagation for some kinds of errors, but not others, and the behaviour is highly application specific. We observe the GPUCPU interaction boundary naturally limits error propagation in these applications compared to traditional non-GPU applications. We also formulate various guidelines for the design of faulttolerance mechanisms in GPU applications based on our results.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.20,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877099,CUDA;Error Propagation;Error Resilience;Fault Injection;GPGPU,Central Processing Unit;Circuit faults;Computational modeling;Graphics processing units;Hardware;Kernel;Programming,graphics processing units;parallel processing;program compilers;software reliability,GPGPU;HPC;compiler-based fault-injection tool;error propagation;general-purpose accelerators;high-performance computing;reliability characteristics,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
ScaleMine: Scalable Parallel Frequent Subgraph Mining in a Single Large Graph,E. Abdelhamid; I. Abdelaziz; P. Kalnis; Z. Khayyat; F. Jamour,"Comput. Electr. & Math. Sci. & Eng. Div., King Abdullah Univ. of Sci. & Technol., Thuwal, Saudi Arabia","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,716,727,"Frequent Subgraph Mining is an essential operation for graph analytics and knowledge extraction. Due to its high computational cost, parallel solutions are necessary. Existing approaches either suffer from load imbalance, or high communication and synchronization overheads. In this paper we propose ScaleMine; a novel parallel frequent subgraph mining system for a single large graph. ScaleMine introduces a novel two-phase approach. The first phase is approximate; it quickly identifies subgraphs that are frequent with high probability, while collecting various statistics. The second phase computes the exact solution by employing the results of the approximation to achieve good load balance; prune the search space; generate efficient execution plans; and guide intra-task parallelism. Our experiments show that ScaleMine scales to 8,192 cores on a Cray XC40 (12ÌÑ more than competitors); supports graphs with one billion edges (10ÌÑ larger than competitors), and is at least an order of magnitude faster than existing solutions.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.60,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877139,,Approximation algorithms;Computational efficiency;Computers;Measurement;Parallel processing;Scalability;Time factors,data mining;graph theory;probability,Cray XC40;ScaleMine;probability;scalable parallel frequent subgraph mining;two-phase approach,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Scheduling-Aware Routing for Supercomputers,J. Domke; T. Hoefler,"Inst. of Comput. Eng., Tech. Univ. Dresden, Dresden, Germany","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,142,153,"The interconnection network has a large influence on total cost, application performance, energy consumption, and overall system efficiency of a supercomputer. Unfortunately, today's routing algorithms do not utilize this important resource most efficiently. We first demonstrate this by defining the dark fiber metric as a measure of unused resource in networks. To improve the utilization, we propose scheduling-aware routing, a new technique that uses the current state of the batch system to determine a new set of network routes and so increases overall system utilization by up to 17.74%. We also show that our proposed routing increases the throughput of communication benchmarks by up to 17.6% on a practical InfiniBand installation. Our routing method is implemented in the standard InfiniBand tool set and can immediately be used to optimize systems. In fact, we are using it to improve the utilization of our production petascale supercomputer for more than one year.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.12,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877011,Computer network management;High performance computing;Routing protocols;Unicast,Indexes;Measurement;Multiprocessor interconnection;Routing;Supercomputers;System recovery;Throughput,computer network management;mainframes;parallel machines;telecommunication network routing,InfiniBand installation;batch system;dark fiber metric;petascale supercomputer;scheduling-aware routing,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Modeling Dilute Solutions Using First-Principles Molecular Dynamics: Computing more than a Million Atoms with over a Million Cores,J. L. Fattebert; D. Osei-Kuffuor; E. W. Draeger; T. Ogitsu; W. D. Krauss,"Lawrence Livermore Nat. Lab., Livermore, CA, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,12,22,"First-Principles Molecular Dynamics (FPMD) methods, although powerful, are notoriously expensive computationally due to the quantum modeling of electrons. Traditional FPMD approaches have typically been limited to a few thousand atoms at most, due to O(N3) or worse solver complexity and the large amount of communication required for highly parallel implementations. Attempts to lower the complexity have often introduced uncontrolled approximations or systematic errors. Using a robust new algorithm, we have developed an O(N) complexity solver for electronic structure problems with fully controllable numerical error. Its minimal use of global communications yields excellent scalability, allowing for very accurate FPMD simulations of more than a million atoms on over a million cores. At these scales, this approach provides multiple orders of magnitude speedup compared to the standard plane-wave approach typically used in condensed matter applications, without sacrificing accuracy. This will open up entire new classes of FPMD simulations, e.g. dilute aqueous solutions.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.88,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877000,,Biological system modeling;Chemistry;Complexity theory;Computational modeling;Discrete Fourier transforms;Mathematical model;Scalability,computational complexity;molecular dynamics method;parallel processing;quantum computing,FPMD;O(N) complexity solver;O(N3);dilute solution modeling;electron quantum modeling;first principles molecular dynamics;highly parallel implementations;systematic errors;uncontrolled approximations;worse solver complexity,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Watch Out for the Bully! Job Interference Study on Dragonfly Network,X. Yang; J. Jenkins; M. Mubarak; R. B. Ross; Z. Lan,"Dept. of Comput. Sci., Illinois Inst. of Technol., Chicago, IL, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,750,760,"High-radix, low-diameter dragonfly networks will be a common choice in next-generation supercomputers. Preliminary studies show that random job placement with adaptive routing should be the rule of thumb to utilize such networks, since it uniformly distributes traffic and alleviates congestion. Nevertheless, in this work we find that while random job placement coupled with adaptive routing is good at load balancing network traffic, it cannot guarantee the best performance for every job. The performance improvement of communication-intensive applications comes at the expense of performance degradation of less intensive ones. We identify this bully behavior and validate its underlying causes with the help of detailed network simulation and real application traces. We further investigate a hybrid contiguous-noncontiguous job placement policy as an alternative. Initial experimentation shows that hybrid job placement aids in reducing the worst-case performance degradation for less communication-intensive applications while retaining the performance of communication-intensive ones.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.63,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877142,,Adaptation models;Adaptive systems;Computational modeling;Degradation;Ports (Computers);Resource management;Routing,digital simulation;parallel machines;resource allocation;telecommunication network routing;telecommunication traffic,adaptive routing;communication-intensive applications;high-radix dragonfly networks;hybrid contiguous-noncontiguous job placement policy;hybrid job placement;job interference study;load balancing network traffic;low-diameter dragonfly networks;network simulation;next-generation supercomputers;random job placement;worst-case performance degradation reduction,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
A Parallel Algorithm for Finding All Pairs ë¼-Mismatch Maximal Common Substrings,S. P. Chockalingam; S. V. Thankachan; S. Aluru,"Dept. of Comput. Sci. & Eng., Indian Inst. of Technol., Mumbai, Mumbai, India","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,784,794,"We present an efficient parallel algorithm for the following problem: Given an input collection D of n sequences of total length N, a length threshold f and a mismatch threshold ë¼, report all ë¼-mismatch maximal common substrings of length at least f over all pairs of strings in D. This problem is motivated by clustering and assembly applications in computational biology, where D is a collection of millions of short DNA sequences. Sequencing errors and massive size of these datasets necessitate efficient parallel approximate sequence matching algorithms. We present a novel distributed memory parallel algorithm that solves this approximate sequence matching problem in O ((N/p log N + occ)log<sup>k</sup> N) expected time and takes only O(log<sup>k+1</sup> N) expected rounds of global communications, under some realistic assumptions, where p is the number of processors and occ is the output size. To our knowledge, this is the first provably sub-quadratic time algorithm for solving this problem. We demonstrate the performance and scalability of our algorithm using large high throughput sequencing data sets.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.66,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877145,,Arrays;Clustering algorithms;Electronic mail;Parallel algorithms;Program processors;Sequential analysis,DNA;bioinformatics;computational complexity;parallel algorithms;pattern clustering;sequences;string matching,ë¼-mismatch maximal common substrings;assembly application;clustering;computational biology;dataset size;distributed memory parallel algorithm;large high throughput sequencing dataset;mismatch threshold;parallel approximate sequence matching algorithm;sequence collection;sequencing error;short DNA sequences;subquadratic time algorithm,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Understanding Performance Interference in Next-Generation HPC Systems,O. H. Mondragon; P. G. Bridges; S. Levy; K. B. Ferreira; P. Widener,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,384,395,"Next-generation systems face a wide range of new potential sources of application interference, including resilience actions, system software adaptation, and in situ analytics programs. In this paper, we present a new model for analyzing the performance of bulk-synchronous HPC applications based on the use of extreme value theory. After validating this model against both synthetic and real applications, the paper then uses both simulation and modeling techniques to profile next-generation interference sources and characterize their behavior and performance impact on a selection of HPC benchmarks, mini-applications, and applications. Lastly, this work shows how the model can be used to understand how current interference mitigation techniques in multi-processors work.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.32,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877111,,Computational modeling;Interference;Maximum likelihood estimation;Predictive models;Random variables;Shape;Stochastic processes,multiprocessing systems;operating systems (computers);parallel processing;program diagnostics;software performance evaluation;statistical analysis,HPC benchmark;OS noise;application interference;behavior characterization;bulk-synchronous HPC application;extreme value theory;in situ analytics program;interference mitigation technique;multiprocessors;next-generation HPC systems;performance analysis;performance impact;performance interference;resilience action;system software adaptation,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Pinpointing Scale-Dependent Integer Overflow Bugs in Large-Scale Parallel Applications,I. Laguna; M. Schulz,"Lawrence Livermore Nat. Lab., Livermore, CA, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,216,227,"We present a technique to pinpoint scale-dependent integer overflow bugs, a class of bugs in large-scale parallel applications that is hard and time-consuming to detect manually. Rather than detecting integer overflows when applications are deployed at large scale, as existing techniques do, our method forecasts these overflows without requiring the application to be run at large scale. Our approach statically identifies integer variables that depend on the scale, and then in a refinement phase, uses data points from small-scale runs to forecast whether variables will actually overflow at large-scale runs. We implement our technique in LLVM and evaluate it on several HPC benchmarks and the MPICH MPI implementation. Our tool finds five instances of previously unknown scale-dependent integer overflow bugs, including one in MPICH, and has few false positives, demonstrating its practical utility.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.18,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877097,Computer errors;MPI;debugging;high-performance computing;integer overflows;static analysis,Computational modeling;Computer bugs;Forecasting;Instruments;Parallel processing;Resource management;Runtime,parallel processing;program compilers;program debugging;program diagnostics,HPC benchmarks;LLVM;MPICH MPI implementation;large-scale parallel applications;scale dependent integer overflow bugs;static analysis,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
DAOS and Friends: A Proposal for an Exascale Storage System,J. Lofstead; I. Jimenez; C. Maltzahn; Q. Koziol; J. Bent; E. Barton,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,585,596,"The DOE Extreme-Scale Technology Acceleration Fast Forward Storage and IO Stack project is going to have significant impact on storage systems design within and beyond the HPC community. With phase two of the project starting, it is an excellent opportunity to explore the complete design and how it will address the needs of extreme scale platforms. This paper examines each layer of the proposed stack in some detail along with cross-cutting topics, such as transactions and metadata management. This paper not only provides a timely summary of important aspects of the design specifications but also captures the underlying reasoning that is not available elsewhere. We encourage the broader community to understand the design, intent, and future directions to foster discussion guiding phase two and the ultimate production storage stack based on this work. An initial performance evaluation of the early prototype implementation is also provided to validate the presented design.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.49,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877128,,Arrays;Big Data;Laboratories;Libraries;Performance evaluation;Prototypes,design;parallel processing;storage management,DAOS;DOE extreme-scale technology;HPC community;IO stack project;acceleration fast forward storage;design specifications;distributed application object storage;exascale storage system,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
"Performance Analysis, Design Considerations, and Applications of Extreme-Scale In Situ Infrastructures",U. Ayachit; A. Bauer; E. P. N. Duque; G. Eisenhauer; N. Ferrier; J. Gu; K. E. Jansen; B. Loring; Z. Lukic; S. Menon; D. Morozov; P. O'Leary; R. Ranjan; M. Rasquin; C. P. Stone; V. Vishwanath; G. H. Weber; B. Whitlock; M. Wolf; K. J. Wu; E. W. Bethel,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,921,932,"A key trend facing extreme-scale computational science is the widening gap between computational and I/O rates, and the challenge that follows is how to best gain insight from simulation data when it is increasingly impractical to save it to persistent storage for subsequent visual exploration and analysis. One approach to this challenge is centered around the idea of in situ processing, where visualization and analysis processing is performed while data is still resident in memory. This paper examines several key design and performance issues related to the idea of in situ processing at extreme scale on modern platforms: scalability, overhead, performance measurement and analysis, comparison and contrast with a traditional post hoc approach, and interfacing with simulation codes. We illustrate these principles in practice with studies, conducted on large-scale HPC platforms, that include a miniapplication and multiple science application codes, one of which demonstrates in situ methods in use at greater than 1M-way concurrency.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.78,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877157,,Analytical models;Computational modeling;Concurrent computing;Data models;Data visualization;Libraries;Scientific computing,concurrency (computers);data analysis;data visualisation;natural sciences computing;parallel processing;performance evaluation;storage management,I/O rates;computational rates;concurrency;data analysis processing;data visualization;design considerations;extreme-scale computational science;extreme-scale in situ infrastructures;in situ processing;large-scale HPC;miniapplication codes;performance analysis;performance measurement;scalability;science application codes;simulation codes;simulation data;visual analysis;visual exploration,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Graph Colouring as a Challenge Problem for Dynamic Graph Processing on Distributed Systems,S. Sallinen; K. Iwabuchi; S. Poudel; M. Gokhale; M. Ripeanu; R. Pearce,"Lawrence Livermore Nat. Lab., Livermore, CA, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,347,358,"An unprecedented growth in data generation is taking place. Data about larger dynamic systems is being accumulated, capturing finer granularity events, and thus processing requirements are increasingly approaching real-time. To keep up, data-analytics pipelines need to be viable at massive scale, and switch away from static, offline scenarios to support fully online analysis of dynamic systems. This paper uses a challenge problem, graph colouring, to explore massive-scale analytics for dynamic graph processing. We present an event-based infrastructure, and a novel, online, distributed graph colouring algorithm. Our implementation for colouring static graphs, used as a performance baseline, is up to an order of magnitude faster than previous results and handles massive graphs with over 257 billion edges. Our framework supports dynamic graph colouring with performance at large scale better than GraphLab's static analysis. Our experience indicates that online solutions are feasible, and can be more efficient than those based on snapshotting.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.29,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877108,,Algorithm design and analysis;Color;Heuristic algorithms;Image color analysis;Image edge detection;Runtime;Training,data analysis;distributed algorithms;graph colouring,GraphLab static analysis;data analytics pipeline;data generation;distributed graph colouring algorithm;distributed system;dynamic graph colouring;dynamic graph processing;event-based infrastructure;massive graph handling;massive-scale analytics;online dynamic system analysis;snapshotting;static graph colouring,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Strassen's Algorithm Reloaded,J. Huang; T. M. Smith; G. M. Henry; R. A. v. d. Geijn,"Dept. of Comput. Sci., Univ. of Texas at Austin, Austin, TX, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,690,701,"We dispel with ‰ÛÏstreet wisdom‰Û regarding the practical implementation of Strassen's algorithm for matrix-matrix multiplication (DGEMM). Conventional wisdom: it is only practical for very large matrices. Our implementation is practical for small matrices. Conventional wisdom: the matrices being multiplied should be relatively square. Our implementation is practical for rank-k updates, where k is relatively small (a shape of importance for libraries like LAPACK). Conventional wisdom: it inherently requires substantial workspace. Our implementation requires no workspace beyond buffers already incorporated into conventional high-performance DGEMM implementations. Conventional wisdom: a Strassen DGEMM interface must pass in workspace. Our implementation requires no such workspace and can be plug-compatible with the standard DGEMM interface. Conventional wisdom: it is hard to demonstrate speedup on multi-core architectures. Our implementation demonstrates speedup over conventional DGEMM even on an Intel<sup>å¨</sup> Xeon Phi‰ã¢ coprocessor1 utilizing 240 threads. We show how a distributed memory matrix-matrix multiplication also benefits from these advances.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.58,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877137,BLAS;Strassen;linear algebra library;matrix multiplication;numerical algorithm;performance model,Approximation algorithms;Computer architecture;Electronic mail;Libraries;Linear algebra;Standards;Switches,computational complexity;coprocessors;distributed memory systems;matrix multiplication,Intel Xeon Phi coprocessor;LAPACK;Strassen DGEMM interface;Strassen algorithm;distributed memory matrix-matrix multiplication;multicore architecture;rank-k update,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Transient Guarantees: Maximizing the Value of Idle Cloud Capacity,S. Shastri; A. Rizk; D. Irwin,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,992,1002,"To prevent rejecting requests, cloud platforms typically provision for their peak demand. Thus, a platform's idle capacity can be significant, as demand varies widely over multiple time scales, e.g., daily and seasonally. To reduce waste, platforms have begun to offer this idle capacity in the form of transient servers, which they may unilaterally revoke, for much lower prices - ~50-90% less - than on-demand servers, which they cannot revoke. However, transient servers' revocation characteristics - their volatility and predictability - influence their performance, since they affect the overhead of fault-tolerance mechanisms applications use to handle revocations. Unfortunately, current cloud platforms offer no guarantees on revocation characteristics, which makes it difficult for users to optimally configure (and correctly value) transient servers. To address the problem, we propose the abstraction of a transient guarantee, which offers probabilistic assurances on revocation characteristics. Transient guarantees have numerous benefits: they increase the performance of transient servers, enable users to optimally use and correctly value them, and permit platforms to control their freedom to revoke them. We present policies for partitioning a variable amount of idle capacity into classes with different transient guarantees to maximize performance and value. We then implement and evaluate these policies on job traces from a production Google cluster. We show that our approach can increase the aggregate revenue from idle server capacity by up to ~6.5ÌÑ compared to existing approaches.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.84,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877163,,Checkpointing;Cloud computing;Fault tolerance;Fault tolerant systems;Measurement;Servers;Transient analysis,cloud computing,Google cluster;cloud platforms;fault-tolerance mechanisms;idle cloud capacity;idle server capacity;predictability;revocation characteristics;transient guarantees;transient servers;volatility,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
MetaMorph: A Library Framework for Interoperable Kernels on Multi- and Many-Core Clusters,A. E. Helal; V. Tech; P. Sathre; W. c. Feng,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,119,129,"To attain scalable performance efficiently, the HPC community expects future exascale systems to consist of multiple nodes, each with different types of hardware accelerators. In addition to GPUs and Intel MICs, additional candidate accelerators include embedded multiprocessors and FPGAs. End users need appropriate tools to efficiently use the available compute resources in such systems, both within a compute node and across compute nodes. As such, we present MetaMorph, a library framework designed to (automatically) extract as much computational capability as possible from HPC systems. Its design centers around three core principles: abstraction, interoperability, and adaptivity. To demonstrate its efficacy, we present a case study that uses the structured grids design pattern, which is heavily used in computational fluid dynamics. We show how MetaMorph significantly reduces the development time, while delivering performance and interoperability across an array of heterogeneous devices, including multicore CPUs, Intel MICs, AMD GPUs, and NVIDIA GPUs.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.10,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877009,CUDA;GPU;MIC;MPI;OpenCL;OpenMP;accelerator-aware MPI;accelerators;exascale;parallel libraries;performance portability;programmability;structured grids,Acceleration;Hardware;Interoperability;Kernel;Libraries;Microwave integrated circuits;Performance evaluation,field programmable gate arrays;graphics processing units;multiprocessing systems;open systems;parallel processing,FPGA;GPU;HPC systems;Intel MICs;MetaMorph;abstraction;adaptivity;embedded multiprocessors;hardware accelerators;interoperability;interoperable kernels;many-core clusters;multicore clusters,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Organizing Committee,,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,xiii,xiii,Provides a listing of current committee members and society officers.,,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.91,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7876998,,,,,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
PIPES: A Language and Compiler for Task-Based Programming on Distributed-Memory Clusters,M. Kong; L. N. Pouchet; P. Sadayappan; V. Sarkar,"Rice Univ., Houston, TX, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,456,467,"Applications running on clusters of shared-memory computers are often implemented using OpenMP+MPI. Productivity can be vastly improved using task-based programming, a paradigm where the user expresses the data and control-flow relations between tasks, offering the runtime maximal freedom to place and schedule tasks. While productivity is increased, high-performance execution remains challenging: the implementation of parallel algorithms typically requires specific task placement and communication strategies to reduce internode communications and exploit data locality. In this work, we present a new macro-dataflow programming environment for distributed-memory clusters, based on the Intel Concurrent Collections (CnC) runtime. Our language extensions let the user define virtual topologies, task mappings, task-centric data placement, task and communication scheduling, etc. We introduce a compiler to automatically generate Intel CnC C++ run-time, with key automatic optimizations including task coarsening and coalescing. We experimentally validate our approach on a variety of scientific computations, demonstrating both productivity and performance.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.38,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877117,Concurrent Collections;Distributed computing;macro-dataflow;polyhedral compilation;task parallelism,C++ languages;Parallel algorithms;Productivity;Programming;Runtime;Tuners,C++ language;concurrency (computers);data flow computing;distributed memory systems;message passing;parallel algorithms;program compilers;scheduling;shared memory systems,Intel CnC C++ run-time;Intel Concurrent Collections runtime;MPI;OpenMP;PIPES;communication scheduling;compiler;control-flow relations;data locality;distributed-memory clusters;internode communications;macrodataflow programming environment;parallel algorithms;shared-memory computers;task coalescing;task coarsening;task communication strategies;task mappings;task placement;task scheduling;task-based programming;task-centric data placement;virtual topologies,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
PFEAST: A High Performance Sparse Eigenvalue Solver Using Distributed-Memory Linear Solvers,J. Kestyn; V. Kalantzis; E. Polizzi; Y. Saad,"Electr. & Comput. Eng. Dept., Univ. of Massachusetts, Amherst, MA, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,178,189,"The FEAST algorithm and eigensolver for interior eigenvalue problems naturally possesses three distinct levels of parallelism. The solver is then suited to exploit modern computer architectures containing many interconnected processors. This paper highlights a recent development within the software package that allows the dominant computational task, solving a set of complex linear systems, to be performed with a distributed memory solver. The software, written with a reverse-communication-interface, can now be interfaced with any generic MPI linear-system solver using a customized data distribution for the eigenvector solutions. This work utilizes two common ‰ÛÏblack-box‰Û distributed memory linear-systems solvers (Cluster-MKL-Pardiso and MUMPS), as well as our own application-specific domain-decomposition MPI solver, for a collection of 3-dimensional finite-element systems. We discuss and analyze how parallel resources can be placed at all three levels simultaneously in order to achieve good scalability and optimal use of the computing platform.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.15,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877094,,Convergence;Eigenvalues and eigenfunctions;Linear systems;Parallel processing;Software algorithms;Software packages,application program interfaces;distributed memory systems;eigenvalues and eigenfunctions;finite element analysis;mathematics computing;message passing;multiprocessor interconnection networks;parallel architectures,3- dimensional finite-element systems;MUMPS;PFEAST;application-specific domain-decomposition MPI solver;black-box distributed memory linear-system solvers;cluster-MKL-Pardiso;computer architectures;computing platform;customized data distribution;eigensolver;eigenvector solutions;generic MPI linear-system solver;high performance sparse eigenvalue solver;interconnected proces- sors;interior eigenvalue problems;parallel resources;reverse-communication-interface;software package,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Simulation and Performance Analysis of the ECMWF Tape Library System,M. MÌ_sker; L. Nagel; T. SÌ_ÌÙ; A. Brinkmann; L. Sorth,"Johannes Gutenberg Univ. Mainz, Mainz, Germany","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,252,263,"Improvements in hardware and software have enabled magnetic disks to become an alternative to tape in backup environments. Nevertheless, even considering its slow access times, tape is still part of most hierarchical storage management strategies. The main reasons are cost-effectiveness, long lifetimes, and that tape, continually improved, keeps up with magnetic disks in terms of capacity. The performance of tape libraries, however, has been scarcely analyzed in the literature. In this paper, we present a tape library simulator and analyze workload traces of the European Centre for Medium-Range Weather Forecasts. Optimizing the cartridge management, we show that the load latency can be reduced by a factor of 2.1 and the number of load operations by 2.5, compared to standard library settings. Furthermore, we present eviction and placement strategies, which can additionally lower the load latency by 20%, achieving the same performance level with considerably fewer drives.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.21,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877100,active archive;latency;robotic tape library;tape library simulation;tertiary storage,Drives;Libraries;Load modeling;Patents;Robots;Weather forecasting,back-up procedures;data handling;environmental science computing;libraries;magnetic tapes;optimisation;storage management;weather forecasting,ECMWF tape library system simulation;European centre for medium-range weather forecasts;backup environments;cartridge management optimization;eviction strategy;hierarchical storage management strategies;magnetic disks;performance analysis;placement strategy;tape alternative;tape library simulator,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Efficient Delaunay Tessellation through K-D Tree Decomposition,D. Morozov; T. Peterka,"Lawrence Berkeley Nat. Lab., Berkeley, CA, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,728,738,"Delaunay tessellations are fundamental data structures in computational geometry. They are important in data analysis, where they can represent the geometry of a point set or approximate its density. The algorithms for computing these tessellations at scale perform poorly when the input data is unbalanced. We investigate the use of k-d trees to evenly distribute points among processes and compare two strategies for picking split points between domain regions. Because resulting point distributions no longer satisfy the assumptions of existing parallel Delaunay algorithms, we develop a new parallel algorithm that adapts to its input and prove its correctness. We evaluate the new algorithm using two late-stage cosmology datasets. The new running times are up to 50 times faster using k-d tree compared with regular grid decomposition. Moreover, in the unbalanced data sets, decomposing the domain into a k-d tree is up to five times faster than decomposing it into a regular grid.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.61,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877140,,Algorithm design and analysis;Approximation algorithms;Clustering algorithms;Computational modeling;Octrees;Parallel algorithms;Three-dimensional displays,computational geometry;data analysis;mesh generation;parallel algorithms;tree data structures,Delaunay tessellation;computational geometry;data analysis;data structures;k-d tree decomposition;late-stage cosmology datasets;parallel algorithm;regular grid decomposition;unbalanced data sets,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Evaluating HPC Networks via Simulation of Parallel Workloads,N. Jain; A. Bhatele; S. White; T. Gamblin; L. V. Kale,"Center for Appl. Sci. Comput., Lawrence Livermore Nat. Lab., Livermore, CA, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,154,165,"This paper presents an evaluation and comparison of three topologies that are popular for building interconnection networks in large-scale supercomputers: torus, fat-tree, and dragonfly. To perform this evaluation, we propose a comprehensive methodology and present a scalable packet-level network simulator, TraceR. Our methodology includes design of prototype systems that are being evaluated, use of proxy applications to determine computation and communication load, simulating individual proxy applications and multi-job workloads, and computing aggregated performance metrics. Using the proposed methodology, prototype systems based on torus, fat-tree, and dragonfly networks with up to 730K endpoints (MPI processes) executed on 46K nodes are compared in the context of multi-job workloads from capability and capacity systems. For the 180 Petaflop/s prototype systems simulated in this paper, we show that different topologies are superior in different scenarios, i.e. there is no single best topology, and the characteristics of parallel workloads determine the optimal choice.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.13,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877012,Computer simulation;High performance computing;Multiprocessor interconnection networks;Network topology;Performance evaluation,Computational modeling;Emulation;Network topology;Prototypes;Routing;Switches;Topology,message passing;multiprocessor interconnection networks;parallel processing,HPC network evaluation;MPI process;TraceR;aggregated performance metrics;capability systems;capacity systems;communication load;computation load;dragonfly interconnection;fat-tree interconnection;interconnection networks;large-scale supercomputers;multijob workload simulation;parallel workload simulation;proxy application simulation;proxy applications;scalable packet-level network simulator;torus interconnection,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Measuring and Understanding Throughput of Network Topologies,S. A. Jyothi; A. Singla; P. B. Godfrey; A. Kolla,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,761,772,"High throughput is of particular interest in data center and HPC networks. Although myriad network topologies have been proposed, a broad head-to-head comparison across topologies and across traffic patterns is absent, and the right way to compare worst-case throughput performance is a subtle problem. In this paper, we develop a framework to benchmark the throughput of network topologies, using a two-pronged approach. First, we study performance on a variety of synthetic and experimentally-measured traffic matrices (TMs). Second, we show how to measure worst-case throughput by generating a near-worst-case TM for any given topology. We apply the framework to study the performance of these TMs in a wide range of network topologies, revealing insights into the performance of topologies with scaling, robustness of performance across TMs, and the effect of scattered workload placement. Our evaluation code is freely available.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.64,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877143,,Benchmark testing;Measurement;Network topology;Robustness;Servers;Throughput;Topology,computer centres;parallel processing;telecommunication computing;telecommunication network topology;telecommunication traffic,HPC networks;TM;data center;network topologies;traffic matrices;two-pronged approach;worst-case throughput,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Accelerating Lattice QCD Multigrid on GPUs Using Fine-Grained Parallelization,M. A. Clark; B. JoÌ_; A. Strelchenko; M. Cheng; A. Gambhir; R. C. Brower,"NVIDIA Corp., Santa Clara, CA, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,795,806,"The past decade has witnessed a dramatic acceleration of lattice quantum chromodynamics calculations in nuclear and particle physics. This has been due to both significant progress in accelerating the iterative linear solvers using multigrid algorithms, and due to the throughput improvements brought by GPUs. Deploying hierarchical algorithms optimally on GPUs is non-trivial owing to the lack of parallelism on the coarse grids, and as such, these advances have not proved multiplicative. Using the QUDA library, we demonstrate that by exposing all sources of parallelism that the underlying stencil problem possesses, and through appropriate mapping of this parallelism to the GPU architecture, we can achieve high efficiency even for the coarsest of grids. Results are presented for the Wilson-Clover discretization, where we demonstrate up to 10x speedup over present state-of-the-art GPU-accelerated methods on Titan. Finally, we look to the future, and consider the software implications of our findings.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.67,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877146,,Acceleration;Color;Electronic mail;Lattices;Parallel processing;Throughput,graphics processing units;iterative methods;lattice theory;parallel processing;quantum chromodynamics;quantum computing,GPU architecture;QUDA library;Titan;Wilson-Clover discretization;coarse grids;fine-grained parallelization;hierarchical algorithms;iterative linear solvers;lattice QCD multigrid;lattice quantum chromodynamics calculations;multigrid algorithms;nuclear physics;particle physics;stencil problem,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Reliable and Efficient Performance Monitoring in Linux,M. Dimakopoulou; S. Eranian; N. Koziris; N. Bambos,"Stanford Univ., Stanford, CA, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,396,408,"Processor hardware performance counters have recently improved in quality and features, while performance monitoring support in Linux has been significantly revamped with the development of the perf_events subsystem, which contributed in making performance analysis an increasingly common practice among developers. However, no performance analysis is possible without an efficient monitoring interface and reliable hardware counter data. In this paper, we first address a reliability issue in the Performance Monitoring Unit of recent Intel processors with Hyper-Threading enabled. A published erratum causes cross hyper-thread hardware counter corruption and may produce unreliable results. We propose a cache-coherence style protocol which we implement in the Linux kernel to address the issue by introducing cross hyper-thread dynamic event scheduling. Second, we improve event scheduling efficiency by introducing an algorithm which optimally schedules events onto hardware counters consistently. The proposed optimizations do not require any user level changes. They leverage the internal design of the perf_events subsystem and have broader applicability in processors. The improvements have been contributed to the upstream Linux kernel 4.1.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.33,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877112,,Hardware;Kernel;Linux;Phasor measurement units;Processor scheduling;Program processors;Radiation detectors,Linux;cache storage;microprocessor chips;multi-threading;operating system kernels;performance evaluation;reliability;scheduling,Intel processor;Linux kernel;Performance Monitoring Unit;cache-coherence style protocol;cross hyper-thread dynamic event scheduling;efficient performance monitoring;event scheduling efficiency;hyper-thread hardware counter corruption;hyper-threading;monitoring interface;optimal event scheduling;perf_events subsystem;performance analysis;processor hardware performance counter;reliability issue,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Translating OpenMP Device Constructs to OpenCL Using Unnecessary Data Transfer Elimination,J. Kim; Y. J. Lee; J. Park; J. Lee,"Dept. of Comput. Sci. & Eng., Seoul Nat. Univ., Seoul, South Korea","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,597,608,"In this paper, we propose a framework that translates OpenMP 4.0 accelerator directives to OpenCL. By translating an OpenMP program to an OpenCL program, the program can be executed on any hardware platform that supports OpenCL. We also propose a run-time optimization technique that automatically eliminates unnecessary data transfers between the host and the target accelerator. It exploits the page-fault mechanism to detect if a copy of the memory object already resides in the accelerator and the copy has not been modified by the host. To evaluate the framework, we develop 17 OpenMP 4.0 benchmark applications in two versions: basic and hand-tuned. By evaluating them on three different GPUs with the original OpenCL and OpenMP programs, we show the effectiveness of the framework. To show the practicality of the framework, we compare the performance of generated OpenCL programs with that of equivalent OpenACC programs compiled by the commercial PGI compiler.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.50,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877129,,Data transfer;Graphics processing units;Optimization;Performance evaluation;Programming;Runtime,application program interfaces;data handling;program compilers;program interpreters,GPU;OpenCL program;OpenMP 4.0 accelerator directives;OpenMP device;OpenMP program;PGI compiler;data transfer elimination;graphics processing unit;hardware platform;page-fault mechanism;program translation;run-time optimization,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
An Exploration of Optimization Algorithms for High Performance Tensor Completion,S. Smith; J. Park; G. Karypis,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,359,371,"Tensor completion is a powerful tool used to estimate or recover missing values in multi-way data. It has seen great success in domains such as product recommendation and healthcare. Tensor completion is most often accomplished via low-rank sparse tensor factorization, a computationally expensive non-convex optimization problem which has only recently been studied in the context of parallel computing. In this work, we study three optimization algorithms that have been successfully applied to tensor completion: alternating least squares (ALS), stochastic gradient descent (SGD), and coordinate descent (CCD++). We explore opportunities for parallelism on shared- and distributed-memory systems and address challenges such as memory- and operation-efficiency, load balance, cache locality, and communication. Among our advancements are an SGD algorithm which combines stratification with asynchronous communication, an ALS algorithm rich in level-3 BLAS routines, and a communication-efficient CCD++ algorithm. We evaluate our optimizations on a variety of real datasets using a modern supercomputer and demonstrate speedups through 1024 cores. These improvements effectively reduce time-to-solution from hours to seconds on real-world datasets. We show that after our optimizations, ALS is advantageous on parallel systems of small-to-moderate scale, while both ALS and CCD++ will provide the lowest time-to-solution on large-scale distributed systems.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.30,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877109,,Complexity theory;Distributed databases;Mathematical model;Optimization;Parallel processing;Prediction algorithms;Tensile stress,cache storage;least squares approximations;matrix decomposition;optimisation;parallel processing;resource allocation;stochastic processes;storage management;tensors,ALS;CCD++;SGD;alternating least squares;cache locality;coordinate descent;distributed memory systems;high performance tensor completion;load balance;optimization algorithms;parallel computing;shared memory systems;sparse tensor factorization;stochastic gradient descent,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Block Iterative Methods and Recycling for Improved Scalability of Linear Solvers,P. Jolivet; P. H. Tournier,"IRIT, ENSEEIHT, France","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,190,203,"Contemporary large-scale Partial Differential Equation (PDE) simulations usually require the solution of large and sparse linear systems. Moreover, it is often needed to solve these linear systems with different or multiple Right-Hand Sides (RHSs). In this paper, various strategies will be presented to extend the scalability of existing multigrid or domain decomposition linear solvers using appropriate recycling strategies or block methods-i.e., by treating multiple right-hand sides simultaneously. The scalability of this work is assessed by performing simulations on up to 8,192 cores for solving linear systems arising from various physical phenomena modeled by Poisson's equation, the system of linear elasticity, or Maxwell's equation. This work is shipped as part of on open-source software, readily available and usable in any C/C++, Python, or Fortran code. In particular, some simulations are performed on top of a well-established library, PETSc, and it is shown how our approaches can be used to decrease time to solution down by 30%.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.16,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877095,Iterative methods;Maxwell's equation;distributed algorithms,Context;Eigenvalues and eigenfunctions;Iterative methods;Libraries;Linear systems;Mathematical model;Recycling,C++ language;Maxwell equations;Poisson equation;distributed algorithms;iterative methods;public domain software,C language;C++ language;Fortran code;Maxwell equation;PDE;Poisson equation;Python language;RHS;block iterative methods;distributed algorithms;improved scalability recycling;linear elasticity system;linear solvers;open source software;partial differential equation;right-hand sides;sparse linear systems,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
An Ephemeral Burst-Buffer File System for Scientific Applications,T. Wang; K. Mohror; A. Moody; K. Sato; W. Yu,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,807,818,"Burst buffers are becoming an indispensable hardware resource on large-scale supercomputers to buffer the bursty I/O from scientific applications. However, there is a lack of software support for burst buffers to be efficiently shared by applications within a batch-submitted job and recycled across different batch jobs. In addition, burst buffers need to cope with a variety of challenging I/O patterns from data-intensive scientific applications. In this study, we have designed an ephemeral Burst Buffer File System (BurstFS) that supports scalable and efficient aggregation of I/O bandwidth from burst buffers while having the same life cycle as a batch-submitted job. BurstFS features several techniques including scalable metadata indexing, co-located I/O delegation, and server-side read clustering and pipelining. Through extensive tuning and analysis, we have validated that BurstFS has accomplished our design objectives, with linear scalability in terms of aggregated I/O bandwidth for parallel writes and reads.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.68,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877147,,Bandwidth;Buffer storage;Distributed databases;File systems;Indexing;Metadata;Pipeline processing,buffer storage;indexing;meta data;parallel processing;pattern clustering,BurstFS;I/O bandwidth aggregation;I/O burst buffers;batch-submitted job;co-located I/O delegation;data-intensive scientific applications;ephemeral burst-buffer file system;hardware resource;large-scale high-performance computing;large-scale supercomputers;metadata indexing scalability;parallel reads;parallel writes;pipelining;server-side read clustering,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Evaluating and Optimizing OpenCL Kernels for High Performance Computing with FPGAs,H. R. Zohouri; N. Maruyamay; A. Smith; M. Matsuda; S. Matsuoka,"Tokyo Inst. of Technol., Tokyo, Japan","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,409,420,"We evaluate the power and performance of the Rodinia benchmark suite using the Altera SDK for OpenCL targeting a Stratix V FPGA against a modern CPU and GPU. We study multiple OpenCL kernels per benchmark, ranging from direct ports of the original GPU implementations to loop-pipelined kernels specifically optimized for FPGAs. Based on our results, we find that even though OpenCL is functionally portable across devices, direct ports of GPU-optimized code do not perform well compared to kernels optimized with FPGA-specific techniques such as sliding windows. However, by exploiting FPGA-specific optimizations, it is possible to achieve up to 3.4x better power efficiency using an Altera Stratix V FPGA in comparison to an NVIDIA K20c GPU, and better run time and power efficiency in comparison to CPU. We also present preliminary results for Arria 10, which, due to hardened FPUs, exhibits noticeably better performance compared to Stratix V in floating-point-intensive benchmarks.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.34,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877113,FPGA;Heterogeneous computing;OpenCL;Performance evaluation,Benchmark testing;Field programmable gate arrays;Graphics processing units;Kernel;Optimization;Performance evaluation;Programming,field programmable gate arrays;floating point arithmetic;graphics processing units;hardware-software codesign;microprocessor chips;operating system kernels;optimisation;parallel processing,Altera SDK;Arria 10;CPU;FPGA-specific optimizations;GPU implementations;GPU-optimized code;NVIDIA K20c GPU;OpenCL kernel optimization;Rodinia benchmark suite;Stratix V FPGA;floating-point-intensive benchmarks;high performance computing;loop-pipelined kernel optimization;power efficiency;sliding windows,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
dCUDA: Hardware Supported Overlap of Computation and Communication,T. Gysi; J. BÌ_r; T. Hoefler,,"SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170316,2016,,,609,620,"Over the last decade, CUDA and the underlying GPU hardware architecture have continuously gained popularity in various high-performance computing application domains such as climate modeling, computational chemistry, or machine learning. Despite this popularity, we lack a single coherent programming model for GPU clusters. We therefore introduce the dCUDA programming model, which implements device-side remote memory access with target notification. To hide instruction pipeline latencies, CUDA programs over-decompose the problem and over-subscribe the device by running many more threads than there are hardware execution units. Whenever a thread stalls, the hardware scheduler immediately proceeds with the execution of another thread ready for execution. This latency hiding technique is key to make best use of the available hardware resources. With dCUDA, we apply latency hiding at cluster scale to automatically overlap computation and communication. Our benchmarks demonstrate perfect overlap for memory bandwidth-bound tasks and good overlap for compute-bound tasks.",,Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0,10.1109/SC.2016.51,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877130,Distributed memory;gpu;latency hiding;programming model;remote memory access,Computational modeling;Graphics processing units;Hardware;Instruction sets;Kernel;Programming,multi-threading;parallel architectures;storage management,GPU clusters;GPU hardware architecture;communication;computation;compute-bound tasks;dCUDA programming model;device-side remote memory access;hardware resources;hardware scheduler;hardware supported overlap;high-performance computing;instruction pipeline latencies;latency hiding;memory bandwidth-bound tasks;threads,,,,,,,13-18 Nov. 2016,,IEEE,IEEE Conference Publications
Enterprise: breadth-first graph traversal on GPUs,H. Liu; H. H. Huang,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"The Breadth-First Search (BFS) algorithm serves as the foundation for many graph-processing applications and analytics workloads. While Graphics Processing Unit (GPU) offers massive parallelism, achieving high-performance BFS on GPUs entails efficient scheduling of a large number of GPU threads and effective utilization of GPU memory hierarchy. In this paper, we present Enterprise, a new GPU-based BFS system that combines three techniques to remove potential performance bottlenecks: (1) streamlined GPU threads scheduling through constructing a frontier queue without contention from concurrent threads, yet containing no duplicated frontiers and optimized for both top-down and bottom-up BFS. (2) GPU workload balancing that classifies the frontiers based on different out-degrees to utilize the full spectrum of GPU parallel granularity, which significantly increases thread-level parallelism; and (3) GPU based BFS direction optimization quantifies the effect of hub vertices on direction-switching and selectively caches a small set of critical hub vertices in the limited GPU shared memory to reduce expensive random data accesses. We have evaluated Enterprise on a large variety of graphs with different GPU devices. Enterprise achieves up to 76 billion traversed edges per second (TEPS) on a single NVIDIA Kepler K40, and up to 122 billion TEPS on two GPUs that ranks No. 45 in the Graph 500 on November 2014. Enterprise is also very energy-efficient as No. 1 in the GreenGraph 500 (small data category), delivering 446 million TEPS per watt.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807594,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832842,,Arrays;Graphics processing units;Hardware;Instruction sets;Optimization;Parallel processing;Switches,graph theory;graphics processing units;parallel processing;performance evaluation;queueing theory;search problems;shared memory systems,BFS;Enterprise;GPU memory hierarchy;GPU parallel granularity;GPU shared memory;GPU threads;GPU workload balancing;Graph 500;GreenGraph 500;NVIDIA Kepler K40;TEPS;analytics workloads;breadth-first graph traversal;breadth-first search algorithm;concurrent threads;direction-switching;frontier queue;graph-processing applications;graphics processing unit;hub vertices;performance bottlenecks;thread-level parallelism;traversed edges per second,,2,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
A work-efficient algorithm for parallel unordered depth-first search,U. A. Acar; A. CharguÌ©raud; M. Rainey,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Advances in processing power and memory technology have made multicore computers an important platform for high-performance graph-search (or graph-traversal) algorithms. Since the introduction of multicore, much progress has been made to improve parallel breadth-first search. However, less attention has been given to algorithms for unordered or loosely ordered traversals. We present a parallel algorithm for unordered depth-first-search on graphs. We prove that the algorithm is work efficient in a realistic algorithmic model that accounts for important scheduling costs. This work-efficiency result applies to all graphs, including those with high diameter and high out-degree vertices. The algorithmic techniques behind this result include a new data structure for representing the frontier of vertices in depth-first search, a new amortization technique for controlling excess parallelism, and an adaptation of the lazy-splitting technique to depth first search. We validate the theoretical results with an implementation and experiments. The experiments show that the algorithm performs well on a range of graphs and that it can lead to significant improvements over comparable algorithms.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807651,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832841,,Algorithm design and analysis;Data structures;Multicore processing;Parallel algorithms;Program processors;Scheduling,data structures;graph theory;multiprocessing systems;parallel algorithms;tree searching,amortization technique;data structure;graph-traversal algorithm;high-performance graph-search;multicore computer;parallel algorithm;parallel unordered depth-first search;work-efficient algorithm,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Adaptive and transparent cache bypassing for GPUs,A. Li; G. J. van den Braak; A. Kumar; H. Corporaal,"Eindhoven Univ. of Technol., Eindhoven, Netherlands","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"In the last decade, GPUs have emerged to be widely adopted for general-purpose applications. To capture on-chip locality for these applications, modern GPUs have integrated multilevel cache hierarchy, in an attempt to reduce the amount and latency of the massive and sometimes irregular memory accesses. However, inferior performance is frequently attained due to serious congestion in the caches results from the huge amount of concurrent threads. In this paper, we propose a novel compile-time framework for adaptive and transparent cache bypassing on GPUs. It uses a simple yet effective approach to control the bypass degree to match the size of applications' runtime footprints. We validate the design on seven GPU platforms that cover all existing GPU generations using 16 applications from widely used GPU benchmarks. Experiments show that our design can significantly mitigate the negative impact due to small cache sizes and improve the overall performance. We analyze the performance across different platforms and applications. We also propose some optimization guidelines on how to efficiently use the GPU caches.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807606,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832791,,Bandwidth;Graphics processing units;Hardware;Prefetching;Registers;Throughput,cache storage;graphics processing units,GPU caches;adaptive cache bypassing;compile-time framework;concurrent threads;general-purpose applications;memory accesses;multilevel cache hierarchy;on-chip locality;optimization guidelines;transparent cache bypassing,,5,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Exploiting asynchrony from exact forward recovery for DUE in iterative solvers,L. Jaulmes; M. Casas; M. MoretÌ_; E. AyguadÌ©; J. Labarta; M. Valero,"Barcelona Supercomput. Center, Spain Univ. Politec. de Catalunya, Barcelona, Spain","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"This paper presents a method to protect iterative solvers from Detected and Uncorrected Errors (DUE) relying on error detection techniques already available in commodity hardware. Detection operates at the memory page level, which enables the use of simple algorithmic redundancies to correct errors. Such redundancies would be inapplicable under coarse grain error detection, but become very powerful when the hardware is able to precisely detect errors. Relations straightforwardly extracted from the solver allow to recover lost data exactly. This method is free of the overheads of backwards recoveries like checkpointing, and does not compromise mathematical convergence properties of the solver as restarting would do. We apply this recovery to three widely used Krylov subspace methods, CG, GMRES and BiCGStab, and their preconditioned versions. We implement our resilience techniques on CG considering scenarios from small (8 cores) to large (1024 cores) scales, and demonstrate very low overheads compared to state-of-the-art solutions. We deploy our recovery techniques either by overlapping them with algorithmic computations or by forcing them to be in the critical path of the application. A trade-off exists between both approaches depending on the error rate the solver is suffering. Under realistic error rates, overlapping decreases overheads from 5.37% down to 3.59% for a non-preconditioned CG on 8 cores.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807599,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832827,,Error analysis;Error correction codes;Hardware;Iterative methods;Program processors;Redundancy;Resilience,checkpointing;iterative methods;parallel processing,BiCGStab;CG;DUE;GMRES;Krylov subspace methods;algorithmic redundancies;checkpointing;coarse grain error detection;detected and uncorrected errors;error detection techniques;exact forward recovery;iterative solvers;mathematical convergence properties;memory page level;realistic error rates;recovery techniques,,4,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Energy-aware data transfer algorithms,I. Alan; E. Arslan; T. Kosar,"Dept. of Comput. Sci. & Eng., Univ. at Buffalo (SUNY), Buffalo, NY, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"The amount of data moved over the Internet per year has already exceeded the Exabyte scale and soon will hit the Zettabyte range. To support this massive amount of data movement across the globe, the networking infrastructure as well as the source and destination nodes consume immense amount of electric power, with an estimated cost measured in billions of dollars. Although considerable amount of research has been done on power management techniques for the networking infrastructure, there has not been much prior work focusing on energy-aware data transfer algorithms for minimizing the power consumed at the end-systems. We introduce novel data transfer algorithms which aim to achieve high data transfer throughput while keeping the energy consumption during the transfers at the minimal levels. Our experimental results show that our energy-aware data transfer algorithms can achieve up to 50% energy savings with the same or higher level of data transfer throughput.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807628,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832818,,Data models;Data transfer;Mathematical model;Pipeline processing;Power demand;Servers;Throughput,Big Data;energy consumption;power aware computing;telecommunication power management,Big Data;electric power;energy aware data transfer algorithms;energy consumption;exabyte scale;networking infrastructure;power management;zettabyte range,,1,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Multi-objective job placement in clusters,S. Blagodurov; A. Fedorova; E. Vinnik; T. Dwyer; F. Hermenier,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"One of the key decisions made by both MapReduce and HPC cluster management frameworks is the placement of jobs within a cluster. To make this decision, they consider factors like resource constraints within a node or the proximity of data to a process. However, they fail to account for the degree of collocation on the cluster's nodes. A tight process placement can create contention for the intra-node shared resources, such as shared caches, memory, disk, or network bandwidth. A loose placement would create less contention, but exacerbate network delays and increase cluster-wide power consumption. Finding the best job placement is challenging, because among many possible placements, we need to find one that gives us an acceptable trade-off between performance and power consumption. We propose to tackle the problem via multi-objective optimization. Our solution is able to balance conflicting objectives specified by the user and efficiently find a suitable job placement.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807636,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832840,,Bandwidth;Containers;Delays;Government;Optimization;Power demand;Virtualization,data handling;optimisation;parallel processing;pattern clustering;power consumption,HPC cluster management frameworks;MapReduce;collocation degree;data proximity;intra-node shared resource contention;multiobjective job placement;multiobjective optimization;power consumption;process placement;resource constraints,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Parallel distributed memory construction of suffix and longest common prefix arrays,P. Flick; S. Aluru,"Georgia Inst. of Technol., Atlanta, GA, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,10,"Suffix arrays and trees are fundamental string data structures of importance to many applications in computational biology. Consequently, their parallel construction is an actively studied problem. To date, algorithms with best practical performance lack efficient worst-case run-time guarantees, and vice versa. In addition, much of the recent work targeted low core count, shared memory parallelization. In this paper, we present parallel algorithms for distributed memory construction of suffix arrays and longest common prefix (LCP) arrays that simultaneously achieve good worst-case run-time bounds and superior practical performance. Our algorithms run in O(Tsort(n, p) åá log n) worst-case time where Tsort(n, p) is the run-time of parallel sorting. We present several algorithm engineering techniques that improve performance in practice. We demonstrate the construction of suffix and LCP arrays of the human genome in less than 8 seconds on 1,024 Intel Xeon cores, reaching speedups of over 110X compared to the best sequential suffix array construction implementation divsufsort.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807609,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832790,,Arrays;Bioinformatics;Genomics;Parallel algorithms;Program processors;Silicon;Sorting,biology computing;data structures;distributed memory systems;parallel processing;trees (mathematics),LCP arrays;computational biology;longest common prefix arrays;parallel distributed memory construction;shared memory parallelization;string data structures;suffix arrays;trees;worst-case run-time bounds,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
<i>C</i><sup>2</sup>-bound: a capacity and concurrency driven analytical model for many-core design,Y. H. Liu; X. H. Sun,"Dept. of Comput. Sci., Illinois Inst. of Technol., Chicago, IL, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,11,"In this paper, we propose C<sup>2</sup>-Bound, a data-driven analytical model, that incorporates both memory capacity and data access concurrency factors to optimize many-core design. C<sup>2</sup>-Bound is characterized by combining the newly proposed latency model, concurrent average memory access time (C-AMAT), with the well-known memory-bounded speedup model (Sun-Ni's law) to facilitate computing tasks. Compared to traditional chip designs that lack the notion of memory concurrency and memory capacity, C<sup>2</sup>-Bound model finds memory bound factors significantly impact the optimal number of cores as well as their optimal silicon area allocations, especially for data-intensive applications with a none parallelizable sequential portion. Therefore, our model is valuable to the design of new generation many-core architectures that target big data processing, where working sets are usually larger than conventional scientific computing. These findings are evidenced by our detailed simulations, which show with C<sup>2</sup>-Bound the design space can be narrowed down significantly up to four orders of magnitude. C<sup>2</sup>-Bound analytic results can be either used in reconfigurable hardware environments or, by software designers, applied to scheduling, partitioning, and allocating resources among diverse applications.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807641,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832822,,Analytical models;Computational modeling;Computer architecture;Concurrent computing;Program processors;Space exploration;System-on-chip,concurrency (computers);multiprocessing systems;parallel architectures;storage management,Big Data processing;C-AMAT;C<sup>2</sup>-bound;Sun-Ni law;capacity driven analytical model;concurrency driven analytical model;concurrent average memory access time;data access concurrency factors;data-driven analytical model;data-intensive applications;latency model;many-core architectures;many-core design;memory bound factors;memory capacity;memory-bounded speedup model;silicon area allocations,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Optimal scheduling of in-situ analysis for large-scale scientific simulations,P. Malakar; V. Vishwanath; T. Munson; C. Knight; M. Hereld; S. Leyffer; M. E. Papka,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,11,"Today's leadership computing facilities have enabled the execution of transformative simulations at unprecedented scales. However, analyzing the huge amount of output from these simulations remains a challenge. Most analyses of this output is performed in post-processing mode at the end of the simulation. The time to read the output for the analysis can be significantly high due to poor I/O bandwidth, which increases the end-to-end simulation-analysis time. Simulation-time analysis can reduce this end-to-end time. In this work, we present the scheduling of in-situ analysis as a numerical optimization problem to maximize the number of online analyses subject to resource constraints such as I/O bandwidth, network bandwidth, rate of computation and available memory. We demonstrate the effectiveness of our approach through two application case studies on the IBM Blue Gene/Q system.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807656,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832826,,Analytical models;Bandwidth;Computational modeling;Data models;Data visualization;Kernel;Optimization,bandwidth allocation;natural sciences computing;scheduling,I/O bandwidth;IBM Blue Gene/Q system;end-to-end simulation-analysis time;in-situ analysis;large-scale scientific simulations;leadership computing facilities;network bandwidth;numerical optimization;online analysis;optimal scheduling;resource constraints;simulation-time analysis;transformative simulations,,1,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Particle tracking in open simulation laboratories,K. Kanov; R. Burns,"Dept. of Comput. Sci., Johns Hopkins Univ., Baltimore, MD, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,11,"Particle tracking along streamlines and pathlines is a common scientific analysis technique, which has demanding data, computation and communication requirements. It has been studied in the context of high-performance computing due to the difficulty in its efficient parallelization and its high demands on communication and computational load. In this paper, we study efficient evaluation methods for particle tracking in open simulation laboratories. Simulation laboratories have a fundamentally different architecture from today's supercomputers and provide publicly-available analysis functionality. We focus on the I/O demands of particle tracking for numerical simulation datasets 100s of TBs in size. We compare data-parallel and task-parallel approaches for the advection of particles and show scalability results on data-intensive workloads from a live production environment. We have developed particle tracking capabilities for the Johns Hopkins Turbulence Databases, which store computational fluid dynamics simulation data, including forced isotropic turbulence, magnetohydrodynamics, channel flow turbulence and homogeneous buoyancy-driven turbulence.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807645,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832817,,Computational modeling;Data models;Distributed databases;Laboratories;Object oriented modeling;Particle tracking,digital simulation;physics computing,Johns Hopkins Turbulence Databases;channel flow turbulence;computational fluid dynamics simulation data;data-parallel approach;forced isotropic turbulence;homogeneous buoyancy-driven turbulence;magnetohydrodynamics;numerical simulation datasets;open simulation laboratories;particle advection;particle tracking;scientific analysis technique;task-parallel approach,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Adaptive data placement for staging-based coupled scientific workflows,Q. Sun; T. Jin; M. Romanus; H. Bui; F. Zhang; H. Yu; H. Kolla; S. Klasky; J. Chen; M. Parashar,"Rutgers Discovery Inf. Inst., Rutgers Univ., Piscataway, NJ, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Data staging and in-situ/in-transit data processing are emerging as attractive approaches for supporting extreme scale scientific workflows. These approaches improve end-to-end performance by enabling runtime data sharing between coupled simulations and data analytics components of the workflow. However, the complex and dynamic data exchange patterns exhibited by the workflows coupled with the varied data access behaviors make efficient data placement within the staging area challenging. In this paper, we present an adaptive data placement approach to address these challenges. Our approach adapts data placement based on application-specific dynamic data access patterns, and applies access pattern-driven and location-aware mechanisms to reduce data access costs and to support efficient data sharing between the multiple workflow components. We experimentally demonstrate the effectiveness of our approach on Titan Cray XK7 using a real combustion-analyses workflow. The evaluation results demonstrate that our approach can effectively improve data access performance and overall efficiency of coupled scientific workflows.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807669,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832839,,Adaptation models;Analytical models;Combustion;Data models;Data processing;Laboratories;Runtime,data analysis;mobile computing;scientific information systems;workflow management software,Titan Cray XK7;access pattern-driven mechanisms;adaptive data placement;application-specific dynamic data access patterns;data access behaviors;data access costs;data analytics components;data processing;data sharing;data staging;extreme scale scientific workflows;location-aware mechanisms;staging-based coupled scientific workflows;workflow components,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Massively parallel models of the human circulatory system,A. Randles; E. W. Draeger; T. Oppelstrup; L. Krauss; J. A. Gunnels,"Lawrence Livermore Nat. Lab., Duke Univ., Durham, NC, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,11,"The potential impact of blood flow simulations on the diagnosis and treatment of patients suffering from vascular disease is tremendous. Empowering models of the full arterial tree can provide insight into diseases such as arterial hypertension and enables the study of the influence of local factors on global hemodynamics. We present a new, highly scalable implementation of the lattice Boltzmann method which addresses key challenges such as multiscale coupling, limited memory capacity and bandwidth, and robust load balancing in complex geometries. We demonstrate the strong scaling of a three-dimensional, high-resolution simulation of hemodynamics in the systemic arterial tree on 1,572,864 cores of Blue Gene/Q. Faster calculation of flow in full arterial networks enables unprecedented risk stratification on a perpatient basis. In pursuit of this goal, we have introduced computational advances that significantly reduce time-to-solution for biofluidic simulations.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807676,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832775,,Arteries;Circulatory system;Computational modeling;Diseases;Geometry;Hemodynamics;Load modeling,cardiovascular system;haemodynamics;patient treatment,arterial hypertension;arterial networks;biofluidic simulations;blood flow simulations;complex geometries;global hemodynamics;human circulatory system;lattice Boltzmann method;limited memory capacity;massively parallel models;multiscale coupling;patient treatment;risk stratification;robust load balancing;systemic arterial tree;time-to-solution;vascular disease,,1,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
An input-adaptive and in-place approach to dense tensor-times-matrix multiply,J. Li; C. Battaglino; I. Perros; J. Sun; R. Vuduc,"Georgia Inst. of Technol., Atlanta, GA, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"This paper describes a novel framework, called INTENSLìã (‰ÛÏintensely‰Û), for producing fast single-node implementations of dense tensor-times-matrix multiply (TTM) of arbitrary dimension. Whereas conventional implementations of TTM rely on explicitly converting the input tensor operand into a matrix-in order to be able to use any available and fast general matrix-matrix multiply (GEMM) implementation- our framework's strategy is to carry out the TTM in-place, avoiding this copy. As the resulting implementations expose tuning parameters, this paper also describes a heuristic empirical model for selecting an optimal configuration based on the TTM's inputs. When compared to widely used singlenode TTM implementations that are available in the TENSOR TOOLBOX and CYCLOPS Tensor Framework (CTF), INTENSLìã's in-place and input-adaptive TTM implementations achieve 4ÌÑ and 13ÌÑ speedups, showing GEMM-like performance on a variety of input sizes.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807671,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832850,,Data analysis;Layout;Libraries;Matrix converters;Tensile stress;Transforms;Tuning,matrix algebra;tensors,CTF;CYCLOPS tensor framework;GEMM;INTENSLI;TENSOR TOOLBOX;dense tensor-times-matrix multiply;general matrix-matrix multiply;heuristic empirical model;input-adaptive TTM implementations;singlenode TTM implementations,,2,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Big omics data experience,P. Kovatch; A. Costa; Z. Giles; E. Fluder; H. M. Cho; S. Mazurkova,"Icahn Sch. of Med. at Mount Sinai, New York, NY, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"As personalized medicine becomes more integrated into healthcare, the rate at which human genomes are being sequenced is rising quickly together with a concomitant acceleration in compute and storage requirements. To achieve the most effective solution for genomic workloads without re-architecting the industry-standard software, we performed a rigorous analysis of usage statistics, benchmarks and available technologies to design a system for maximum throughput. We share our experiences designing a system optimized for the ""Genome Analysis ToolKit (GATK) Best Practices"" whole genome DNA and RNA pipeline based on an evaluation of compute, workload and I/O characteristics. The characteristics of genomic-based workloads are vastly different from those of traditional HPC workloads, requiring different configurations of the scheduler and the I/O subsystem to achieve reliability, performance and scalability. By understanding how our researchers and clinicians work, we were able to employ techniques not only to speed up their workflow yielding improved and repeatable performance, but also to make more efficient use of storage and compute resources.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807595,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832813,,Best practices;Bioinformatics;DNA;File systems;Genomics;Pipelines;Sequential analysis,Big Data;DNA;RNA;bioinformatics;data analysis;genomics;health care;pipeline processing;resource allocation;scheduling;storage management,GATK Best Practices;Genome Analysis ToolKit Best Practices;I/O subsystem;benchmark;big omics data;genomic workload;genomic-based workload;healthcare;human genome sequencing;optimized system design;personalized medicine;reliability;sample analysis;scalability;scheduler configuration;storage requirements;usage statistics;whole genome DNA pipeline;whole genome RNA pipeline,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Massively parallel phase-field simulations for ternary eutectic directional solidification,M. Bauer; J. HÌ¦tzer; M. Jainta; P. Steinmetz; M. Berghoff; F. Schornbaum; C. Godenschwager; H. KÌ¦stler; B. Nestler; U. RÌ_de,"Dept. of Syst. Simulation, Friedrich Alexander Univ. Erlangen-Nurnberg, Erlangen, Germany","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Microstructures forming during ternary eutectic directional solidification processes have significant influence on the macroscopic mechanical properties of metal alloys. For a realistic simulation, we use the well established thermodynamically consistent phase-field method and improve it with a new grand potential formulation to couple the concentration evolution. This extension is very compute intensive due to a temperature dependent diffusive concentration. We significantly extend previous simulations that have used simpler phase-field models or were performed on smaller domain sizes. The new method has been implemented within the massively parallel HPC framework waLBerla that is designed to exploit current supercomputers efficiently. We apply various optimization techniques, including buffering techniques, explicit SIMD kernel vectorization, and communication hiding. Simulations utilizing up to 262,144 cores have been run on three different supercomputing architectures and weak scalability results are shown. Additionally, a hierarchical, mesh-based data reduction strategy is developed to keep the I/O problem manageable at scale.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807662,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832782,,Chemicals;Computational modeling;Kernel;Mathematical model;Metals;Solid modeling;Thermodynamics,data reduction;eutectic alloys;input-output programs;mechanical engineering computing;mechanical properties;parallel processing;simulation;vectors,I/O problem;SIMD kernel vectorization;macroscopic mechanical properties;massively parallel HPC framework;massively parallel phase-field simulations;mesh-based data reduction;metal alloys;microstructures;phase-field method;temperature dependent diffusive concentration;ternary eutectic directional solidification;waLBerla,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Practical scalable consensus for pseudo-synchronous distributed systems,T. Herault; A. Bouteiller; G. Bosilca; M. Gamell; K. Teranishi; M. Parashar; J. Dongarra,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"The ability to consistently handle faults in a distributed environment requires, among a small set of basic routines, an agreement algorithm allowing surviving entities to reach a consensual decision between a bounded set of volatile resources. This paper presents an algorithm that implements an Early Returning Agreement (ERA) in pseudo-synchronous systems, which optimistically allows a process to resume its activity while guaranteeing strong progress. We prove the correctness of our ERA algorithm, and expose its logarithmic behavior, which is an extremely desirable property for any algorithm which targets future exascale platforms. We detail a practical implementation of this consensus algorithm in the context of an MPI library, and evaluate both its efficiency and scalability through a set of benchmarks and two fault tolerant scientific applications.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807665,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832805,,Context;Fault tolerance;Fault tolerant systems;Government;Libraries;Scalability,application program interfaces;distributed algorithms;fault tolerance;software fault tolerance,ERA algorithm;MPI library;agreement algorithm;consensual decision;consensus algorithm;early returning agreement;exascale platforms;fault tolerant scientific applications;practical scalable consensus;pseudo-synchronous distributed systems;volatile resource bounded set,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
HydraDB: a resilient RDMA-driven key-value middleware for in-memory cluster computing,Y. Wang; L. Zhang; J. Tan; M. Li; Y. Gao; X. Guerin; X. Meng; S. Meng,"Thomas J. Watson Res. Center, IBM, New York, NY, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,11,"In this paper, we describe our experiences and lessons learned from building a general-purpose in-memory key-value middleware, called HydraDB. HydraDB synthesizes a collection of state-of-the-art techniques, including continuous fault-tolerance, Remote Direct Memory Access (RDMA), as well as awareness for multicore systems, etc, to deliver a high-throughput, low-latency access service in a reliable manner for cluster computing applications. The uniqueness of HydraDB mainly lies in its design commitment to fully exploit the RDMA protocol to comprehensively optimize various aspects of a general-purpose key-value store, including latency-critical operations, read enhancement, and data replications for high-availability service, etc. At the same time, HydraDB strives to efficiently utilize multicore systems to prevent data manipulation on the servers from curbing the potential of RDMA. Many teams in our organization have adopted HydraDB to improve the execution of their cluster computing frameworks, including Hadoop, Spark, Sensemaking analytics, and Call Record Processing. In addition, our performance evaluation with a variety of YCSB workloads also shows that HydraDB can substantially outperform several existing in-memory key-value stores by an order of magnitude. Our detailed performance evaluation further corroborates our design choices.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807614,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832796,,Engines;Multicore processing;Protocols;Random access memory;Servers;Sparks;Throughput,fault tolerant computing;memory protocols;middleware;multiprocessing systems;performance evaluation;workstation clusters,Call Record Processing;Hadoop;HydraDB;RDMA protocol;Sensemaking analytics;Spark;YCSB workloads;continuous fault-tolerance;data manipulation;data replications;general-purpose in-memory key-value middleware;high-throughput access service;in-memory cluster computing;latency-critical operations;low-latency access service;multicore systems;performance evaluation;read enhancement;remote direct memory access;resilient RDMA-driven key-value middleware,,1,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
BD-CATS: big data clustering at trillion particle scale,M. M. A. Patwary; S. Byna; N. R. Satish; N. Sundaram; Z. Luki€à; V. Roytershteyn; M. J. Anderson; Y. Yao; Prabhat; P. Dubey,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Modern cosmology and plasma physics codes are now capable of simulating trillions of particles on petascale systems. Each timestep output from such simulations is on the order of 10s of TBs. Summarizing and analyzing raw particle data is challenging, and scientists often focus on density structures, whether in the real 3D space, or a high-dimensional phase space. In this work, we develop a highly scalable version of the clustering algorithm DBSCAN, and apply it to the largest datasets produced by state-of-the-art codes. Our system, called BD-CATS, is the first one capable of performing end-to-end analysis at trillion particle scale (including: loading the data, geometric partitioning, computing kd-trees, performing clustering analysis, and storing the results). We show analysis of 1.4 trillion particles from a plasma physics simulation, and a 10,240<sup>3</sup> particle cosmological simulation, utilizing ~100,000 cores in 30 minutes. BD-CATS is helping infer mechanisms behind particle acceleration in plasma physics and holds promise for qualitatively superior clustering in cosmology. Both of these results were previously intractable at the trillion particle scale.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807616,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832780,,Analytical models;Computational modeling;Magnetomechanical effects;Magnetosphere;Physics;Plasmas;Superconducting magnets,Big Data;astronomy computing;astrophysical plasma;cosmology;data analysis;digital simulation;pattern clustering;trees (mathematics),BD-CATS;DBSCAN clustering algorithm;clustering analysis;cosmology;data summarization;density structure;geometric partitioning;high-dimensional phase space;kd-tree computing;particle acceleration;particle cosmological simulation;particle data data analysis;petascale system;plasma physics code;plasma physics simulation;trillion particle scale big data clustering,,1,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Bridging OpenCL and CUDA: a comparative analysis and translation,J. Kim; T. T. Dao; J. Jung; J. Joo; J. Lee,"Dept. of Comput. Sci. & Eng., Seoul Nat. Univ., Seoul, South Korea","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Heterogeneous systems are widening their user-base, and heterogeneous computing is becoming popular in supercomputing. Among others, OpenCL and CUDA are the most popular programming models for heterogeneous systems. Although OpenCL inherited many features from CUDA and they have almost the same platform model, they are not compatible with each other. In this paper, we present similarities and differences between them and propose an automatic translation framework for both OpenCL to CUDA and CUDA to OpenCL. We describe features that make it difficult to translate from one to the other and provide our solution. We show that our translator achieves comparable performance between the original and target applications in both directions. Since each programming model separately has a wide user-base and large code-base, our translation framework is useful to extend the code-base for each programming model and unifies the efforts to develop applications for heterogeneous systems.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807621,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832856,,Computational modeling;Graphics processing units;Instruction sets;Kernel;Multicore processing;Programming;Registers,parallel architectures;programming languages;source code (software),CUDA;OpenCL;code-base extension;heterogeneous computing;heterogeneous system;high performance computing;programming model;supercomputing;translation framework,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
A parallel connectivity algorithm for de Bruijn graphs in metagenomic applications,P. Flick; C. Jain; T. Pan; S. Aluru,"Georgia Inst. of Technol., Atlanta, GA, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,11,"Dramatic advances in DNA sequencing technology have made it possible to study microbial environments by direct sequencing of environmental DNA samples. Yet, due to the huge volume and high data complexity, current de novo assemblers cannot handle large metagenomic datasets or fail to perform assembly with acceptable quality. This paper presents the first parallel solution for decomposing the metagenomic assembly problem without compromising the post-assembly quality. We transform this problem into that of finding weakly connected components in the de Bruijn graph. We propose a novel distributed memory algorithm to identify the connected subgraphs, and present strategies to minimize the communication volume. We demonstrate the scalability of our algorithm on a soil metagenome dataset with 1.8 billion reads. Our approach achieves a runtime of 22 minutes using 1280 Intel Xeon cores for a 421 GB uncompressed FASTQ dataset. Moreover, our solution is generalizable to finding connected components in arbitrary undirected graphs.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807619,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832789,,Algorithm design and analysis;Genomics;Labeling;Phase change random access memory;Scalability;Sequential analysis;Soil,DNA;data handling;directed graphs;distributed memory systems;genomics;parallel algorithms;program assemblers,Intel Xeon core;communication volume minimization;connected subgraph;data complexity;de Bruijn graphs;de novo assemblers;distributed memory algorithm;environmental DNA direct sequencing;metagenomic applications;metagenomic assembly problem decomposition;microbial environments;parallel connectivity algorithm;soil metagenome dataset;uncompressed FASTQ dataset;undirected graphs;weakly connected components,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
A case for application-oblivious energy-efficient MPI runtime,A. Venkatesh; A. Vishnu; K. Hamidouche; N. Tallent; D. Panda; D. Kerbyson; A. Hoisie,"Ohio State Univ. Columbus, Columbus, OH, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Power has become a major impediment in designing large scale high-end systems. Message Passing Interface (MPI) is the de facto communication interface used as the back-end for designing applications, programming models and runtime for these systems. Slack --- the time spent by an MPI process in a single MPI call---provides a potential for energy and power savings, if an appropriate power reduction technique such as core-idling/Dynamic Voltage and Frequency Scaling (DVFS) can be applied without affecting the application's performance. Existing techniques that exploit slack for power savings assume that application behavior repeats across iterations/executions. However, an increasing use of adaptive and data-dependent workloads combined with system factors (OS noise, congestion) negates this assumption. This paper proposes and implements Energy Aware MPI (EAM) --- an application-oblivious energy-efficient MPI runtime. EAM uses a combination of communication models for common MPI primitives (point-to-point, collective, progress, blocking/non-blocking) and an online observation of slack to maximize energy efficiency and to honor performance degradation limits. Each power lever incurs time overhead, which must be amortized over slack to minimize degradation. When predicted communication time exceeds a lever overhead, the lever is used as soon as possible --- to maximize energy efficiency. When a misprediction occurs, the lever(s) are used automatically at specific intervals for amortization. We implement EAM using MVAPICH2 and evaluate it on ten applications using up to 4,096 processes. Our performance evaluation on an InfiniBand cluster indicates that EAM can reduce energy consumption by 5-41% in comparison to the default approach, which prioritizes performance alone, with negligible (less than 4% in all cases) performance loss.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807658,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832803,,Computer architecture;Degradation;Energy consumption;Message passing;Performance evaluation;Predictive models;Runtime,application program interfaces;energy conservation;message passing;power aware computing,DVFS;EAM;MVAPICH2;Slack;application-oblivious energy-efficient MPI runtime;core-idling technique;dynamic voltage and frequency scaling;message passing interface,,2,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
VOCL-FT: introducing techniques for efficient soft error coprocessor recovery,A. J. PeÌ±a; W. Bland; P. Balaji,"Argonne Nat. Lab., Argonne, IL, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Popular accelerator programming models rely on offloading computation operations and their corresponding data transfers to the coprocessors, leveraging synchronization points where needed. In this paper we identify and explore how such a programming model enables optimization opportunities not utilized in traditional checkpoint/restart systems, and we analyze them as the building blocks for an efficient fault-tolerant system for accelerators. Although we leverage our techniques to protect from detected but uncorrected ECC errors in the device memory in OpenCL-accelerated applications, coprocessor reliability solutions based on different error detectors and similar API semantics can directly adopt the techniques we propose. Adding error detection and protection involves a tradeoff between runtime overhead and recovery time. Although optimal configurations depend on the particular application, the length of the run, the error rate, and the temporary storage speed, our test cases reveal a good balance with significantly reduced runtime overheads.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807640,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832845,,Checkpointing;Coprocessors;Error correction codes;Kernel;Libraries;Optimization;Runtime,application program interfaces;coprocessors;fault tolerant computing;system recovery,API semantics;OpenCL-accelerated applications;VOCL-FT;accelerator programming models;coprocessor reliability;data transfers;device memory;error detectors;fault-tolerant system;offloading computation operations;recovery time;runtime overhead;soft error coprocessor recovery,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
AnalyzeThis: an analysis workflow-aware storage system,H. Sim; Y. Kim; S. S. Vazhkudai; D. Tiwari; A. Anwar; A. R. Butt; L. Ramakrishnan,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"The need for novel data analysis is urgent in the face of a data deluge from modern applications. Traditional approaches to data analysis incur significant data movement costs, moving data back and forth between the storage system and the processor. Emerging Active Flash devices enable processing on the flash, where the data already resides. An array of such Active Flash devices allows us to revisit how analysis workflows interact with storage systems. By seamlessly blending together the flash storage and data analysis, we create an analysis workflow-aware storage system, AnalyzeThis. Our guiding principle is that analysis-awareness be deeply ingrained in each and every layer of the storage, elevating data analyses as first-class citizens, and transforming AnalyzeThis into a potent analytics-aware appliance. We implement the AnalyzeThis storage system atop an emulation platform of the Active Flash array. Our results indicate that AnalyzeThis is viable, expediting workflow execution and minimizing data movement.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807622,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832794,,Arrays;Bandwidth;Computational modeling;Data analysis;Data models;Kernel;Performance evaluation,data analysis;flash memories;storage management,AnalyzeThis;active flash device array;analysis workflow-aware storage system;data analysis;data movement costs;flash storage;workflow execution,,1,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
ScaAnalyzer: a tool to identify memory scalability bottlenecks in parallel programs,X. Liu; B. Wu,"Dept. of Comput. Sci., Coll. of William & Mary, Williamsburg, VA, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"It is difficult to scale parallel programs in a system that employs a large number of cores. To identify scalability bottlenecks, existing tools principally pinpoint poor thread synchronization strategies or unnecessary data communication. Memory subsystem is one of the key contributors to poor parallel scaling in multicore machines. State-of-the-art tools, however, either lack sophisticated capabilities or are completely ignorant in pinpointing scalability bottlenecks arising from the memory subsystem. To address this issue, we develop a tool - ScaAnalyzer - to pinpoint scaling losses due to poor memory access behaviors of parallel programs. ScaAnalyzer collects, attributes, and analyzes memory-related metrics during program execution while incurring very low overhead. ScaAnalyzer provides high-level, detailed guidance to programmers for scalability optimization. We demonstrate the utility of ScaAnalyzer with case studies of three parallel programs. For each benchmark, ScaAnalyzer identifies scalability bottlenecks caused by poor memory access behaviors and provides optimization guidance that yields significant improvement in scalability.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807648,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832821,,Bandwidth;Benchmark testing;Hardware;Instruction sets;Monitoring;Optimization;Scalability,data communication;file organisation;parallel programming;software tools;synchronisation,ScaAnalyzer tool;data communication;memory access behaviors;memory scalability bottleneck identification;memory subsystem;multicore machines;parallel programs;parallel scaling;thread synchronization,,1,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Smart: a MapReduce-like framework for in-situ scientific analytics,Y. Wang; G. Agrawal; T. Bicer; W. Jiang,"Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"In-situ analytics has lately been shown to be an effective approach to reduce both I/O and storage costs for scientific analytics. Developing an efficient in-situ implementation, however, involves many challenges, including parallelization, data movement or sharing, and resource allocation. Based on the premise that MapReduce can be an appropriate API for specifying scientific analytics applications, we present a novel MapReduce-like framework that supports efficient in-situ scientific analytics, and address several challenges that arise in applying the MapReduce idea for in-situ processing. Specifically, our implementation can load simulated data directly from distributed memory, and it uses a modified API that helps meet the strict memory constraints of in-situ analytics. The framework is designed so that analytics can be launched from the parallel code region of a simulation program. We have developed both time sharing and space sharing modes for maximizing the performance in different scenarios, with the former even avoiding any copying of data from simulation to the analytics program. We demonstrate the functionality, efficiency, and scalability of our system, by using different simulation and analytics programs, executed on clusters with multi-core and many-core nodes.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807650,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832825,,Analytical models;Data models;Distributed databases;Loading;Memory management;Programming;Sparks,application program interfaces;data analysis;data handling;digital simulation;microprocessor chips;multiprocessing systems;natural sciences computing;parallel processing;resource allocation,API;IO costs;MapReduce-like framework;Smart;data movement;in-situ scientific analytics;many-core nodes;multicore nodes;parallel code region;resource allocation;simulation program;space sharing modes;storage costs;time sharing modes,,5,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Implicit nonlinear wave simulation with 1.08T DOF and 0.270T unstructured finite elements to enhance comprehensive earthquake simulation,T. Ichimura; K. Fujita; P. E. B. Quinay; L. Maddegedara; M. Hori; S. Tanaka; Y. Shizawa; H. Kobayashi; K. Minami,"Dept. of Civil Eng., Univ. of Tokyo, Tokyo, Japan","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"This paper presents a new heroic computing method for unstructured, low-order, finite-element, implicit nonlinear wave simulation: 1.97 PFLOPS (18.6% of peak) was attained on the full K computer when solving a 1.08T degrees-of-freedom (DOF) and 0.270T-element problem. This is 40.1 times more DOF and elements, a 2.68-fold improvement in peak performance, and 3.67 times faster in time-to-solution compared to the SC14 Gordon Bell finalist's state-of-the-art simulation. The method scales up to the full K computer with 663,552 CPU cores with 96.6% sizeup efficiency, enabling solving of a 1.08T DOF problem in 29.7 s per time step. Using such heroic computing, we solved a practical problem involving an area 23.7 times larger than the state-of-the-art, and conducted a comprehensive earthquake simulation by combining earthquake wave propagation analysis and evacuation analysis. Application at such scale is a groundbreaking accomplishment and is expected to change the quality of earthquake disaster estimation and contribute to society.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807674,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832778,,Computational modeling;Computers;Earthquakes;Mathematical model;Propagation;Soil;Urban areas,digital simulation;disasters;earthquake engineering;estimation theory;finite element analysis;structural engineering computing;wave propagation,0.270T element problem;1.08T DOF;comprehensive earthquake simulation;earthquake disaster estimation;earthquake wave evacuation analysis;earthquake wave propagation analysis;full K computer;heroic computing;implicit nonlinear wave simulation;unstructured finite elements,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Improving the scalability of the ocean barotropic solver in the community earth system model,Y. Hu; X. Huang; A. H. Baker; Y. h. Tseng; F. O. Bryan; J. M. Dennis; G. Yang,"Center for Earth Syst. Sci., Tsinghua Univ., Beijing, China","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"High-resolution climate simulations are increasingly in demand and require tremendous computing resources. In the Community Earth SystemModel (CESM), the Parallel Ocean Model (POP) is computationally expensive for high-resolution grids (e.g., 0.1å¡) and is frequently the least scalable component of CESM for certain production simulations. In particular, the modified Preconditioned Conjugate Gradient (PCG), used to solve the elliptic system of equations in the barotropic mode, scales poorly at the high core counts, which is problematic for high-resolution simulations. In this work, we demonstrate that the communication costs in the barotropic solver occupy an increasing portion of the total POP execution time as core counts are increased. To mitigate this problem, we implement a preconditioned Chebyshev-type iterative method in POP (called P-CSI), which requires far fewer global reductions than PCG. We also develop an effective block preconditioner based on the Error Vector Propagation Method to attain a competitive convergence rate for P-CSI. We demonstrate that the improved scalability of P-CSI results in a 5.2x speedup of the barotropic mode in high-resolution POP on 16,875 cores, which yields a 1.7x speedup of the overall POP simulation. Further, we ensure that the new solver produces an ocean climate consistent with the original one via an ensemble-based statistical method.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807596,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832816,,Atmospheric modeling;Computational modeling;Earth;Mathematical model;Meteorology;Oceans;Scalability,Chebyshev approximation;climatology;conjugate gradient methods;digital simulation;geophysics computing;iterative methods;oceanographic techniques;statistical analysis;vectors,CESM;Community Earth System Model;P-CSI;PCG;Parallel Ocean Model;block preconditioner;ensemble-based statistical method;error vector propagation method;high-resolution POP;high-resolution climate simulations;high-resolution grids;ocean barotropic solver;ocean climate;preconditioned Chebyshev-type iterative method;preconditioned conjugate gradient,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Improving backfilling by using machine learning to predict running times,E. Gaussier; D. Glesser; V. Reis; D. Trystram,"LIG, Univ. Grenoble-Alpes, Grenoble, France","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,10,"The job management system is the HPC middleware responsible for distributing computing power to applications. While such systems generate an ever increasing amount of data, they are characterized by uncertainties on some parameters like the job running times. The question raised in this work is: To what extent is it possible/useful to take into account predictions on the job running times for improving the global scheduling? We present a comprehensive study for answering this question assuming the popular EASY backfilling policy. More precisely, we rely on some classical methods in machine learning and propose new cost functions well-adapted to the problem. Then, we assess our proposed solutions through intensive simulations using several production logs. Finally, we propose a new scheduling algorithm that outperforms the popular EASY backfilling algorithm by 28% considering the average bounded slowdown objective.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807646,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832838,,Estimation;Hidden Markov models;Prediction algorithms;Resource management;Scheduling algorithms;Uncertainty,learning (artificial intelligence);middleware;parallel processing;scheduling,EASY backfilling policy;HPC middleware;cost functions;distributing computing power;global scheduling;job management system;job running time prediction;machine learning;production logs,,1,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Scaling iterative graph computations with GraphMap,K. Lee; L. Liu; K. Schwan; C. Pu; Q. Zhang; Y. Zhou; E. Yigitoglu; P. Yuan,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"In recent years, systems researchers have devoted considerable effort to the study of large-scale graph processing. Existing distributed graph processing systems such as Pregel, based solely on distributed memory for their computations, fail to provide seamless scalability when the graph data and their intermediate computational results no longer fit into the memory; and most distributed approaches for iterative graph computations do not consider utilizing secondary storage a viable solution. This paper presents GraphMap, a distributed iterative graph computation framework that maximizes access locality and speeds up distributed iterative graph computations by effectively utilizing secondary storage. GraphMap has three salient features: (1) It distinguishes data states that are mutable during iterative computations from those that are read-only in all iterations to maximize sequential access and minimize random access. (2) It entails a two-level graph partitioning algorithm that enables balanced workloads and locality-optimized data placement. (3) It contains a proposed suite of locality-based optimizations that improve computational efficiency. Extensive experiments on several real-world graphs show that GraphMap outperforms existing distributed memory-based systems for various iterative graph algorithms.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807604,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832831,,,distributed memory systems;graph theory;iterative methods;optimisation;resource allocation,GraphMap;Pregel;access locality;balanced workloads;data states;distributed graph processing systems;distributed iterative graph computation framework;distributed memory;iterative graph algorithms;iterative graph computations scaling;large-scale graph processing;locality-based optimizations;locality-optimized data placement;random access;salient features;sequential access;two-level graph partitioning algorithm,,2,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Dynamic power sharing for higher job throughput,D. A. Ellsworth; A. D. Malony; B. Rountree; M. Schulz,"Univ. of Oregon, Eugene, OR, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,11,"Current trends for high-performance systems are leading towards hardware overprovisioning where it is no longer possible to run all components at peak power without exceeding a system- or facility-wide power bound. The standard practice of static power scheduling is likely to lead to inefficiencies with over- and under-provisioning of power to components at runtime. In this paper we investigate the performance and scalability of an application agnostic runtime power scheduler (POWsched) that is capable of enforcing a system-wide power limit. Our experimental results show POWsched is robust, has negligible overhead, and can take advantage of opportunities to shift wasted power to more power-intensive applications, improving overall workload runtime by as much as 14% without job scheduler integration or application specific profiling. In addition, we conduct scalability studies to determine POWsched's overhead for large node counts. Lastly, we contribute a model and simulator (POWsim) for investigating dynamic power scheduling behavior and enforcement at scale.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807643,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832854,,Hardware;Power demand;Processor scheduling;Resource management;Runtime;Sockets;Software,parallel processing;power aware computing;scheduling,POWsched;POWsim simulator;application agnostic runtime power scheduler;dynamic power scheduling;dynamic power sharing;facility-wide power bound;hardware overprovisioning;high-performance systems;job throughput;power overprovisioning;power underprovisioning;power-intensive applications;static power scheduling;system-wide power bound;system-wide power limit,,4,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
"A practical approach to reconciling availability, performance, and capacity in provisioning extreme-scale storage systems",L. Wan; F. Wang; S. Oral; D. Tiwari; S. S. Vazhkudai; Q. Cao,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"The increasing data demands from high-performance computing applications significantly accelerate the capacity, capability and reliability requirements of storage systems. As systems scale, component failures and repair times increase, significantly impacting data availability. A wide array of decision points must be balanced in designing such systems. We propose a systematic approach that balances and optimizes both initial and continuous spare provisioning based on a detailed investigation of the anatomy and field failure data analysis of extreme-scale storage systems. We consider the component failure characteristics and its cost and impact at the system level simultaneously. We build a tool to evaluate different provisioning schemes, and the results demonstrate that our optimized provisioning can reduce the duration of data unavailability by as much as 52% under a fixed budget. We also observe that non-disk components have much higher failure rates than disks, and warrant careful considerations in the overall provisioning process.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807615,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832849,,Buildings;Disk drives;Measurement;Power supplies;Procurement;Reliability;Uninterruptible power systems,storage management,capability requirement;capacity requirement;data availability;extreme-scale storage system provisioning;high-performance computing application;reliability requirement,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
CIVL: the concurrency intermediate verification language,S. F. Siegel; M. Zheng; Z. Luo; T. K. Zirkel; A. V. Marianiello; J. G. Edenhofner; M. B. Dwyer; M. S. Rogers,"Dept. of Comput. & Inf. Sci., Univ. of Delaware, Newark, DE, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"There are many ways to express parallel programs: message-passing libraries (MPI) and multithreading/GPU language extensions such as OpenMP, Pthreads, and CUDA, are but a few. This multitude creates a serious challenge for developers of software verification tools: it takes enormous effort to develop such tools, but each development effort typically targets one small part of the concurrency landscape, with little sharing of techniques and code among efforts. To address this problem, we present CIVL: the Concurrency Intermediate Verification Language. CIVL provides a general concurrency model capable of representing programs in a variety of concurrency dialects, including those listed above. The CIVL framework currently includes front-ends for the four dialects, and a back-end verifier which uses model checking and symbolic execution to check a number of properties, including the absence of deadlocks, race conditions, assertion violations, illegal pointer dereferences and arithmetic, memory leaks, divisions by zero, and out-of-bound array indexing; it can also check that two programs are functionally equivalent.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807635,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832835,,Concurrent computing;Government;Graphics processing units;Libraries;Message systems;Model checking;Standards,concurrency (computers);message passing;multi-threading;program verification,CIVL;CUDA;GPU language extensions;MPI;OpenMP;Pthreads;assertion violations;back-end verifier;concurrency intermediate verification language;deadlock absence;divisions by zero;illegal pointer dereferences;memory leaks;message-passing libraries;model checking;multithreading;out-of-bound array indexing;parallel programs;race conditions;software verification tools;symbolic execution,,1,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Automatic sharing classification and timely push for cache-coherent systems,M. Musleh; V. S. Pai,"Purdue Univ., West Lafayette, IN, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"This paper proposes and evaluates Sharing/Timing Adaptive Push (STAP), a dynamic scheme for preemptively sending data from producers to consumers to minimize critical-path communication latency. STAP uses small hardware buffers to dynamically detect sharing patterns and timing requirements. The scheme applies to both intra-node and inter-socket directory-based shared memory networks. We integrate STAP into a MOESI cache-coherence (prefetching-enabled) protocol using heuristics to detect different data sharing patterns, including broadcasts, producer/consumer, and migratory-data sharing. Using 15 benchmarks from the PARSEC and SPLASH-2 suites we show that our scheme significantly reduces communication latency in NUMA systems and achieves an average of 9% performance improvement, with at most 3% on-chip storage overhead.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807649,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832787,,Benchmark testing;Coherence;Data transfer;Hardware;Prefetching;Protocols;Timing,cache storage;critical path analysis;pattern classification;protocols;shared memory systems,MOESI cache-coherence protocol;NUMA systems;PARSEC;SPLASH-2;STAP;automatic sharing classification;cache-coherent systems;critical-path communication latency;data sharing patterns;inter-socket directory-based shared memory networks;intra-node directory-based shared memory networks;sharing/timing adaptive push,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
GossipMap: a distributed community detection algorithm for billion-edge directed graphs,S. H. Bae; B. Howe,"Dept. of Comput. Sci. & Eng., Univ. of Washington, Seattle, WA, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"In this paper, we describe a new distributed community detection algorithm for billion-edge directed graphs that, unlike modularity-based methods, achieves cluster quality on par with the best-known algorithms in the literature. We show that a simple approximation to the best-known serial algorithm dramatically reduces computation and enables distributed evaluation yet incurs only a very small impact on cluster quality. We present three main results: First, we show that the clustering produced by our scalable approximate algorithm compares favorably with prior results on small synthetic benchmarks and small real-world datasets (70 million edges). Second, we evaluate our algorithm on billion-edge directed graphs (a 1.5B edge social network graph, and a 3.7B edge web crawl), and show that the results exhibit the structural properties predicted by analysis of much smaller graphs from similar sources. Third, we show that our algorithm exhibits over 90% parallel efficiency on massive graphs in weak scaling experiments.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807668,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832801,,Algorithm design and analysis;Approximation algorithms;Benchmark testing;Clustering algorithms;Detection algorithms;Linear programming;Mathematical model,directed graphs;distributed processing;pattern clustering,GossipMap;billion-edge directed graphs;cluster quality;distributed community detection algorithm;massive graphs;modularity-based methods;scalable approximate algorithm;serial algorithm;weak scaling experiments,,1,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Reliability lessons learned from GPU experience with the Titan supercomputer at Oak Ridge leadership computing facility,D. Tiwari; S. Gupta; G. Gallarno; J. Rogers; D. Maxwell,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"The high computational capability of graphics processing units (GPUs) is enabling and driving the scientific discovery process at large-scale. The world's second fastest supercomputer for open science, Titan, has more than 18,000 GPUs that computational scientists use to perform scientific simulations and data analysis. Understanding of GPU reliability characteristics, however, is still in its nascent stage since GPUs have only recently been deployed at large-scale. This paper presents a detailed study of GPU errors and their impact on system operations and applications, describing experiences with the 18,688 GPUs on the Titan supercomputer as well as lessons learned in the process of efficient operation of GPUs at scale. These experiences are helpful to HPC sites which already have large-scale GPU clusters or plan to deploy GPUs in the future.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807666,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832812,,Computer architecture;Error correction codes;Graphics processing units;Hardware;Instruction sets;Reliability;Supercomputers,fault tolerant computing;graphics processing units;mainframes;parallel machines;system recovery,GPU computational capability;GPU errors;GPU system failures;Oak Ridge leadership computing facility;Titan supercomputer;data analysis;graphics processing units;reliability lessons;scientific simulations;system operations,,4,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
GraphBIG: understanding graph computing in the context of industrial solutions,L. Nai; Y. Xia; I. G. Tanase; H. Kim; C. Y. Lin,"Georgia Inst. of Technol., Atlanta, GA, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"With the emergence of data science, graph computing is becoming a crucial tool for processing big connected data. Although efficient implementations of specific graph applications exist, the behavior of full-spectrum graph computing remains unknown. To understand graph computing, we must consider multiple graph computation types, graph frameworks, data representations, and various data sources in a holistic way. In this paper, we present GraphBIG, a benchmark suite inspired by IBM System G project. To cover major graph computation types and data sources, GraphBIG selects representative datastructures, workloads and data sets from 21 real-world use cases of multiple application domains. We characterized GraphBIG on real machines and observed extremely irregular memory patterns and significant diverse behavior across different computations. GraphBIG helps users understand the impact of modern graph computing on the hardware architecture and enables future architecture and system research.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807626,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832843,,Arrays;Benchmark testing;Big data;Graphics processing units;Hardware;Prototypes,Big Data;graph theory,GraphBIG;IBM System G project;benchmark suite;big connected data processing;data representations;data science;data sources;graph applications;graph computing;graph frameworks;hardware architecture;industrial solutions;memory patterns;multiple graph computation types,,1,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
ELF: maximizing memory-level parallelism for GPUs with coordinated warp and fetch scheduling,J. J. K. Park; Y. Park; S. Mahlke,"Univ. of Michigan, Ann Arbor, MI, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Graphics processing units (GPUs) are increasingly utilized as throughput engines in the modern computer systems. GPUs rely on fast context switching between thousands of threads to hide long latency operations, however, they still stall due to the memory operations. To minimize the stalls, memory operations should be overlapped with other operations as much as possible to maximize memory-level parallelism (MLP). In this paper, we propose Earliest Load First (ELF) warp scheduling, which maximizes the MLP by giving higher priority to the warps that have the fewest instructions to the next memory load. ELF utilizes the same warp priority for the fetch scheduling so that both are coordinated. We also show that ELF reveals its full benefits when there are fewer memory conflicts and fetch stalls. Evaluations show that ELF can improve the performance by 4.1% and achieve total improvement of 11.9% when used with other techniques over commonly-used greedy-then-oldest scheduling.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807598,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832792,,Geophysical measurement techniques;Graphics processing units;Ground penetrating radar;Instruction sets;Kernel;Processor scheduling;Scheduling,graphics processing units;parallel processing;program compilers;scheduling;storage management,ELF fetch scheduling;ELF warp scheduling;GPU;MLP maximization;compiler;earliest load first;graphics processing unit;memory-level parallelism maximization,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
High-performance algebraic multigrid solver optimized for multi-core based distributed parallel systems,J. Park; M. Smelyanskiy; U. M. Yang; D. Mudigere; P. Dubey,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Algebraic Multigrid (AMG) is a linear solver, well known for its linear computational complexity and excellent parallelization scalability. As a result, AMG is expected to be a solver of choice for emerging extreme scale systems capable of delivering hundred Pflops and beyond. While node level performance of AMG is generally limited by memory bandwidth, achieving high bandwidth efficiency is challenging due to highly sparse irregular computation, such as triple sparse matrix products, sparse-matrix dense-vector multiplications, independent set coarsening algorithms, and smoothers such as Gauss-Seidel. We develop and analyze a highly optimized AMG implementation, based on the well-known HYPRE library. Compared to the HYPRE baseline implementation, our optimized implementation achieves 2.0ÌÑ speedup on a recent Intel<sup>å¨</sup> Xeon<sup>å¨</sup> Haswell processor. Combined with our other multi-node optimizations, this translates into similarly high speedups when weak-scaled multiple nodes. In addition, our implementation achieves 1.3ÌÑ speedup compared to AmgX, NVIDIA's high-performance implementation of AMG, running on K40c.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807603,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832828,,Bandwidth;Complexity theory;Convergence;Interpolation;Optimization;Scalability;Sparse matrices,algebra;computational complexity;multiprocessing systems;optimisation;parallel processing,AMG solver optimization;HYPRE library;algebraic multigrid solver optimization;computational complexity;multicore based distributed parallel system;parallelization scalability,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
IOrchestra: supporting high-performance data-intensive applications in the cloud via collaborative virtualization,R. C. Chiang; H. H. Huang; T. Wood; C. Liu; O. Spatscheck,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Multi-tier data-intensive applications are widely deployed in virtualized data centers for high scalability and reliability. As the response time is vital for user satisfaction, this requires achieving good performance at each tier of the applications in order to minimize the overall latency. However, in such virtualized environments, each tier (e.g., application, database, web) is likely to be hosted by different virtual machines (VMs) on multiple physical servers, where a guest VM is unaware of changes outside its domain, and the hypervisor also does not know the configuration and runtime status of a guest VM. As a result, isolated virtualization domains lend themselves to performance unpredictability and variance. In this paper, we propose IOrchestra, a holistic collaborative virtualization framework, which bridges the semantic gaps of I/O stacks and system information across multiple VMs, improves virtual I/O performance through collaboration from guest domains, and increases resource utilization in data centers. We present several case studies to demonstrate that IOrchestra is able to address numerous drawbacks of the current practice and improve the I/O latency of various distributed cloud applications by up to 31%.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807633,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832819,,Cloud computing;Collaboration;Monitoring;Performance evaluation;Semantics;Virtual machine monitors;Virtualization,cloud computing;computer centres;data handling;file servers;parallel processing;virtual machines;virtualisation,I/O performance improvement;IOrchestra;VM;cloud application;collaborative virtualization framework;data center;high-performance data-intensive application;hypervisor;physical server;virtual machine,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Recovering logical structure from Charm++ event traces,K. E. Isaacs; A. Bhatele; J. Lifflander; D. BÌ¦hme; T. Gamblin; M. Schulz; B. Hamann; P. T. Bremer,"Dept. of Comput. Sci., Univ. of California, Davis, Davis, CA, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Asynchrony and non-determinism in Charm++ programs present a significant challenge in analyzing their event traces. We present a new framework to organize event traces of parallel programs written in Charm++. Our reorganization allows one to more easily explore and analyze such traces by providing context through logical structure. We describe several heuristics to compensate for missing dependencies between events that currently cannot be easily recorded. We introduce a new task ordering that recovers logical structure from the non-deterministic execution order. Using the logical structure, we define several metrics to help guide developers to performance problems. We demonstrate our approach through two proxy applications written in Charm++. Finally, we discuss the applicability of this framework to other task-based runtimes and provide guidelines for tracing to support this form of analysis.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807634,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832823,,Algorithm design and analysis;Computational modeling;Measurement;Organizations;Program processors;Programming;Runtime,C++ language;parallel programming;program diagnostics;task analysis,Charm++ programs;event traces;logical structure recovery;nondeterministic execution order;parallel programs;task ordering,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
The in-silico lab-on-a-chip: petascale and high-throughput simulations of microfluidics at cell resolution,D. Rossinelli; Y. H. Tang; K. Lykov; D. Alexeev; M. Bernaschi; P. Hadjidoukas; M. Bisson; W. Joubert; C. Conti; G. Karniadakis; M. Fatica; I. Pivkin; P. Koumoutsakos,"ETH Zurich, Zurich, Switzerland","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"We present simulations of blood and cancer cell separation in complex microfluidic channels with subcellular resolution, demonstrating unprecedented time to solution, performing at 65.5% of the available 39.4 PetaInstructions/s in the 18, 688 nodes of the Titan supercomputer. These simulations outperform by one to three orders of magnitude the current state of the art in terms of numbers of simulated cells and computational elements. The computational setup emulates the conditions and the geometric complexity of microfluidic experiments and our results reproduce the experimental findings. These simulations provide sub-micron resolution while accessing time scales relevant to engineering designs. We demonstrate an improvement of up to 45X over competing state-of-the-art solvers, thus establishing the frontiers of simulations by particle based methods. Our simulations redefine the role of computational science for the development of microfluidics -- a technology that is becoming as important to medicine as integrated circuits have been to computers.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807677,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832776,,Biological system modeling;Blood;Computational modeling;Computer architecture;Mathematical model;Supercomputers;Throughput,biology computing;blood;cancer;digital simulation;lab-on-a-chip;microfluidics;parallel machines,PetaInstructions;Titan supercomputer;blood;cancer cell separation;cell resolution;computational science;geometric complexity;high-throughput simulation;in-silico lab-on-a-chip;medicine;microfluidic channel;microfluidic experiment;petascale simulation;subcellular resolution,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Scalable sparse tensor decompositions in distributed memory systems,O. Kaya; B. UÌ¤ar,"ENS Lyon, Inria, LIP, UCB Lyon 1, Lyon, France","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,11,"We investigate an efficient parallelization of the most common iterative sparse tensor decomposition algorithms on distributed memory systems. A key operation in each iteration of these algorithms is the matricized tensor times Khatri-Rao product (MTTKRP). This operation amounts to element-wise vector multiplication and reduction depending on the sparsity of the tensor. We investigate a fine and a coarse-grain task definition for this operation, and propose hypergraph partitioning-based methods for these task definitions to achieve the load balance as well as reduce the communication requirements. We also design a distributed memory sparse tensor library, HyperTensor, which implements a well-known algorithm for the CANDECOMP-/PARAFAC (CP) tensor decomposition using the task definitions and the associated partitioning methods. We use this library to test the proposed implementation of MTTKRP in CP decomposition context, and report scalability results up to 1024 MPI ranks. We observed up to 194 fold speedups using 512 MPI processes on a well-known real world data, and significantly better performance results with respect to a state of the art implementation.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807624,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832851,,Algorithm design and analysis;Approximation algorithms;Computed tomography;Libraries;Signal processing algorithms;Sparse matrices;Tensile stress,application program interfaces;distributed memory systems;graph theory;matrix algebra;message passing;parallel processing;resource allocation;tensors,CANDECOMP-/PARAFAC tensor decomposition;CP tensor decomposition;MPI;MTTKRP;distributed memory system;hypergraph partitioning;load balancing;matricized tensor times Khatri-Rao product;message passing interface;parallelization;sparse tensor decomposition;task definition,,3,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
The Spack package manager: bringing order to HPC software chaos,T. Gamblin; M. LeGendre; M. R. Collette; G. L. Lee; A. Moody; B. R. de Supinski; S. Futral,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Large HPC centers spend considerable time supporting software for thousands of users, but the complexity of HPC software is quickly outpacing the capabilities of existing software management tools. Scientific applications require specific versions of compilers, MPI, and other dependency libraries, so using a single, standard software stack is infeasible. However, managing many configurations is difficult because the configuration space is combinatorial in size. We introduce Spack, a tool used at Lawrence Livermore National Laboratory to manage this complexity. Spack provides a novel, recursive specification syntax to invoke parametric builds of packages and dependencies. It allows any number of builds to coexist on the same system, and it ensures that installed packages can find their dependencies, regardless of the environment. We show through real-world use cases that Spack supports diverse and demanding applications, bringing order to HPC software chaos.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807623,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832814,,Complexity theory;Libraries;Operating systems;Production;Syntactics,chaos;configuration management;formal specification;message passing;parallel processing;program compilers;software management;software tools,HPC software chaos;Lawrence Livermore National Laboratory;MPI;Spack package manager;compilers;configuration space;recursive specification syntax;software management tools;software stack,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Parallel implementation and performance optimization of the configuration-interaction method,H. Shan; S. Williams; C. Johnson; K. McElvain; W. E. Ormand,"Comput. Res. Div., Lawrence Berkeley Lab., Berkeley, CA, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"The configuration-interaction (CI) method, long a popular approach to describe quantum many-body systems, is cast as a very large sparse matrix eigenpair problem with matrices whose dimension can exceed one billion. Such formulations place high demands on memory capacity and memory bandwidth --- two quantities at a premium today. In this paper, we describe an efficient, scalable implementation, BIGSTICK, which, by factorizing both the basis and the interaction into two levels, can reconstruct the nonzero matrix elements on the fly, reduce the memory requirements by one or two orders of magnitude, and enable researchers to trade reduced resources for increased computational time. We optimize BIGSTICK on two leading HPC platforms --- the Cray XC30 and the IBM Blue Gene/Q. Specifically, we not only develop an empirically-driven load balancing strategy that can evenly distribute the matrix-vector multiplication across 256K threads, we also developed techniques that improve the performance of the Lanczos reorthogonalization. Combined, these optimizations improved performance by 1.3-8ÌÑ depending on platform and configuration.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807618,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832783,,Eigenvalues and eigenfunctions;Load management;Manganese;Neutrons;Physics;Protons;Sparse matrices,eigenvalues and eigenfunctions;matrix algebra;parallel processing;resource allocation;software performance evaluation;vectors,BIGSTICK;CI method;Cray XC30;HPC platform;IBM Blue Gene/Q;configuration-interaction method;load balancing strategy;matrix-vector multiplication;parallel implementation;performance optimization;sparse matrix eigenpair problem,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Monetary cost optimizations for MPI-based HPC applications on Amazon clouds: checkpoints and replicated execution,Y. Gong; B. He; A. C. Zhou,"Interdiscipl. Grad. Sch., Nanyang Technol. Univ., Singapore, Singapore","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"In this paper, we propose monetary cost optimizations for MPI-based applications with deadline constraints on Amazon EC2. Particularly, we consider to utilize two kinds of Amazon EC2 instances (on-demand and spot instances). As a spot instance can fail at any time due to out-of-bid events, fault tolerant executions are necessary. Through detailed studies, we have found that two common fault tolerant mechanisms, i.e., checkpoints and replicated executions, are complementary for cost-effective MPI executions on spot instances. We formulate the optimization problem and propose a novel cost model to minimize the expected monetary cost. The experimental results with NPB benchmarks on Amazon EC2 demonstrate that 1) it is feasible to run MPI applications with performance constraints on spot instances, 2) our proposal achieves significant monetary cost reduction compared to the state-of-the-art algorithm and 3) it is necessary to adaptively choose checkpoint and replication techniques for cost-effective and reliable MPI executions on Amazon EC2.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807612,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832806,,Cloud computing;Computational modeling;Fault tolerance;Fault tolerant systems;History;Optimization;Pricing,application program interfaces;checkpointing;cloud computing;cost reduction;fault tolerant computing;message passing;optimisation;parallel programming,Amazon EC2 instance;Amazon cloud;MPI-based HPC application;checkpoint;cost model;cost-effective MPI execution;deadline constraint;expected monetary cost minimization;fault tolerant execution;fault tolerant mechanism;monetary cost optimization;monetary cost reduction;on-demand instance;performance constraint;replicated execution;spot instance,,1,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Full correlation matrix analysis of fMRI data on Intelå¨ Xeon Phi‰ã¢ coprocessors,Y. Wang; M. J. Anderson; J. D. Cohen; A. Heinecke; K. Li; N. Satish; N. Sundaram; N. B. Turk-Browne; T. L. Willke,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Full correlation matrix analysis (FCMA) is an unbiased approach for exhaustively studying interactions among brain regions in functional magnetic resonance imaging (fMRI) data from human participants. In order to answer neuroscientific questions efficiently, we are developing a closed-loop analysis system with FCMA on a cluster of nodes with Intel<sup>å¨</sup> Xeon Phi‰ã¢ coprocessors. Here we propose several ideas for data-driven algorithmic modification to improve the performance on the coprocessor. Our experiments with real datasets show that the optimized single-node code runs 5x-16x faster than the baseline implementation using the well-known Intel<sup>å¨</sup> MKL and LibSVM libraries, and that the cluster implementation achieves near linear speedup on 5760 cores.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807631,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832797,,Computer architecture;Coprocessors;Correlation;Hardware;Neuroscience;Real-time systems;Support vector machines,biomedical MRI;coprocessors;medical image processing,Intel MKL library;Intel Xeon Phi coprocessors;LibSVM library;closed-loop analysis system;data-driven algorithmic modification;fMRI data;full correlation matrix analysis;functional magnetic resonance imaging,,1,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Performance optimization for the k-nearest neighbors kernel on x86 architectures,C. D. Yu; J. Huang; W. Austin; B. Xiao; G. Biros,"Dept. of Comput. Sci., Univ. of Texas at Austin, Austin, TX, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Nearest neighbor search is a cornerstone problem in computational geometry, non-parametric statistics, and machine learning. For N points, exhaustive search requires quadratic work, but many fast algorithms reduce the complexity for exact and approximate searches. The common kernel (kNN kernel) in all these algorithms solves many small-size problems exactly using exhaustive search. We propose an efficient implementation and performance analysis for the kNN kernel on x86 architectures. By fusing the distance calculation with the neighbor selection, we are able to utilize memory throughput. We present an analysis of the algorithm and explain parameter selection. We perform an experimental study varying the size of the problem, the dimension of the dataset, and the number of nearest neighbors. Overall we observe significant speedups. For example, when searching for 16 neighbors in a point dataset with 1.6 million points in 64 dimensions, our kernel is over 4 times faster than existing methods.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807601,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832781,,Algorithm design and analysis;Approximation algorithms;Complexity theory;Computer architecture;Kernel;Optimization;Search problems,computer architecture;learning (artificial intelligence);pattern classification;search problems,computational geometry;dataset dimension;distance calculation;exhaustive search;k-nearest neighbors kernel;kNN kernel;machine learning;memory throughput;nearest neighbor search;neighbor selection;nonparametric statistics;parameter selection;x86 architectures,,1,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
CilkSpec: optimistic concurrency for Cilk,S. Aga; S. Krishnamoorthy; S. Narayanasamy,"Univ. of Michigan, Ann Arbor, MI, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Recursive parallel programming models such as Cilk strive to simplify the task of parallel programming by enabling a simple divide-and-conquer programming model. This model is effective in recursively partitioning work into smaller parts and combining their results. However, recursive work partitioning can impose additional constraints on concurrency than is implied by the true dependencies in a program. In this paper, we present a speculation-based approach to alleviate the concurrency constraints imposed by such recursive parallel programs. We design a runtime infrastructure that supports speculative execution and a predictor to accurately learn and identify opportunities to relax extraneous concurrency constraints. Experimental evaluation demonstrates that speculative relaxation of concurrency constraints can deliver gains of up to 1.6x on 30 cores over baseline Cilk.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807597,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832857,,Concurrent computing;Government;Kernel;Parallel programming;Runtime;Synchronization,concurrency (computers);divide and conquer methods;parallel programming,CilkSpec;divide-and-conquer programming;optimistic concurrency;recursive parallel programming;speculation-based approach,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Improving concurrency and asynchrony in multithreaded MPI applications using software offloading,K. Vaidyanathan; D. D. Kalamkar; K. Pamnany; J. R. Hammond; P. Balaji; D. Das; J. Park; B. JoÌ_,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"We present a new approach for multithreaded communication and asynchronous progress in MPI applications, wherein we offload communication processing to a dedicated thread. The central premise is that given the rapidly increasing core counts on modern systems, the improvements in MPI performance arising from dedicating a thread to drive communication outweigh the small loss of resources for application computation, particularly when overlap of communication and computation can be exploited. Our approach allows application threads to make MPI calls concurrently, enqueuing these as communication tasks to be processed by a dedicated communication thread. This not only guarantees progress for such communication operations, but also reduces load imbalance. Our implementation additionally significantly reduces the overhead of mutual exclusion seen in existing implementations for applications using MPI_THREAD_MULTIPLE. Our technique requires no modification to the application, and we demonstrate significant performance improvement (up to 2X) for QCD, 1-D FFT and deep learning CNN applications.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807602,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832804,,Data transfer;Government;Instruction sets;Message systems;Parallel processing;Standards,application program interfaces;concurrency control;message passing;multi-threading;resource allocation,1D FFT;MPI performance;MPI_THREAD_MULTIPLE;QCD;asynchronous progress;asynchrony;communication operation;communication processing;concurrency;concurrent MPI calls;deep learning CNN application;load imbalance reduction;multithreaded MPI application;multithreaded communication;mutual exclusion overhead;software offloading,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Understanding the propagation of transient errors in HPC applications,R. A. Ashraf; R. Gioiosa; G. Kestor; R. F. DeMara; C. Y. Cher; P. Bose,"Univ. of Central Florida, Orlando, FL, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Resiliency of exascale systems has quickly become an important concern for the scientific community. Despite its importance, still much remains to be determined regarding how faults disseminate or at what rate do they impact HPC applications. The understanding of where and how fast faults propagate could lead to more efficient implementation of application-driven error detection and recovery. In this work, we propose a fault propagation framework to analyze how faults propagate in MPI applications and to understand their vulnerability to faults. We employ a combination of compiler-level code transformation and instrumentation, along with a runtime checker. Using the information provided by our framework, we employ machine learning technique to derive application fault propagation models that can be used to estimate the number of corrupted memory locations at runtime.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807670,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832846,,Circuit faults;Computational modeling;Hardware;Measurement;Registers;Resilience;Transient analysis,learning (artificial intelligence);parallel processing;program processors;software fault tolerance,HPC application;application fault propagation models;application-driven error detection;application-driven error recovery;compiler-level code transformation;exascale systems;fault propagation;high performance computing;instrumentation;machine learning technique;memory locations;runtime checker;transient error propagation,,2,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Mantle: a programmable metadata load balancer for the ceph file system,M. A. Sevilla; N. Watkins; C. Maltzahn; I. Nassi; S. A. Brandt; S. A. Weil; G. Farnum; S. Fineberg,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Migrating resources is a useful tool for balancing load in a distributed system, but it is difficult to determine when to move resources, where to move resources, and how much of them to move. We look at resource migration for file system metadata and show how CephFS's dynamic subtree partitioning approach can exploit varying degrees of locality and balance because it can partition the namespace into variable sized units. Unfortunately, the current metadata balancer is complicated and difficult to control because it struggles to address many of the general resource migration challenges inherent to the metadata management problem. To help decouple policy from mechanism, we introduce a programmable storage system that lets the designer inject custom balancing logic. We show the flexibility and transparency of this approach by replicating the strategy of a state-of-the-art metadata balancer and conclude by comparing this strategy to other custom balancers on the same system.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807607,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832795,,Distributed databases;File systems;Heart beat;Measurement;Metadata;Radiation detectors;Servers,meta data;network operating systems;resource allocation;storage management,Ceph file system;CephFS dynamic subtree partitioning approach;Mantle;custom balancing logic;distributed file system;file system metadata;metadata management problem;namespace partitioning;programmable metadata load balancer;programmable storage system;resource migration,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
An extreme-scale implicit solver for complex PDEs: highly heterogeneous flow in earth's mantle,J. Rudi; A. C. I. Malossi; T. Isaac; G. Stadler; M. Gurnis; P. W. J. Staar; Y. Ineichen; C. Bekas; A. Curioni; O. Ghattas,"Inst. for Comput. Eng. & Sci., Univ. of Texas at Austin, Austin, TX, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Mantle convection is the fundamental physical process within earth's interior responsible for the thermal and geological evolution of the planet, including plate tectonics. The mantle is modeled as a viscous, incompressible, non-Newtonian fluid. The wide range of spatial scales, extreme variability and anisotropy in material properties, and severely nonlinear rheology have made global mantle convection modeling with realistic parameters prohibitive. Here we present a new implicit solver that exhibits optimal algorithmic performance and is capable of extreme scaling for hard PDE problems, such as mantle convection. To maximize accuracy and minimize runtime, the solver incorporates a number of advances, including aggressive multi-octree adaptivity, mixed continuous-discontinuous discretization, arbitrarily-high-order accuracy, hybrid spectral/geometric/algebraic multigrid, and novel Schur-complement preconditioning. These features present enormous challenges for extreme scalability. We demonstrate that-contrary to conventional wisdom-algorithmically optimal implicit solvers can be designed that scale out to 1.5 million cores for severely nonlinear, ill-conditioned, heterogeneous, and anisotropic PDEs.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807675,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832779,,Complexity theory;Convection;Earth;Mathematical model;Plastics;Rheology;Viscosity,Earth mantle;compressible flow;geophysical fluid dynamics;geophysics computing;non-Newtonian flow;octrees;optimisation;partial differential equations;rheology;tectonics,Earth interior;Earth mantle;PDE;Schur-complement preconditioning;extreme-scale implicit solver;geological evolution;heterogeneous flow;hybrid spectral/geometric/algebraic multigrid;incompressible fluid;mantle convection;material properties;mixed continuous-discontinuous discretization;multioctree adaptivity;nonNewtonian fluid;nonlinear rheology;optimal algorithmic performance;partial differential equation;plate tectonics;thermal evolution;viscous fluid,,1,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
PGX.D: a fast distributed graph processing engine,S. Hong; S. Depner; T. Manhardt; J. Van Der Lugt; M. Verstraaten; H. Chafi,"Oracle Labs., USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Graph analysis is a powerful method in data analysis. Although several frameworks have been proposed for processing large graph instances in distributed environments, their performance is much lower than using efficient single-machine implementations provided with enough memory. In this paper, we present a fast distributed graph processing system, namely PGX.D. We show that PGX.D outperforms other distributed graph systems like GraphLab significantly (3x - 90x). Furthermore, PGX.D on 4 to 16 machines is also faster than an implementation optimized for single-machine execution. Using a fast cooperative context-switching mechanism, we implement PGX.D as a low-overhead, bandwidth-efficient communication framework that supports remote data-pulling patterns. Moreover, PGX.D achieves large traffic reduction and good workload balance by applying selective ghost nodes, edge partitioning, and edge chunking transparently to the user. Our analysis confirms that each of these features is indeed crucial for overall performance of certain kinds of graph algorithms. Finally, we advocate the use of balanced beefy clusters where the sustained random DRAM-access bandwidth in aggregate is matched with the bandwidth of the underlying interconnection fabric.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807620,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832832,,Algorithm design and analysis;Bandwidth;Clustering algorithms;Computational modeling;Data models;Kernel;Programming,data analysis;distributed processing;graph theory;pattern clustering;resource allocation,DRAM;PGX.D;access bandwidth;balanced beefy clusters;bandwidth-efficient communication;cooperative context-switching mechanism;data analysis;distributed graph processing engine;edge chunking;edge partitioning;ghost nodes;graph algorithms;graph analysis;low-overhead communication;remote data-pulling patterns;traffic reduction;workload balance,,4,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Regent: a high-productivity programming language for HPC with logical regions,E. Slaughter; W. Lee; S. Treichler; M. Bauer; A. Aiken,"Stanford Univ., Stanford, CA, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"We present Regent, a high-productivity programming language for high performance computing with logical regions. Regent users compose programs with tasks (functions eligible for parallel execution) and logical regions (hierarchical collections of structured objects). Regent programs appear to execute sequentially, require no explicit synchronization, and are trivially deadlock-free. Regent's type system catches many common classes of mistakes and guarantees that a program with correct serial execution produces identical results on parallel and distributed machines. We present an optimizing compiler for Regent that translates Regent programs into efficient implementations for Legion, an asynchronous task-based model. Regent employs several novel compiler optimizations to minimize the dynamic overhead of the runtime system and enable efficient operation. We evaluate Regent on three benchmark applications and demonstrate that Regent achieves performance comparable to hand-tuned Legion.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807629,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832855,,C++ languages;Image color analysis;Optimization;Programming;Reactive power;Runtime;Semiconductor optical amplifiers,data flow analysis;parallel processing;program compilers;programming languages,HPC;Legion;Regent;asynchronous task-based model;compiler optimization;distributed machine;dynamic overhead minimization;high performance computing;logical region;parallel machine;programming language;runtime system;serial execution,,1,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Clock delta compression for scalable order-replay of non-deterministic parallel applications,K. Sato; D. H. Ahn; I. Laguna; G. L. Lee; M. Schulz,"Lawrence Livermore Nat. Lab., Livermore, CA, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"The ability to record and replay program execution helps significantly in debugging non-deterministic MPI applications by reproducing message-receive orders. However, the large amount of data that traditional record-and-reply techniques record precludes its practical applicability to massively parallel applications. In this paper, we propose a new compression algorithm, Clock Delta Compression (CDC), for scalable record and replay of non-deterministic MPI applications. CDC defines a reference order of message receives based on a totally ordered relation using Lamport clocks, and only records the differences between this reference logical-clock order and an observed order. Our evaluation shows that CDC significantly reduces the record data size. For example, when we apply CDC to Monte Carlo particle transport Benchmark (MCB), which represents common non-deterministic communication patterns, CDC reduces the record size by approximately two orders of magnitude compared to traditional techniques and incurs between 13.1% and 25.5% of runtime overhead.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807642,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832836,,Clocks;Computational modeling;Debugging;Decoding;Encoding;Monte Carlo methods;Parallel processing,Monte Carlo methods;application program interfaces;clocks;data compression;message passing;parallel processing;program debugging,CDC;MCB;MPI application debugging;Monte Carlo particle transport benchmark;clock delta compression;message-passing interface;order-replay;parallel application,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
HipMer: an extreme-scale de novo genome assembler,E. Georganas; A. BuluÌ¤; J. Chapman; S. Hofmeyr; C. Aluru; R. Egan; L. Oliker; D. Rokhsar; K. Yelick,"Comput. Res. Div., Lawrence Berkeley Nat. Lab., Berkeley, CA, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,11,"De novo whole genome assembly reconstructs genomic sequences from short, overlapping, and potentially erroneous DNA segments and is one of the most important computations in modern genomics. This work presents HipMer, the first high-quality end-to-end de novo assembler designed for extreme scale analysis, via efficient parallelization of the Meraculous code. First, we significantly improve scalability of parallel k-mer analysis for complex repetitive genomes that exhibit skewed frequency distributions. Next, we optimize the traversal of the de Bruijn graph of k-mers by employing a novel communication-avoiding parallel algorithm in a variety of use-case scenarios. Finally, we parallelize the Meraculous scaffolding modules by leveraging the one-sided communication capabilities of the Unified Parallel C while effectively mitigating load imbalance. Large-scale results on a Cray XC30 using grand-challenge genomes demonstrate efficient performance and scalability on thousands of cores. Overall, our pipeline accelerates Meraculous performance by orders of magnitude, enabling the complete assembly of the human genome in just 8.4 minutes on 15K cores of the Cray XC30, and creating unprecedented capability for extreme-scale genomic analysis.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807664,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832788,,Algorithm design and analysis;Bioinformatics;DNA;Genomics;Pipelines;Scalability;Sequential analysis,biology computing;genomics;graph theory;molecular biophysics;parallel algorithms,HipMer;Meraculous code parallelization;Meraculous scaffolding module;Unified Parallel C;communication-avoiding parallel algorithm;complex repetitive genomes;de Bruijn graph;de novo genome assembler;extreme-scale genomic analysis;genome assembly;parallel k-mer analysis;skewed frequency distribution;use-case scenario,,1,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
GraphReduce: processing large-scale graphs on accelerator-based systems,D. Sengupta; S. L. Song; K. Agarwal; K. Schwan,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Recent work on real-world graph analytics has sought to leverage the massive amount of parallelism offered by GPU devices, but challenges remain due to the inherent irregularity of graph algorithms and limitations in GPU-resident memory for storing large graphs. We present GraphReduce, a highly efficient and scalable GPU-based framework that operates on graphs that exceed the device's internal memory capacity. GraphReduce adopts a combination of edge- and vertex-centric implementations of the Gather-Apply-Scatter programming model and operates on multiple asynchronous GPU streams to fully exploit the high degrees of parallelism in GPUs with efficient graph data movement between the host and device. GraphReduce-based programming is performed via device functions that include gatherMap, gatherReduce, apply, and scatter, implemented by programmers for the graph algorithms they wish to realize. Extensive experimental evaluations for a wide variety of graph inputs and algorithms demonstrate that GraphReduce significantly outperforms other competing out-of-memory approaches.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807655,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832802,,Acceleration;Computational modeling;Graphics processing units;Memory management;Parallel processing;Partitioning algorithms;Programming,data handling;graph theory;parallel processing,GPU devices;GPU-based framework;GraphReduce;accelerator-based systems;device functions;edge-centric implementations;gather-apply-scatter programming model;gatherMap;gatherReduce;graph analytics;graph data movement;internal memory capacity;large-scale graphs processing;multiple asynchronous GPU streams;parallelism;vertex-centric implementations,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Local recovery and failure masking for stencil-based applications at extreme scales,M. Gamell; K. Teranishi; M. A. Heroux; J. Mayo; H. Kolla; J. Chen; M. Parashar,"Rutgers Discovery Inf. Inst., Rutgers Univ., Piscataway, NJ, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Application resilience is a key challenge that has to be addressed to realize the exascale vision. Online recovery, even when it involves all processes, can dramatically reduce the overhead of failures as compared to the more traditional approach where the job is terminated and restarted from the last checkpoint. In this paper we explore how local recovery can be used for certain classes of applications to further reduce overheads due to resilience. Specifically we develop programming support and scalable runtime mechanisms to enable online and transparent local recovery for stencil-based parallel applications on current leadership class systems. We also show how multiple independent failures can be masked to effectively reduce the impact on the total time to solution. We integrate these mechanisms with the S3D combustion simulation, and experimentally demonstrate (using the Titan Cray-XK7 system at ORNL) the ability to tolerate high failure rates (i.e., node failures every 5 seconds) with low overhead while sustaining performance, at scales up to 262144 cores.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807672,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832844,,Checkpointing;Computational modeling;Programming;Protocols;Resilience;Runtime;Scalability,digital simulation;fault tolerant computing;parallel processing,S3D combustion simulation;application resilience;exascale vision;extreme scales;failure masking;leadership class systems;local recovery;online recovery;programming support;scalable runtime mechanisms;stencil-based applications;stencil-based parallel applications;transparent local recovery,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Memory access patterns: the missing piece of the multi-GPU puzzle,T. Ben-Nun; E. Levy; A. Barak; E. Rubin,"Dept. of Comput. Sci., Hebrew Univ. of Jerusalem, Jerusalem, Israel","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"With the increased popularity of multi-GPU nodes in modern HPC clusters, it is imperative to develop matching programming paradigms for their efficient utilization. In order to take advantage of the local GPUs and the low-latency high-throughput interconnects that link them, programmers need to meticulously adapt parallel applications with respect to load balancing, boundary conditions and device synchronization. This paper presents MAPS-Multi, an automatic multi-GPU partitioning framework that distributes the workload based on the underlying memory access patterns. The framework consists of host- and device-level APIs that allow programs to efficiently run on a variety of GPU and multi-GPU architectures. The framework implements several layers of code optimization, device abstraction, and automatic inference of inter-GPU memory exchanges. The paper demonstrates that the performance of MAPS-Multi achieves near-linear scaling on fundamental computational operations, as well as real-world applications in deep learning and multivariate analysis.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807611,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832793,,Containers;Graphics processing units;Instruction sets;Kernel;Memory management;Optimization;Programming,application program interfaces;file organisation;graphics processing units;parallel processing;pattern matching;resource allocation;synchronisation,MAPS-Multi;automatic inference;automatic multiGPU partitioning;boundary conditions;code optimization;deep learning;device abstraction;device level API;device synchronization;host level API;load balancing;low latency high throughput;matching programming paradigms;memory access patterns;modern HPC clusters;multiGPU architectures;multiGPU puzzle;multivariate analysis;parallel applications,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
STS-k: a multilevel sparse triangular solution scheme for NUMA multicores,H. Kabir; J. D. Booth; G. Aupy; A. Benoit; Y. Robert; P. Raghavan,"Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,11,"We consider techniques to improve the performance of parallel sparse triangular solution on non-uniform memory architecture multicores by extending earlier coloring and level set schemes for single-core multiprocessors. We develop STS-k, where k represents a small number of transformations for latency reduction from increased spatial and temporal locality of data accesses. We propose a graph model of data reuse to inform the development of STS-k and to prove that computing an optimal cost schedule is NP-complete. We observe significant speed-ups with STS-3 on 32-core Intel Westmere-Ex and 24-core AMD `MagnyCours' processors. Incremental gains solely from the 3-level transformations in STS-3 for a fixed ordering, correspond to reductions in execution times by factors of 1.4(Intel) and 1.5(AMD) for level sets and 2(Intel) and 2.2(AMD) for coloring. On average, execution times are reduced by a factor of 6(Intel) and 4(AMD) for STS-3 with coloring compared to a reference implementation using level sets.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807667,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832829,,Image color analysis;Level set;Lips;Multicore processing;Parallel processing;Program processors;Sparse matrices,computational complexity;graph theory;memory architecture;multiprocessing systems,24-core AMD MagnyCours processors;32-core Intel Westmere-Ex processors;NP-complete problem;NUMA multicores;STS-3;STS-k;coloring schemes;data access spatial locality;data access temporal locality;data reuse graph model;latency reduction;level set schemes;multilevel sparse triangular solution scheme;nonuniform memory architecture multicores;optimal cost schedule;parallel sparse triangular solution;single-core multiprocessors,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
An elegant sufficiency: load-aware differentiated scheduling of data transfers,R. Kettimuthu; G. Vardoyan; G. Agrawal; P. Sadayappan; I. Foster,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"We investigate the file transfer scheduling problem, where transfers among different endpoints must be scheduled to maximize pertinent metrics. We propose two new algorithms that exploit the fact that the aggregate bandwidth obtained over a network or at a storage system tends to increase with the number of concurrent transfers---but only up to a certain limit. The first algorithm, SEAL, uses runtime information and data-driven models to approximate system load and adapt transfer schedules and concurrency so as to maximize performance while avoiding saturation. We implement this algorithm using GridFTP as the transfer protocol and evaluate it using real transfer logs in a production WAN environment. Results show that SEAL can improve average slowdowns and turnaround times by up to 25% and worst-case slowdown and turnaround times by up to 50%, compared with the best-performing baseline scheme. Our second algorithm, STEAL, further leverages user-supplied categorization of transfers as either ""interactive"" (requiring immediate processing) or ""batch"" (less time-critical). Results show that STEAL reduces the average slowdown of interactive transfers by 63% compared to the best-performing baseline and by 21% compared to SEAL. For batch transfers, compared to the best-performing baseline, STEAL improves by 18% the utilization of the bandwidth unused by interactive transfers. By elegantly ensuring a sufficient, but not excessive, allocation of concurrency to the right transfers, we significantly improve overall performance despite constraints.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807660,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832820,,Aggregates;Bandwidth;Concurrent computing;Delays;Seals;Wide area networks,grid computing;protocols;scheduling;wide area networks,GridFTP;SEAL;STEAL;aggregate bandwidth;batch transfers;data transfers;data-driven models;file transfer scheduling problem;load-aware differentiated scheduling;pertinent metrics;production WAN environment;runtime information;storage system;system load approximation;transfer schedules;turnaround times;user-supplied categorization;worst-case slowdown times,,2,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Large-scale compute-intensive analysis via a combined in-situ and co-scheduling workflow approach,C. Sewell; K. Heitmann; H. Finkel; G. Zagaris; S. T. Parete-Koon; P. K. Fasel; A. Pope; N. Frontiere; L. t. Lo; B. Messer; S. Habib; J. Ahrens,"CCS-7, Los Alamos Nat. Lab., Los Alamos, NM, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,11,"Large-scale simulations can produce hundreds of terabytes to petabytes of data, complicating and limiting the efficiency of workflows. Traditionally, outputs are stored on the file system and analyzed in post-processing. With the rapidly increasing size and complexity of simulations, this approach faces an uncertain future. Trending techniques consist of performing the analysis in-situ, utilizing the same resources as the simulation, and/or off-loading subsets of the data to a compute-intensive analysis system. We introduce an analysis framework developed for HACC, a cosmological N-body code, that uses both in-situ and co-scheduling approaches for handling petabyte-scale outputs. We compare different analysis set-ups ranging from purely off-line, to purely in-situ to in-situ/co-scheduling. The analysis routines are implemented using the PISTON/VTK-m framework, allowing a single implementation of an algorithm that simultaneously targets a variety of GPU, multi-core, and many-core architectures.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807663,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832824,,Algorithm design and analysis;Analytical models;Computational modeling;Computer architecture;Data models;Libraries;Physics,N-body problems;data analysis;multiprocessing systems;scheduling;workflow management software,GPU;HACC;PISTON/VTK-m framework;co-scheduling workflow;cosmological N-body code;data off-loading subsets;file system storage;in-situ analysis;in-situ workflow;large-scale compute-intensive analysis;large-scale simulations;many-core architectures;multi-core architectures;petabyte-scale output handling;post-processing analysis,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Pushing back the limit of <i>ab-initio</i> quantum transport simulations on hybrid supercomputers,M. Calderara; S. BrÌ_ck; A. Pedersen; M. H. Bani-Hashemian; J. VandeVondele; M. Luisier,"Integrated. Syst. Lab., ETH Zurich, Zurich, Switzerland","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"The capabilities of CP2K, a density-functional theory package and OMEN, a nano-device simulator, are combined to study transport phenomena from first-principles in unprecedentedly large nanostructures. Based on the Hamiltonian and overlap matrices generated by CP2K for a given system, OMEN solves the Schroíödinger equation with open boundary conditions (OBCs) for all possible electron momenta and energies. To accelerate this core operation a robust algorithm called SplitSolve has been developed. It allows to simultaneously treat the OBCs on CPUs and the Schroíödinger equation on GPUs, taking advantage of hybrid nodes. Our key achievements on the Cray-XK7 Titan are (i) a reduction in time-to-solution by more than one order of magnitude as compared to standard methods, enabling the simulation of structures with more than 50000 atoms, (ii) a parallel efficiency of 97% when scaling from 756 up to 18564 nodes, and (iii) a sustained performance of 15 DP-PFlop/s.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807673,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832777,,Discrete Fourier transforms;Logic gates;Mathematical model;Nanostructures;Silicon;Standards;Transistors,graphics processing units;quantum computing,CP2K density-functional theory package;Cray-XK7 Titan;GPU;Hamiltonian matrix;OBC;OMEN nano-device simulator;Schrodinger equation;SplitSolve algorithm;ab-initio quantum transport simulations;graphics processing unit;hybrid supercomputers;open boundary conditions;overlap matrix,,1,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Analyzing and mitigating the impact of manufacturing variability in power-constrained supercomputing,Y. Inadomi; T. Patki; K. Inoue; M. Aoyagi; B. Rountree; M. Schulz; D. Lowenthal; Y. Wada; K. Fukazawa; M. Ueda; M. Kondo; I. Miyoshi,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"A key challenge in next-generation supercomputing is to effectively schedule limited power resources. Modern processors suffer from increasingly large power variations due to the chip manufacturing process. These variations lead to power inhomogeneity in current systems and manifest into performance inhomogeneity in power constrained environments, drastically limiting supercomputing performance. We present a first-of-its-kind study on manufacturing variability on four production HPC systems spanning four microarchitectures, analyze its impact on HPC applications, and propose a novel variation-aware power budgeting scheme to maximize effective application performance. Our low-cost and scalable budgeting algorithm strives to achieve performance homogeneity under a power constraint by deriving application-specific, module-level power allocations. Experimental results using a 1,920 socket system show up to 5.4X speedup, with an average speedup of 1.8X across all benchmarks when compared to a variation-unaware power allocation scheme.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807638,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832852,,Benchmark testing;Manufacturing;Nonhomogeneous media;Power demand;Power measurement;Random access memory;Resource management,parallel processing;power aware computing,application-specific module-level power allocation;chip manufacturing process;manufacturing variability;next-generation supercomputing;power constrained environment;power inhomogeneity;power-constrained supercomputing;supercomputing performance;variation-aware power budgeting scheme,,6,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
STELLA: a domain-specific tool for structured grid methods in weather and climate models,T. Gysi; C. Osuna; O. Fuhrer; M. Bianco; T. C. Schulthess,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Many high-performance computing applications solving partial differential equations (PDEs) can be attributed to the class of kernels using stencils on structured grids. Due to the disparity between floating point operation throughput and main memory bandwidth these codes typically achieve only a low fraction of peak performance. Unfortunately, stencil computation optimization techniques are often hardware dependent and lead to a significant increase in code complexity. We present a domain-specific tool, STELLA, which eases the burden of the application developer by separating the architecture dependent implementation strategy from the user-code and is targeted at multi- and manycore processors. On the example of a numerical weather prediction and regional climate model (COSMO) we demonstrate the usefulness of STELLA for a real-world production code. The dynamical core based on STELLA achieves a speedup factor of 1.8ÌÑ (CPU) and 5.8ÌÑ (GPU) with respect to the legacy code while reducing the complexity of the user code.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807627,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832815,,Atmospheric modeling;Computational modeling;Computer architecture;Graphics processing units;Hardware;Mathematical model;Meteorology,geophysics computing;parallel processing;partial differential equations;software architecture;software maintenance;weather forecasting,PDE;STELLA;climate model;domain-specific architecture;domain-specific tool;high-performance computing;legacy code;partial differential equation;structured grid method;weather model,,1,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Efficient implementation of quantum materials simulations on distributed CPU-GPU systems,R. Solca; A. Kozhevnikov; A. Haidar; S. Tomov; J. Dongarra; T. C. Schulthess,"Inst. for Theor. Phys., ETH Zurich, Zurich, Switzerland","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"We present a scalable implementation of the Linearized Augmented Plane Wave method for distributed memory systems, which relies on an efficient distributed, block-cyclic setup of the Hamiltonian and overlap matrices and allows us to turn around highly accurate 1000+ atom all-electron quantum materials simulations on clusters with a few hundred nodes. The implementation runs efficiently on standard multi-core CPU nodes, as well as hybrid CPU-GPU nodes. The key for the latter is a novel algorithm to solve the generalized eigenvalue problem for dense, complex Hermitian matrices on distributed hybrid CPU-GPU systems. Performance tests for Li-intercalated CoO2 supercells containing 1501 atoms demonstrate that high-accuracy, transferable quantum simulations can now be used in throughput materials search problems. While our application can benefit and get scalable performance through CPU-only libraries like ScaLAPACK or ELPA2, our new hybrid solver enables the efficient use of GPUs and shows that a hybrid CPU-GPU architecture scales to a desired performance using substantially fewer cluster nodes, and notably, is considerably more energy efficient than the traditional multi-core CPU only systems for such complex applications.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807654,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832784,,Computational modeling;Computer architecture;Eigenvalues and eigenfunctions;Electric potential;Libraries;Mathematical model;Random access memory,distributed memory systems;eigenvalues and eigenfunctions;graphics processing units;materials science computing;matrix algebra;quantum computing,ELPA2;Hamiltonian matrices;Li-intercalated CoO2 supercells;ScaLAPACK;all-electron quantum materials simulation;dense complex Hermitian matrices;distributed CPU-GPU systems;distributed memory system;efficient distributed block-cyclic setup;generalized eigenvalue problem;high-accuracy transferable quantum simulation;hybrid CPU-GPU architecture;hybrid CPU-GPU node;linearized augmented plane wave method;multicore CPU node;overlap matrices,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Elastic job bundling: an adaptive resource request strategy for large-scale parallel applications,F. Liu; J. B. Weissman,"Univ. of Minnesota, Minneapolis, MN, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"In today's batch queue HPC cluster systems, the user submits a job requesting a fixed number of processors. The system will not start the job until all of the requested resources become available simultaneously. When cluster workload is high, large sized jobs will experience long waiting time due to this policy. In this paper, we propose a new approach that dynamically decomposes a large job into smaller ones to reduce waiting time, and lets the application expand across multiple subjobs while continuously achieving progress. This approach has three benefits: (i) application turnaround time is reduced, (ii) system fragmentation is diminished, and (iii) fairness is promoted. Our approach does not depend on job queue time prediction but exploits available backfill opportunities. Simulation results have shown that our approach can reduce application mean turnaround time by up to 48%.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807610,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832807,,Degradation;Manganese;Mathematical model;Processor scheduling;Program processors;Shape,parallel processing;resource allocation,adaptive resource request strategy;backfill opportunities;batch queue HPC cluster systems;elastic job bundling strategy;high performance computing;job queue time prediction;large-scale parallel application,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
A kernel-independent FMM in general dimensions,W. B. March; B. Xiao; S. Tharakan; C. D. Yu; G. Biros,"Inst. for Comput. Eng. & Sci., Univ. of Texas, Austin, TX, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"We introduce a general-dimensional, kernel-independent, algebraic fast multipole method and apply it to kernel regression. The motivation for this work is the approximation of kernel matrices, which appear in mathematical physics, approximation theory, non-parametric statistics, and machine learning. Existing fast multipole methods are asymptotically optimal, but the underlying constants scale quite badly with the ambient space dimension. We introduce a method that mitigates this shortcoming; it only requires kernel evaluations and scales well with the problem size, the number of processors, and the ambient dimension---as long as the intrinsic dimension of the dataset is small. We test the performance of our method on several synthetic datasets. As a highlight, our largest run was on an image dataset with 10 million points in 246 dimensions.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807647,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832798,,Acceleration;Approximation algorithms;Complexity theory;Kernel;Machine learning algorithms;Skeleton;Transforms,data mining;learning (artificial intelligence);matrix algebra;parallel algorithms;regression analysis,algebraic fast multipole method;approximation theory;data mining;general-dimensional method;kernel evaluations;kernel matrices approximation;kernel regression;kernel-independent FMM;machine learning;mathematical physics;nonparametric statistics;parallel algorithm;synthetic datasets,,1,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Scientific benchmarking of parallel computing systems: twelve ways to tell the masses when reporting performance results,T. Hoefler; R. Belli,"Dept. of Comput. Sci., ETH Zurich, Zurich, Switzerland","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Measuring and reporting performance of parallel computers constitutes the basis for scientific advancement of high-performance computing (HPC). Most scientific reports show performance improvements of new techniques and are thus obliged to ensure reproducibility or at least interpretability. Our investigation of a stratified sample of 120 papers across three top conferences in the field shows that the state of the practice is lacking. For example, it is often unclear if reported improvements are deterministic or observed by chance. In addition to distilling best practices from existing work, we propose statistically sound analysis and reporting techniques and simple guidelines for experimental design in parallel computing and codify them in a portable benchmarking library. We aim to improve the standards of reporting research results and initiate a discussion in the HPC field. A wide adoption of our minimal set of rules will lead to better interpretability of performance results and improve the scientific culture in HPC.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807644,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832847,,Benchmark testing;Computer science;Guidelines;Parallel processing;Software;Standards;Time measurement,benchmark testing;parallel processing,HPC;high-performance computing;parallel computing system;performance result reporting;portable benchmarking library;reporting technique;scientific benchmarking;statistically sound analysis,,4,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Randomized algorithms to update partial singular value decomposition on a hybrid CPU/GPU cluster,I. Yamazaki; J. Kurzak; P. Luszczek; J. Dongarra,"Dept. of Electr. Eng. & Comput. Sci., Univ. of Tennessee, Knoxville, TN, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"For data analysis, a partial singular value decomposition (SVD) of the sparse matrix representing the data is a powerful tool. However, computing the SVD of a large matrix can take a significant amount of time even on a current high-performance supercomputer. Hence, there is a growing interest in a novel algorithm that can quickly compute the SVD for efficiently processing massive amounts of data that are being generated from many modern applications. To respond to this demand, in this paper, we study randomized algorithms that update the SVD as changes are made to the data, which is often more efficient than recomputing the SVD from scratch. Furthermore, in some applications, recomputing the SVD may not be possible because the original data, for which the SVD has been already computed, is no longer available. Our experimental results with the data sets for the Latent Semantic Indexing and population clustering demonstrate that these randomized algorithms can obtain the desired accuracy of the SVD with a small number of data accesses, and compared to the state-of-the-art updating algorithm, they often require much lower computational and communication costs. Our performance results on a hybrid CPU/GPU cluster show that these randomized algorithms can obtain significant speedups over the state-of-the-art updating algorithm.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807608,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832833,,Algorithm design and analysis;Approximation algorithms;Clustering algorithms;Singular value decomposition;Sociology;Sparse matrices;Statistics,data analysis;graphics processing units;indexing;microprocessor chips;parallel machines;pattern clustering;randomised algorithms;singular value decomposition;sparse matrices,SVD;data analysis;high performance supercomputer;hybrid CPU cluster;hybrid GPU cluster;latent semantic indexing;partial singular value decomposition;population clustering;randomized algorithms;sparse matrix,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Relative debugging for a highly parallel hybrid computer system,L. DeRose; A. Gontarek; A. Vose; R. Moench; D. Abramson; M. N. Dinh; C. Jin,"Cray Inc., St. Paul, MN, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Relative debugging traces software errors by comparing two executions of a program concurrently - one code being a reference version and the other faulty. Relative debugging is particularly effective when code is migrated from one platform to another, and this is of significant interest for hybrid computer architectures containing CPUs accelerators or coprocessors. In this paper we extend relative debugging to support porting stencil computation on a hybrid computer. We describe a generic data model that allows programmers to examine the global state across different types of applications, including MPI/OpenMP, MPI/OpenACC, and UPC programs. We present case studies using a hybrid version of the 'stellarator' particle simulation DELTA5D, on Titan at ORNL, and the UPC version of Shallow Water Equations on Crystal, an internal supercomputer of Cray. These case studies used up to 5,120 GPUs and 32,768 CPU cores to illustrate that the debugger is effective and practical.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807605,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832837,,Computational modeling;Data models;Debugging;Parallel processing;Programming;Runtime;Supercomputers,application program interfaces;coding errors;computer debugging;coprocessors;hybrid computers;parallel architectures;program debugging,CPU accelerators;CPU coprocessors;Cray internal supercomputer;MPI OpenACC programs;MPI OpenMP programs;ORNL;Shallow Water Equations on Crystal;Titan;UPC programs;faulty code;hybrid computer architectures;parallel hybrid computer system;porting stencil computation;reference code;relative debugging;software errors;stellarator particle simulation DELTA5D,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Data partitioning strategies for graph workloads on heterogeneous clusters,M. LeBeane; S. Song; R. Panda; J. H. Ryoo; L. K. John,"Dept. of Electr. & Comput. Eng., Univ. of Texas at AustinAustin, Austin, TX, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Large scale graph analytics are an important class of problem in the modern data center. However, while data centers are trending towards a large number of heterogeneous processing nodes, graph analytics frameworks still operate under the assumption of uniform compute resources. In this paper, we develop heterogeneity-aware data ingress strategies for graph analytics workloads using the popular PowerGraph framework. We illustrate how simple estimates of relative node computational throughput can guide heterogeneity-aware data partitioning algorithms to provide balanced graph cutting decisions. Our work enhances five online data ingress strategies from a variety of sources to optimize application execution for throughput differences in heterogeneous data centers. The proposed partitioning algorithms improve the runtime of several popular machine learning and data mining applications by as much as a 65% and on average by 32% as compared to the default, balanced partitioning approaches.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807632,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832830,,Big data;Clustering algorithms;Computational modeling;Engines;Partitioning algorithms;Synchronization;Throughput,Big Data;cloud computing;computer centres;distributed programming;graph theory;network analysis;resource allocation,Big Data computing;PowerGraph framework;cloud computing;data partitioning strategies;distributed programming languages;graph analytics workloads;graph cutting decisions;heterogeneity-aware data partitioning algorithms;heterogeneous clusters;large scale graph analytics;load balancing;modern data center,,2,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Finding the limits of power-constrained application performance,P. E. Bailey; A. Marathe; D. K. Lowenthal; B. Rountree; M. Schulz,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"As we approach exascale systems, power is turning from an optimization goal to a critical operating constraint. With power bounds imposed by both stakeholders and the limitations of existing infrastructure, we need to develop new techniques that work with limited power to extract maximum performance. In this paper, we explore this area and provide an approach to find the theoretical upper bound of computational performance on a per-application basis in hybrid MPI + OpenMP applications. We use a linear programming (LP) formulation to optimize application schedules under various power constraints, where a schedule consists of a DVFS state and number of OpenMP threads for each section of computation between consecutive MPI calls. We also provide a more flexible mixed integer-linear (ILP) formulation and show that the resulting schedules closely match schedules from the LP formulation. Across four applications, we use our LP-derived upper bounds to show that current approaches trail optimal, power-constrained performance by up to 41.1%. This demonstrates the untapped potential of current systems, and our LP formulation provides future optimization approaches with a quantitative optimization target.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807637,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832853,,Government;Linear programming;Message systems;Optimization;Runtime;Schedules;Upper bound,application program interfaces;integer programming;linear programming;message passing;power aware computing;program diagnostics;scheduling,DVFS state;OpenMP thread;application execution tracing;application schedule optimization;computational performance;consecutive MPI calls;exascale system;hybrid MPI-OpenMP application;linear programming;mixed ILP formulation;mixed integer-linear formulation;optimal power-constrained performance;power constraint,,7,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Runtime-driven shared last-level cache management for task-parallel programs,A. Pan; V. S. Pai,"Sch. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Task-parallel programming models with input annotation-based concurrency extraction at runtime present a promising paradigm for programming multicore processors. Through management of dependencies, task assignments, and orchestration, these models markedly simplify the programming effort for parallelization while exposing higher levels of concurrency. In this paper we show that for multicores with a shared last-level cache (LLC), the concurrency extraction framework can be used to improve the shared LLC performance. Based on the input annotations for future tasks, the runtime instructs the hardware to prioritize data blocks with future reuse while evicting blocks with no future reuse. These instructions allow the hardware to preserve all the blocks for at least some of the future tasks and evict dead blocks. This leads to a considerable improvement in cache efficiency over what is achieved by hardware-only replacement policies, which can replace blocks for all future tasks resulting in poor hit-rates for all future tasks. The proposed hardware-software technique leads to a mean improvement of 18% in application performance and a mean reduction of 26% in misses over a shared LLC managed by the Least Recently Used replacement policy for a set of input-annotated task-parallel programs using the OmpSs programming model implemented on the NANOS++ runtime. In contrast, the state-of-the-art thread-based partitioning scheme suffers an average performance loss of 2% and an average increase of 15% in misses over the baseline.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807625,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832785,,Computers;Concurrent computing;Hardware;Instruction sets;Multicore processing;Programming;Runtime,cache storage;concurrency control;data flow analysis;multiprocessing systems;parallel programming,LLC management;concurrency extraction;data block prioritization;hardware-software technique;input annotation;last-level cache management;multicore processor;system runtime;task-parallel programming model,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Fault tolerant MapReduce-MPI for HPC clusters,Y. Guo; W. Bland; P. Balaji; X. Zhou,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Building MapReduce applications using the Message-Passing Interface (MPI) enables us to exploit the performance of large HPC clusters for big data analytics. However, due to the lacking of native fault tolerance support in MPI and the incompatibility between the MapReduce fault tolerance model and HPC schedulers, it is very hard to provide a fault tolerant MapReduce runtime for HPC clusters. We propose and develop FT-MRMPI, the first fault tolerant MapReduce framework on MPI for HPC clusters. We discover a unique way to perform failure detection and recovery by exploiting the current MPI semantics and the new proposal of user-level failure mitigation. We design and develop the checkpoint/restart model for fault tolerant MapReduce in MPI. We further tailor the detect/resume model to conserve work for more efficient fault tolerance. The experimental results on a 256-node HPC cluster show that FT-MRMPI effectively masks failures and reduces the job completion time by 39%.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807617,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832808,,Delays;Engines;Fault tolerance;Fault tolerant systems;Runtime;Semantics;Servers,Big Data;checkpointing;data analysis;message passing;parallel processing;scheduling;software fault tolerance,256-node HPC cluster;Big Data analytics;FT-MRMPI;HPC schedulers;MapReduce fault tolerance model;checkpoint/restart model;detect/resume model;failure detection;failure recovery;fault tolerant MapReduce-MPI;message-passing interface;user-level failure mitigation,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Engineering inhibitory proteins with InSiPS: the in-silico protein synthesizer,A. Schoenrock; D. Burnside; H. Moteshareie; A. Wong; A. Golshani; F. Dehne,"Sch. of Comput. Sci., Carleton Univ., Ottawa, ON, Canada","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,11,"Engineered proteins are synthetic novel proteins (not found in nature) that are designed to fulfill a predetermined biological function. Such proteins can be used as molecular markers, inhibitory agents, or drugs. For example, a synthetic protein could bind to a critical protein of a pathogen, thereby inhibiting the function of the target protein and potentially reducing the impact of the pathogen. In this paper we present the In-Silico Protein Synthesizer (InSiPS), a massively parallel computational tool for the IBM Blue Gene/Q that is aimed at designing inhibitory proteins. More precisely, InSiPS designs proteins that are predicted to interact with a given target protein (and may inhibit the target's cellular functions) while leaving non-target proteins unaffected (to minimize side-effects). As proof-of-concepts, two InSiPS designed proteins have been synthesized in the lab and their inhibitory properties have been experimentally verified through wet-lab experimentation.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807630,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832799,,Databases;Next generation networking;Pathogens;Protein sequence;Synthesizers,bioinformatics;medical computing;parallel algorithms;proteins,InSiPS;engineering inhibitory proteins;insilico protein synthesizer;parallel algorithm;parallel computational tool,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
"Node variability in large-scale power measurements: perspectives from the Green500, Top500 and EEHPCWG",T. Scogland; J. Azose; D. Rohr; S. Rivoire; N. Bates; D. Hackenberg,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,11,"The last decade has seen power consumption move from an afterthought to the foremost design constraint of new supercomputers. Measuring the power of a supercomputer can be a daunting proposition, and as a result, many published measurements are extrapolated. This paper explores the validity of these extrapolations in the context of inter-node power variability and power variations over time within a run. We characterize power variability across nodes in systems at eight supercomputer centers across the globe. This characterization shows that the current requirement for measurements submitted to the Green500 and others is insufficient, allowing variations of up to 20% due to measurement timing and a further 10--15% due to insufficient sample sizes. This paper proposes new power and energy measurement requirements for supercomputers, some of which have been accepted for use by the Green500 and Top500, to ensure consistent accuracy.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807653,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832848,,Atmospheric measurements;Benchmark testing;Current measurement;Particle measurements;Power demand;Power measurement;Supercomputers,parallel processing;power aware computing;power measurement,EEHPCWG;Green500;HPC system;Top500;high performance computing;internode power variability;power measurement;supercomputer,,1,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Performance of random sampling for computing low-rank approximations of a dense matrix on GPUs,T. Mary; I. Yamazaki; J. Kurzak; P. Luszczek; S. Tomov; J. Dongarra,"UPS-IRIT, Univ. de Toulouse, Toulouse, France","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,11,"A low-rank approximation of a dense matrix plays an important role in many applications. To compute such an approximation, a common approach uses the QR factorization with column pivoting (QRCP). Though the reliability and efficiency of QRCP have been demonstrated, this deterministic approach requires costly communication at each step of the factorization. Since such communication is becoming increasingly expensive on modern computers, an alternative approach based on random sampling, which can be implemented using communication-optimal kernels, is becoming attractive. To study its potential, in this paper, we compare the performance of random sampling with that of QRCP on an NVIDIA Kepler GPU. Our performance results demonstrate that random sampling can be up to 12.8x faster than the deterministic approach for computing the approximation of the same accuracy. We also present the parallel scaling of the random sampling over multiple GPUs on a single compute node, showing a speedup of 3.8x over three Kepler GPUs. These results demonstrate the potential of the random sampling as an excellent computational tool for many applications, and its potential is likely to grow on the emerging computers with the increasing communication costs.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807613,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832834,,Approximation algorithms;Computers;Graphics processing units;Hardware;Kernel;Reliability;Standards,graphics processing units;matrix decomposition;parallel processing;random processes;sampling methods,NVIDIA Kepler GPU;QR factorization;QRCP;column pivoting;communication-optimal kernel;dense matrix;deterministic approach;low-rank approximation;parallel scaling;random sampling,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Frugal ECC: efficient and versatile memory error protection through fine-grained compression,J. Kim; M. Sullivan; S. L. Gong; M. Erez,,"SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Because main memory is vulnerable to errors and failures, large-scale systems and critical servers utilize error checking and correcting (ECC) mechanisms to meet their reliability requirements. We propose a novel mechanism, <i>Frugal ECC (FECC)</i>, that combines ECC with fine-grained compression to provide versatile protection that can be both stronger and lower overhead than current schemes, without sacrificing performance. FECC compresses main memory at cache-block granularity, using any left over space to store ECC information. Compressed data and its ECC information are then frequently read with a single access even without redundant memory chips; insufficiently compressed blocks require additional storage and accesses. As examples, we present chipkill-correct ECCs on a non-ECC DIMM with x4 chips and the first true chipkill-correct ECC for x8 devices using an ECC DIMM. FECC relies on a new Coverage-oriented-Compression that we developed specifically for the modest compression needs of ECC and for floating-point data.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807659,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832786,,DRAM chips;Error correction codes;Memory management;Organizations;Redundancy,,,,3,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Network endpoint congestion control for fine-grained communication,N. Jiang; L. Dennison; W. J. Dally,"NVIDIA Res., Santa Clara, CA, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,12,"Endpoint congestion in HPC networks creates tree saturation that is detrimental to performance. Endpoint congestion can be alleviated by reducing the injection rate of traffic sources, but requires fast reaction time to avoid congestion buildup. Congestion control becomes more challenging as application communication shift from traditional two-sided model to potentially fine-grained, one-sided communication embodied by various global address space programming models. Existing hardware solutions, such as Explicit Congestion Notification (ECN) and Speculative Reservation Protocol (SRP), either react too slowly or incur too much overhead for small messages. In this study we present two new endpoint congestion-control protocols, Small-Message SRP (SMSRP) and Last-Hop Reservation Protocol (LHRP), both targeted specifically for small messages. Experiments show they can quickly respond to endpoint congestion and prevent tree saturation in the network. Under congestion-free traffic conditions, the new protocols generate minimal overhead with performance comparable to networks with no endpoint congestion control.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807600,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832809,,Bandwidth;Fabrics;Government;Hardware;Payloads;Protocols;Routing,computer networks;protocols;telecommunication congestion control;telecommunication traffic,HPC networks;congestion-free traffic conditions;explicit congestion notification;fine-grained communication;global address space programming;high-performance computing networks;last-hop reservation protocol;network endpoint congestion control protocols;one-sided communication;small-message speculative reservation protocol;traffic sources;tree saturation,,2,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Exploring network optimizations for large-scale graph analytics,X. Que; F. Checconi; F. Petrini; X. Liu; D. Buono,"T.J. Watson Res. Center, IBM, Yorktown Heights, NY, USA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,10,"Graph analytics are arguably one of the most demanding workloads for high-performance systems and interconnection networks. Graph applications often display all-to-all, fine-grained, high-rate communication patterns that expose the limits of the network protocol stacks. Load and communication imbalance generate hard-to-predict network hot-spots, and may require computational steering due to unpredictable data distributions. In this paper we present a lightweight communication library, implemented ""on the metal"" of BlueGene/Q and POWER7 IH that we have used to support large-scale graph algorithms up to 96K processing nodes and 6 million threads. With this library we have explored several optimization techniques, including overlapped communication, non-blocking collectives, message aggregation, and computation in the network for special collective communication patterns, such as parallel prefix. The experimental results show significant performance improvements, ranging from 5X to 10X, when compared to equally optimized MPI implementations.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807661,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832800,,Algorithm design and analysis;Hardware;Libraries;Message systems;Optimization;Programming;Software,graph theory;optimisation;parallel processing,BlueGene/Q;POWER7 IH;collective communication patterns;computational steering;distributed graph processing;high-performance systems;interconnection networks;large-scale graph algorithms;large-scale graph analytics;lightweight communication library;message aggregation;network optimizations;network protocol stack limits;nonblocking collectives;optimization techniques;overlapped communication;unpredictable data distributions,,1,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Cost-effective diameter-two topologies: analysis and evaluation,G. Kathareios; C. Minkenberg; B. Prisacari; G. Rodriguez; T. Hoefler,"IBM Res. - Zurich, Ruschlikon, Switzerland","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,11,"HPC network topology design is currently shifting from high-performance, higher-cost Fat-Trees to more cost-effective architectures. Three diameter-two designs, the Slim Fly, Multi-Layer Full-Mesh, and Two-Level Orthogonal Fat-Tree excel in this, exhibiting a cost per endpoint of only 2 links and 3 router ports with lower end-to-end latency and higher scalability than traditional networks of the same total cost. However, other than for the Slim Fly, there is currently no clear understanding of the performance and routing of these emerging topologies. For each network, we discuss minimal, indirect random, and adaptive routing algorithms along with deadlock-avoidance mechanisms. Using these, we evaluate the performance of a series of representative workloads, from global uniform and worst-case traffic to the all-to-all and near-neighbor exchange patterns prevalent in HPC applications. We show that while all three topologies have similar performance, OFTs scale to twice as many endpoints at the same cost as the others.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807652,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832810,,Bandwidth;Network topology;Ports (Computers);Routing;Scalability;System recovery;Topology,computer networks;parallel processing;telecommunication network topology,HPC network topology design;all-to-all exchange pattern;cost-effective diameter-two topologies;deadlock-avoidance mechanism;high performance computing;multilayer full-mesh design;near-neighbor exchange pattern;slim fly design;two-level orthogonal fat-tree design,,3,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Profile-based power shifting in interconnection networks with on/off links,S. Miwa; H. Nakamura,"Univ. of Electro-Commun., Chofu, Japan","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20170126,2015,,,1,11,"Overprovisioning hardware devices and coordinating their power budgets are proposed to improve the application performance of future power-constrained HPC systems. This coordination process is called power shifting. Meanwhile, recent studies have revealed that on/off links can save network power in HPC systems. Future HPC systems will thus adopt on/off links in addition to power shifting. This paper explores power shifting in interconnection networks with on/off links. Given that on/off links keep network power low at application runtime, we can transfer appreciable quantities of power budgets on networks to other devices before an application runs. We thus propose a profile-based power shifting technique that allows HPC users to transfer the power budget remaining on networks to other devices at the time of job dispatch. Experimental results show that the proposed technique appreciably improves application performance under various power constraints.",,Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3,10.1145/2807591.2807639,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832811,,Data transfer;Hardware;Multiprocessor interconnection;Performance evaluation;Runtime;Servers;Supercomputers,inter-computer links;multiprocessor interconnection networks;parallel processing;power aware computing,coordination process;hardware device overprovisioning;interconnection networks;network power budgets;on/off links;power budget coordination;power transfer;power-constrained HPC systems;profile-based power shifting,,,,,,,15-20 Nov. 2015,,IEEE,IEEE Conference Publications
Scalable and High Performance Betweenness Centrality on the GPU,A. McLaughlin; D. A. Bader,"Sch. of Electr. & Comput. Eng., Georgia Inst. of Technol., Atlanta, GA, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,572,583,"Graphs that model social networks, numerical simulations, and the structure of the Internet are enormous and cannot be manually inspected. A popular metric used to analyze these networks is between ness centrality, which has applications in community detection, power grid contingency analysis, and the study of the human brain. However, these analyses come with a high computational cost that prevents the examination of large graphs of interest. Prior GPU implementations suffer from large local data structures and inefficient graph traversals that limit scalability and performance. Here we present several hybrid GPU implementations, providing good performance on graphs of arbitrary structure rather than just scale-free graphs as was done previously. We achieve up to 13x speedup on high-diameter graphs and an average of 2.71x speedup overall over the best existing GPU algorithm. We observe near linear speedup and performance exceeding tens of GTEPS when running between ness centrality on 192 GPUs.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.52,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013034,GPUs;Graph Algorithms;Parallel Algorithms,Algorithm design and analysis;Arrays;Graphics processing units;Instruction sets;Parallel processing;Scalability,complex networks;graph theory;graphics processing units;parallel processing,GPU algorithm;GTEPS;computational cost;graph traversals;high-diameter graphs;hybrid GPU implementations;local data structures;scalable high performance betweenness centrality;scale-free graphs,,11,,38,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Pardicle: Parallel Approximate Density-Based Clustering,M. M. A. Patwary; N. Satish; N. Sundaram; F. Manne; S. Habib; P. Dubey,,"SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,560,571,"DBSCAN is a widely used is density-based clustering algorithm for particle data well-known for its ability to isolate arbitrarily-shaped clusters and to filter noise data. The algorithm is super-linear (O(nlogn)) and computationally expensive for large datasets. Given the need for speed, we propose a fast heuristic algorithm for DBSCAN using density based sampling, which performs equally well in quality compared to exact algorithms, but is more than an order of magnitude faster. Our experiments on astrophysics and synthetic massive datasets (8.5 billion numbers) shows that our approximate algorithm is up to 56ÌÑ faster than exact algorithms with almost identical quality (Omega-Index ‰ä´ 0.99). We develop a new parallel DBSCAN algorithm, which uses dynamic partitioning to improve load balancing and locality. We demonstrate near-linear speedup on shared memory (15ÌÑ using 16 cores, single node Intel<sup>å¨</sup> Xeon<sup>å¨</sup> processor) and distributed memory (3917ÌÑ using 4096 cores, multinode) computers, with 2ÌÑ additional performance improvement using Intel<sup>å¨</sup> Xeon Phi‰ã¢ coprocessors. Additionally, existing exact algorithms can achieve up to 3.4 times speedup using dynamic partitioning.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.51,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013033,Density based clustering;Disjoint-set data structure;Union-Find algorithm;approximate clustering algorithm,Approximation algorithms;Approximation methods;Clustering algorithms;Data structures;Heuristic algorithms;Instruction sets;Partitioning algorithms,approximation theory;computational complexity;distributed shared memory systems;pattern clustering;resource allocation;sampling methods,Intel Xeon Phi coprocessor;approximate algorithm;arbitrarily-shaped cluster;astrophysics;density based sampling;density-based clustering algorithm;distributed memory;dynamic partitioning;exact algorithm;heuristic algorithm;load balancing;load locality;multinode computer;near-linear speedup;noise data filter;parallel DBSCAN algorithm;parallel approximate density-based clustering;pardicle;particle data;performance improvement;shared memory;single node Intel Xeon processor;synthetic massive datasets,,3,,56,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
[Copyright notice],,,"SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,iv,iv,Presents the copyright for this conference publication.,2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.3,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012185,,,,,,0,,,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
"Exploring Automatic, Online Failure Recovery for Scientific Applications at Extreme Scales",M. Gamell; D. S. Katz; H. Kolla; J. Chen; S. Klasky; M. Parashar,"NSF Cloud & Autonomic Comput. Center, Rutgers Univ., Piscataway, NJ, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,895,906,"Application resilience is a key challenge that must be addressed in order to realize the exascale vision. Process/node failures, an important class of failures, are typically handled today by terminating the job and restarting it from the last stored checkpoint. This approach is not expected to scale to exascale. In this paper we present Fenix, a framework for enabling recovery from process/node/blade/cabinet failures for MPI-based parallel applications in an online (i.e., Without disrupting the job) and transparent manner. Fenix provides mechanisms for transparently capturing failures, re-spawning new processes, fixing failed communicators, restoring application state, and returning execution control back to the application. To enable automatic data recovery, Fenix relies on application-driven, diskless, implicitly coordinated check pointing. Using the S3D combustion simulation running on the Titan Cray-XK7 production system at ORNL, we experimentally demonstrate Felix's ability to tolerate high failure rates (e.g., More than one per minute) with low overhead while sustaining performance.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.78,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013060,,Checkpointing;Combustion;Fault tolerance;Fault tolerant systems;Peer-to-peer computing;Runtime;Synchronization,application program interfaces;checkpointing;parallel processing,Fenix;MPI-based parallel application;S3D combustion simulation;application resilience;automatic data recovery;check pointing;exascale vision;extreme scales;node failures;online failure recovery;process-node-blade-cabinet failure;scientific application,,4,,59,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Quantitatively Modeling Application Resilience with the Data Vulnerability Factor,L. Yu; D. Li; S. Mittal; J. S. Vetter,"Illinois Inst. of Technol., Chicago, IL, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,695,706,"Recent strategies to improve the observable resilience of applications require the ability to classify vulnerabilities of individual components (e.g., Data structures, instructions) of an application, and then, selectively apply protection mechanisms to its critical components. To facilitate this vulnerability classification, it is important to have accurate, quantitative techniques that can be applied uniformly and automatically across real-world applications. Traditional methods cannot effectively quantify vulnerability, because they lack a holistic view to examine system resilience, and come with prohibitive evaluation costs. In this paper, we introduce a data-driven, practical methodology to analyze these application vulnerabilities using a novel resilience metric: the data vulnerability factor (DVF). DVF integrates knowledge from both the application and target hardware into the calculation. To calculate DVF, we extend a performance modeling language to provide a structured, fast modeling solution. We evaluate our methodology on six representative computational kernels, we demonstrate the significance of DVF by quantifying the impact of algorithm optimization on vulnerability, and by quantifying the effectiveness of specific hardware protection mechanisms.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.62,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013044,,Algorithm design and analysis;Analytical models;Computational modeling;Data models;Data structures;Hardware;Resilience,data protection;pattern classification;safety-critical software;software fault tolerance,DVF;application resilience modelling;data vulnerability factor;protection mechanism;representative computational kernel;vulnerability classification,,7,,41,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Faster Parallel Traversal of Scale Free Graphs at Extreme Scale with Vertex Delegates,R. Pearce; M. Gokhale; N. M. Amato,"Center for Appl. Sci. Comput., Lawrence Livermore Nat. Lab., Livermore, CA, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,549,559,"At extreme scale, irregularities in the structure of scale-free graphs such as social network graphs limit our ability to analyze these important and growing datasets. A key challenge is the presence of high-degree vertices (hubs), that leads to parallel workload and storage imbalances. The imbalances occur because existing partitioning techniques are not able to effectively partition high-degree vertices. We present techniques to distribute storage, computation, and communication of hubs for extreme scale graphs in distributed memory supercomputers. To balance the hub processing workload, we distribute hub data structures and related computation among a set of delegates. The delegates coordinate using highly optimized, yet portable, asynchronous broadcast and reduction operations. We demonstrate scalability of our new algorithmic technique using Breadth-First Search (BFS), Single Source Shortest Path (SSSP), K-Core Decomposition, and Page-Rank on synthetically generated scale-free graphs. Our results show excellent scalability on large scale-free graphs up to 131K cores of the IBM BG/P, and outperform the best known Graph500 performance on BG/P Intrepid by 15%.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.50,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013032,,Algorithm design and analysis;Benchmark testing;Computational modeling;Data structures;Partitioning algorithms;Scalability;Supercomputers,data structures;distributed memory systems;graph theory;parallel machines;tree searching,BFS;IBM BG-P;Page-Rank;SSSP;asynchronous broadcast operations;breadth-first search;distributed memory supercomputers;extreme scale graphs;high-degree vertices;hub data structures;k-core decomposition;parallel workload;scale free graph parallel traversal;single source shortest path;social network graphs;storage imbalances,,6,,27,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Domain Decomposition Preconditioners for Communication-Avoiding Krylov Methods on a Hybrid CPU/GPU Cluster,I. Yamazaki; S. Rajamanickam; E. G. Boman; M. Hoemmen; M. A. Heroux; S. Tomov,"Univ. of Tennessee, Knoxville, TN, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,933,944,"Krylov subspace projection methods are widely used iterative methods for solving large-scale linear systems of equations. Researchers have demonstrated that communication avoiding (CA) techniques can improve Krylov methods' performance on modern computers, where communication is becoming increasingly expensive compared to arithmetic operations. In this paper, we extend these studies by two major contributions. First, we present our implementation of a CA variant of the Generalized Minimum Residual (GMRES) method, called CAGMRES, for solving no symmetric linear systems of equations on a hybrid CPU/GPU cluster. Our performance results on up to 120 GPUs show that CA-GMRES gives a speedup of up to 2.5x in total solution time over standard GMRES on a hybrid cluster with twelve Intel Xeon CPUs and three Nvidia Fermi GPUs on each node. We then outline a domain decomposition framework to introduce a family of preconditioners that are suitable for CA Krylov methods. Our preconditioners do not incur any additional communication and allow the easy reuse of existing algorithms and software for the sub domain solves. Experimental results on the hybrid CPU/GPU cluster demonstrate that CA-GMRES with preconditioning achieve a speedup of up to 7.4x over CAGMRES without preconditioning, and speedup of up to 1.7x over GMRES with preconditioning in total solution time. These results confirm the potential of our framework to develop a practical and effective preconditioned CA Krylov method.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.81,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013063,,Central Processing Unit;Graphics processing units;Jacobian matrices;Kernel;Linear systems;Sparse matrices;Vectors,graphics processing units;iterative methods;mathematics computing,CA techniques;CAGMRES;Intel Xeon CPU;Krylov subspace projection methods;Nvidia Fermi GPU;communication-avoiding Krylov method;domain decomposition preconditioners;generalized minimum residual method;hybrid CPU-GPU cluster;iterative methods;large-scale linear systems of equations,,2,,19,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
High-Productivity Framework on GPU-Rich Supercomputers for Operational Weather Prediction Code ASUCA,T. Shimokawabe; T. Aoki; N. Onodera,"Tokyo Inst. of Technol., Tokyo, Japan","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,251,261,"The weather prediction code demands large computational performance to achieve fast and high-resolution simulations. Skillful programming techniques are required for obtaining good parallel efficiency on GPU supercomputers. Our framework-based weather prediction code ASUCA has achieved good scalability with hiding complicated implementation and optimizations required for distributed GPUs, contributing to increasing the maintainability, ASUCA is a next-generation high resolution meso-scale atmospheric model being developed by the Japan Meteorological Agency. Our framework automatically translates user-written stencil functions that update grid points and generates both GPU and CPU codes. User-written codes are parallelized by MPI with intra-node GPU peer-to-peer direct access. These codes can easily utilize optimizations such as overlapping technique to hide communication overhead by computation. Our simulations on the GPU-rich supercomputer TSUBAME 2.5 at the Tokyo Institute of Technology have demonstrated good strong and weak scalability achieving 209.6 TFlops in single precision for our largest model using 4,108 NVIDIA K20X GPUs.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.26,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013008,,Atmospheric modeling;Computational modeling;Graphics processing units;Mathematical model;Numerical models;Programming;Weather forecasting,application program interfaces;geophysics computing;graphics processing units;mainframes;message passing;parallel machines;peer-to-peer computing,CPU codes;GPU codes;GPU-rich supercomputer TSUBAME 2.5;Japan Meteorological Agency;MPI;NVIDIA K20X GPU;Tokyo Institute of Technology;high-productivity framework;intranode GPU peer-to-peer direct access;next-generation high resolution meso-scale atmospheric model;operational weather prediction code ASUCA;parallel efficiency;skillful programming techniques;user-written codes,,2,,26,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Best Practices and Lessons Learned from Deploying and Operating Large-Scale Data-Centric Parallel File Systems,S. Oral; J. Simmons; J. Hill; D. Leverman; F. Wang; M. Ezell; R. Miller; D. Fuller; R. Gunasekaran; Y. Kim; S. Gupta; D. T. S. S. Vazhkudai; J. H. Rogers; D. Dillow; G. M. Shipman; A. S. Bland,"Oak Ridge Leadership Comput. Facility, Oak Ridge Nat. Lab., Oak Ridge, TN, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,217,228,"The Oak Ridge Leadership Computing Facility (OLCF) has deployed multiple large-scale parallel file systems (PFS) to support its operations. During this process, OLCF acquired significant expertise in large-scale storage system design, file system software development, technology evaluation, benchmarking, procurement, deployment, and operational practices. Based on the lessons learned from each new PFS deployment, OLCF improved its operating procedures, and strategies. This paper provides an account of our experience and lessons learned in acquiring, deploying, and operating large-scale parallel file systems. We believe that these lessons will be useful to the wider HPC community.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.23,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013005,,Bandwidth;Benchmark testing;Computational modeling;Data models;Procurement;Servers;System performance,parallel processing;software engineering;storage management,HPC;PFS;data-centric parallel file system;file system software development;storage system design;technology evaluation,,3,,35,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
[Title page iii],,,"SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,iii,iii,Presents the title page of the proceedings record.,2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.2,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012184,,,,,,0,,,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Finding Constant from Change: Revisiting Network Performance Aware Optimizations on IaaS Clouds,Y. Gong; B. He; D. Li,"Interdiscipl. Grad. Sch., Nanyang Technol. Univ., Singapore, Singapore","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,982,993,"Network performance aware optimizations have long been an effective approach to optimizing distributed applications on traditional network environments. However, the assumptions of network topology or direct use of several measurements of pair-wise network performance for optimizations are no longer valid on IaaS clouds. Virtualization hides network topology from users, and direct use of network performance measurements may not represent long-term performance. To enable existing network performance aware optimizations on IaaS clouds, we propose to decouple constant component from dynamic network performance while minimizing the difference by a mathematical method called RPCA (Robust Principal Component Analysis). We use the constant component to guide network performance aware optimizations and demonstrate the efficiency of our approach by adopting network aware optimizations for collective communications of MPI and generic topology mapping as well as two real-world applications, N-body and conjugate gradient (CG). Our experiments on Amazon EC2 and simulations demonstrate significant performance improvement on guiding the optimizations.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.85,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013067,Cloud Computing;Network Performance Aware Optimization;RPCA,Bandwidth;Knowledge engineering;Network topology;Optimization;Sparse matrices;Topology;Virtual machining,application program interfaces;cloud computing;conjugate gradient methods;message passing;principal component analysis;virtualisation,Amazon EC2;CG;IaaS clouds;MPI;N-body;RPCA;collective communications;conjugate gradient;constant component;distributed application optimization;dynamic network performance;generic topology mapping;mathematical method;network performance aware optimizations;network topology;pair-wise network performance measurements;robust principal component analysis;virtualization,,1,,44,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Understanding the Effects of Communication and Coordination on Checkpointing at Scale,K. B. Ferreira; P. Widener; S. Levy; D. Arnold; T. Hoefler,"Scalable Syst. Software, Sandia Nat. Labs., Albuquerque, NM, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,883,894,"Fault-tolerance poses a major challenge for future large-scale systems. Active research into coordinated, uncoordinated, and hybrid check pointing systems has explored how the introduction of asynchrony can address anticipated scalability issues. However, few insights into selection and tuning of these protocols for applications at scale have emerged. In this paper, we use a simulation-based approach to show that local checkpoint activity in resilience mechanisms can significantly affect the performance of key workloads, even when less than 1% of a local node's compute time is allocated to resilience mechanisms (a very generous assumption). Specifically, we show that even though much work on uncoordinated check pointing has focused on optimizing message log volumes, local check pointing activity may dominate the overheads of this technique at scale. Our study shows that local checkpoints lead to process delays that can propagate through messaging relations to other processes causing a cascading series of delays. We demonstrate how to tune hierarchical uncoordinated check pointing protocols designed to reduce log volumes to significantly reduce these synchronization overheads at scale. Our work provides a critical analysis and comparison of coordinated and uncoordinated check pointing and enables users and system administrators to fine-tune the check pointing scheme to the application and system characteristics.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.77,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013059,,Checkpointing;Computational modeling;Delays;Mathematical model;Protocols;Resilience;Synchronization,checkpointing;fault tolerant computing;synchronisation,anticipated scalability issues;communication effects;coordination effects;critical analysis;fault-tolerance;hierarchical uncoordinated checkpointing protocols;hybrid checkpointing systems;large-scale systems;local checkpoint activity;local node compute time;message log volume optimization;process delays;resilience mechanisms;simulation-based approach;synchronization overheads;system administrators,,1,,41,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Practical Symbolic Race Checking of GPU Programs,P. Li; G. Li; G. Gopalakrishnan,"Sch. of Comput., Univ. of Utah, Salt Lake City, UT, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,179,190,"Even the careful GPU programmer can inadvertently introduce data races while writing and optimizing code. Currently available GPU race checking methods fall short either in terms of their formal guarantees, ease of use, or practicality. Existing symbolic methods: (1) do not fully support existing CUDA kernels, (2) may require user-specified assertions or invariants, (3) often require users to guess which inputs may be safely made concrete, (4) tend to explode in complexity when the number of threads is increased, and (5) explode in the face of thread-ID based decisions, especially in a loop. We present SESA, a new tool combining Symbolic Execution and Static Analysis to analyze C++ CUDA programs that overcomes all these limitations. SESA also scales well to handle non-trivial benchmarks such as Parboil and Lonestar, and is the only tool of its class that handles such practical examples. This paper presents SESA's methodological innovations and practical results.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.20,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013002,CUDA;Data Flow Analsis;Formal Verification;GPU;Parallelism;Symbolic Execution;Taint Analysis;Virtual Machine,Concrete;Graphics processing units;History;Indexes;Instruction sets;Kernel;Schedules,C++ language;graphics processing units;parallel architectures;program diagnostics,C++ CUDA program;CUDA kernel;GPU program;Lonestar;Parboil;SESA;static analysis;symbolic execution;symbolic race checking;thread-ID based decision,,5,,27,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Fence Scoping,C. Lin; V. Nagarajan; R. Gupta,"CSE Dept., Univ. of California, Riverside, Riverside, CA, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,105,116,"We observe that fence instructions used by programmers are usually only intended to order memory accesses within a limited scope. Based on this observation, we propose the concept fence scope which defines the scope within which a fence enforces the order of memory accesses, called scoped fence (S-Fence). S-Fence is a customizable fence, which enables programmers to express ordering demands by specifying the scope of fences when they only want to order part of memory accesses. At runtime, hardware uses the scope information conveyed by programmers to execute fence instructions in a manner that imposes fewer memory ordering constraints than a traditional fence, and hence improves program performance. Our experimental results show that the benefit of S-Fence hinges on the characteristics of applications and hardware parameters. A group of lock-free algorithms achieve peak speedups ranging from 1.13x to 1.34x, while full applications achieve speedups ranging from 1.04x to 1.23x.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.14,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012996,Fence instructions;Memory models;Scope,Buffer storage;Frequency selective surfaces;Hardware;Memory management;Program processors;Programming;Semantics,program control structures;program diagnostics;software performance evaluation,S-Fence;customizable fence;fence instructions;fence scoping;lock-free algorithms;memory accesses;program performance;scope information,,2,,43,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Scaling the Power Wall: A Path to Exascale,O. Villa; D. R. Johnson; M. Oconnor; E. Bolotin; D. Nellans; J. Luitjens; N. Sakharnykh; P. Wang; P. Micikevicius; A. Scudiero; S. W. Keckler; W. J. Dally,,"SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,830,841,"Modern scientific discovery is driven by an insatiable demand for computing performance. The HPC community is targeting development of supercomputers able to sustain 1 ExaFlops by the year 2020 and power consumption is the primary obstacle to achieving this goal. A combination of architectural improvements, circuit design, and manufacturing technologies must provide over a 20ÌÑ improvement in energy efficiency. In this paper, we present some of the progress NVIDIA Research is making toward the design of Exascale systems by tailoring features to address the scaling challenges of performance and energy efficiency. We evaluate several architectural concepts for a set of HPC applications demonstrating expected energy efficiency improvements resulting from circuit and packaging innovations such as low-voltage SRAM, low-energy signalling, and on-package memory. Finally, we discuss the scaling of these features with respect to future process technologies and provide power and performance projections for our Exascale research architecture.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.73,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013055,,Bandwidth;Computer architecture;Graphics processing units;Instruction sets;Kernel;Registers;Supercomputers,multiprocessing systems;parallel machines;performance evaluation;power aware computing,ExaFlops;HPC application;energy efficiency improvement;exascale system;performance projection;supercomputer development,,9,,43,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
24.77 Pflops on a Gravitational Tree-Code to Simulate the Milky Way Galaxy with 18600 GPUs,J. BÌ©dorf; E. Gaburov; M. S. Fujii; K. Nitadori; T. Ishiyama; S. Portegies Zwart,,"SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,54,65,"We have simulated, for the first time, the long term evolution of the Milky Way Galaxy using 51 billion particles on the Swiss Piz Daint supercomputer with our N-body gravitational tree-code Bonsai. Herein, we describe the scientific motivation and numerical algorithms. The Milky Way model was simulated for 6 billion years, during which the bar structure and spiral arms were fully formed. This improves upon previous simulations by using 1000 times more particles, and provides a wealth of new data that can be directly compared with observations. We also report the scalability on both the Swiss Piz Daint and the US ORNL Titan. On Piz Daint the parallel efficiency of Bonsai was above 95%. The highest performance was achieved with a 242 billion particle Milky Way model using 18600 GPUs on Titan, thereby reaching a sustained GPU and application performance of 33.49 Pflops and 24.77 Pflops respectively.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.10,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012192,Submitted in the categories: Scalability;Time-to-solution and Peak performance,Computational modeling;Graphics processing units;Gravity;Instruction sets;Supercomputers,Galaxy;N-body simulations (astronomical);graphics processing units;gravitation;parallel machines;tree codes,GPU;Milky Way Galaxy model;Milky Way Galaxy simulation;N-body gravitational tree-code Bonsai;Swiss Piz Daint supercomputer;US ORNL Titan;bar structure;graphics processing unit;long term evolution;numerical algorithms;parallel efficiency;scientific motivation;spiral arms,,6,,58,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Optimized Scheduling Strategies for Hybrid Density Functional theory Electronic Structure Calculations,W. Dawson; F. Gygi,"Dept. of Comput. Sci., Univ. of California Davis, Davis, CA, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,685,692,"Hybrid Density Functional Theory (DFT) has recently gained popularity as an accurate model of electronic interactions in chemistry and materials science applications. The most computationally expensive part of hybrid DFT simulations is the calculation of exchange integrals between pairs of electrons. We present strategies to achieve improved load balancing and scalability for the parallel computation of these integrals. First, we develop a cost model for the calculation, and utilize random search algorithms to optimize the data distribution and calculation schedule. Second, we further improve performance using partial data-replication to increase data availability across cores. We demonstrate these improvements using an implementation in the Qbox Density Functional Theory code on the Mira Blue Gene/Q computer at Argonne National Laboratory. We perform calculations in the range of 8k to 128k cores on two representative simulation samples from materials science and chemistry applications: liquid water and a metal-water interface.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.61,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013043,,Computational modeling;Discrete Fourier transforms;Load management;Optimal scheduling;Processor scheduling;Schedules,chemistry computing;density functional theory;molecular electronic states;parallel machines;resource allocation;scheduling;search problems,Argonne National Laboratory;Mira Blue Gene/Q computer;Qbox density functional theory code;calculation schedule;chemistry;data availability;data distribution;electronic interaction;exchange integral;hybrid DFT simulation;hybrid density functional theory electronic structure calculation;liquid water;load balancing;load scalability;materials science application;metal-water interface;optimized scheduling strategy;parallel computation;partial data-replication;random search algorithm;representative simulation,,0,,31,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
"The DRIHM Project: A Flexible Approach to Integrate HPC, Grid and Cloud Resources for Hydro-Meteorological Research",D. DÌÁgostino; A. Clematis; A. Galizia; A. Quarati; E. Danovaro; L. Roverelli; G. Zereik; D. KranzlmÌ_ller; M. Schiffers; N. G. Felde; C. Straube; O. Caumontz; E. Richard; L. Garrote; Q. Harphamk; H. R. A. Jagers; V. Dimitrijevic; L. Dekic; E. Fiorizz; F. Delogu; A. Parodi,"Inst. of Appl. Math. & Inf. Technol., Italy","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,536,546,"The distributed research infrastructure for hydrometeorology (DRIHM) project focuses on the development of an e-Science infrastructure to provide end-to-end hydro meteorological research (HMR) services (models, data, and post processing tools) by exploiting HPC, Grid and Cloud facilities. In particular, the DRIHM infrastructure supports the execution and analysis of high-resolution simulations through the definition of workflows composed by heterogeneous HMR models in a scalable and interoperable way, while hiding all the low level complexities. This contribution gives insights into best practices adopted to satisfy the requirements of an emerging multidisciplinary scientific community composed of earth and atmospheric scientists. To this end, DRIHM supplies innovative services leveraging high performance and distributed computing resources. Hydro meteorological requirements shape this IT infrastructure through an iterative ""learning-by-doing"" approach that permits tight interactions between the application community and computer scientists, leading to the development of a flexible, extensible, and interoperable framework.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.49,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013031,,Atmospheric modeling;Biological system modeling;Computational modeling;Data models;Forecasting;Meteorology;Predictive models,cloud computing;geophysics computing;grid computing;hydrology;meteorology;parallel processing,DRIHM project;HPC;cloud resources;distributed computing resources;distributed research infrastructure for hydrometeorology project;e-science infrastructure;grid resources;heterogeneous HMR models;high performance computing resources;hydrometeorological research;iterative learning-by-doing approach,,3,,49,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
RAHTM: Routing Algorithm Aware Hierarchical Task Mapping,A. H. Abdel-Gawad; M. Thottethodi; A. Bhatele,"Sch. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,325,335,"The mapping of MPI processes to compute nodes on a supercomputer can have a significant impact on communication performance. For high performance computing (HPC) applications with iterative communication, rich offline analysis of such communication can improve performance by optimizing the mapping. Unfortunately, current practices for at-scale HPC consider only the communication graph and network topology in solving this problem. We propose Routing Algorithm aware Hierarchical Task Mapping (RAHTM) which leverages the knowledge of the routing algorithm to improve task mapping. RAHTM achieves high quality mappings by combining (1) a divide-and-conquer strategy to achieve scalability, (2) a limited search of mappings, and (3) a linear programming based routing-aware approach to evaluate possible mappings in the search space. RAHTM achieves 20% reduction in the communication time and 9% reduction in the overall execution time for three communication-heavy benchmarks scaled up to 16,384 processes on a Blue Gene/Q platform.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.32,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013014,divide-and-conquer;linear programming;routing;task mapping;torus,Algorithm design and analysis;Bandwidth;Benchmark testing;Measurement;Network topology;Routing;Topology,divide and conquer methods;graph theory;parallel processing,Blue Gene/Q platform;HPC applications;MPI process mapping;RAHTM;communication graph;communication performance;communication-heavy benchmarks;divide-and-conquer strategy;high performance computing applications;iterative communication;linear programming;mapping optimization;network topology;offline analysis;performance improvement;routing algorithm aware hierarchical task mapping;supercomputer,,3,,21,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Microbank: Architecting Through-Silicon Interposer-Based Main Memory Systems,Y. H. Son; O. Seongil; H. Yang; D. Jung; J. H. Ahn; J. Kim; J. K. J. W. Lee,"Seoul Nat. Univ., Seoul, South Korea","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,1059,1070,"Through-Silicon Interposer (TSI) has recently been proposed to provide high memory bandwidth and improve energy efficiency of the main memory system. However, the impact of TSI on main memory system architecture has not been well explored. While TSI improves the I/O energy efficiency, we show that it results in an unbalanced memory system design in terms of energy efficiency as the core DRAM dominates overall energy consumption. To balance and enhance the energy efficiency of a TSI-based memory system, we propose ë_bank, a novel DRAM device organization in which each bank is partitioned into multiple smaller banks (or ë_banks) that operate independently like conventional banks with minimal area overhead. The ë_bank organization significantly increases the amount of bank-level parallelism to improve the performance and energy efficiency of the TSI-based memory system. The massive number of ë_banks reduces bank conflicts, hence simplifying the memory system design. We evaluated a sophisticated prediction-based DRAM page-management policy, which can improve performance by up to 20.5% in a conventional memory system without ë_banks. However, a ë_bank-based design does not require such a complex page-management policy and a simple open-page policy is often sufficient -- achieving within 5% of a perfect predictor. Our proposed ë_bank-based memory system improves the IPC and system energy-delay product by 1.62ÌÑ and 4.80ÌÑ, respectively, for memory-intensive SPEC 2006 benchmarks on average, over the baseline DDR3-based memory system.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.91,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013073,,Bandwidth;Data transfer;Decoding;Random access memory;Silicon;Substrates;Wires,DRAM chips;storage management,ë_bank;DDR3-based memory system;DRAM page-management policy;I/O energy efficiency;IPC;TSI-based memory system;bank-level parallelism;energy consumption;main memory system architecture;memory-intensive SPEC 2006 benchmark;microbank;prediction-based DRAM;through-silicon interposer,,4,,64,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
"Fail-in-Place Network Design: Interaction Between Topology, Routing Algorithm and Failures",J. Domke; T. Hoefler; S. Matsuoka,"Global Sci. Inf. & Comput. Center, Tokyo Inst. of Technol., Tokyo, Japan","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,597,608,"The growing system size of high performance computers results in a steady decrease of the mean time between failures. Exchanging network components often requires whole system downtime which increases the cost of failures. In this work, we study a fail-in-place strategy where broken network elements remain untouched. We show, that a fail-in-place strategy is feasible for todays networks and the degradation is manageable, and provide guidelines for the design. Our network failure simulation tool chain allows system designers to extrapolate the performance degradation based on expected failure rates, and it can be used to evaluate the current state of a system. In a case study of real-world HPC systems, we will analyze the performance degradation throughout the systems lifetime under the assumption that faulty network components are not repaired, which results in a recommendation to change the used routing algorithm to improve the network performance as well as the fail-in-place characteristic.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.54,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013036,Network design;availability;fail-in-place;fault tolerance;network management;network simulations;routing protocols,Algorithm design and analysis;Degradation;Hardware;Network topology;Routing;Throughput;Topology,computer network management;computer network performance evaluation;computer network reliability;telecommunication network routing;telecommunication network topology,broken network element;fail-in-place characteristic;fail-in-place network design;fail-in-place strategy;failure cost;failure rate;faulty network component;high performance computer;network failure simulation tool chain;network performance;performance degradation;real-world HPC system;routing algorithm;system designer;system downtime;system lifetime;topology,,6,,43,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Introduction,,,"SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,xv,xvi,Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.,2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.4,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012187,,,,,,0,,,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Parallelization of Reordering Algorithms for Bandwidth and Wavefront Reduction,K. I. Karantasis; A. Lenharth; D. Nguyen; M. J. GarzarÌÁn; K. Pingali,"Dept. of Comput. Sci., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,921,932,"Many sparse matrix computations can be speeded up if the matrix is first reordered. Reordering was originally developed for direct methods but it has recently become popular for improving the cache locality of parallel iterative solvers since reordering the matrix to reduce bandwidth and wave front can improve the locality of reference of sparse matrix-vector multiplication (SpMV), the key kernel in iterative solvers. In this paper, we present the first parallel implementations of two widely used reordering algorithms: Reverse Cut hill-McKee (RCM) and Sloan. On 16 cores of the Stampede supercomputer, our parallel RCM is 5.56 times faster on the average than a state-of-the-art sequential implementation of RCM in the HSL library. Sloan is significantly more constrained than RCM, but our parallel implementation achieves a speedup of 2.88X on the average over sequential HSL-Sloan. Reordering the matrix using our parallel RCM and then performing 100 SpMV iterations is twice as fast as using HSL-RCM and then performing the SpMV iterations, it is also 1.5 times faster than performing the SpMV iterations without reordering the matrix.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.80,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013062,,Arrays;Bandwidth;Heuristic algorithms;Indexes;Parallel processing;Runtime;Sparse matrices,cache storage;iterative methods;matrix multiplication;parallel algorithms;parallel machines;sparse matrices,HSL library;HSL-RCM;SpMV iteration;Stampede supercomputer;bandwidth reduction;cache locality;matrix reordering;parallel RCM;parallel implementation;parallel iterative solver;parallelization;reordering algorithm;reverse cut hill-McKee;sequential HSL-Sloan;sparse matrix computation;sparse matrix-vector multiplication;wavefront reduction,,2,,44,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Fault-Tolerant Dynamic Task Graph Scheduling,M. C. Kurt; S. Krishnamoorthy; K. Agrawal; G. Agrawal,,"SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,719,730,"In this paper, we present an approach to fault tolerant execution of dynamic task graphs scheduled using work stealing. In particular, we focus on selective and localized recovery of tasks in the presence of soft faults. From users, we elicit the basic task graph structure in terms of successor and predecessor relationships. The work-stealing-based algorithm to schedule such a task graph is augmented to enable recovery when the data and metadata associated with a task get corrupted. We use this redundancy, and knowledge of the task graph structure, to selectively recover from faults with low space and time overheads. We show that the fault tolerant design retains the essential properties of the underlying work stealing-based task scheduling algorithm, and that the fault tolerant execution is asymptotically optimal when task re-execution is taken into account. Experimental evaluation demonstrates the low cost of recovery under various fault scenarios.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.64,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013046,cilk;dag;fault tolerance;task graphs;work stealing,Arrays;Dynamic scheduling;Fault tolerance;Fault tolerant systems;Instruction sets;Radiation detectors;Scheduling algorithms,fault tolerant computing;graph theory;meta data;parallel processing;scheduling;task analysis,asymptotically optimal fault tolerant execution;fault tolerant design;fault-tolerant dynamic task graph scheduling;localized task recovery;metadata;space overheads;successor-predecessor relationships;task graph structure;time overheads;work stealing-based task scheduling algorithm,,7,,41,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
IndexFS: Scaling File System Metadata Performance with Stateless Caching and Bulk Insertion,K. Ren; Q. Zheng; S. Patil; G. Gibson,"Carnegie Mellon Univ., Pittsburgh, PA, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,237,248,"The growing size of modern storage systems is expected to exceed billions of objects, making metadata scalability critical to overall performance. Many existing distributed file systems only focus on providing highly parallel fast access to file data, and lack a scalable metadata service. In this paper, we introduce a middleware design called Index FS that adds support to existing file systems such as PVFS, Lustre, and HDFS for scalable high-performance operations on metadata and small files. Index FS uses a table-based architecture that incrementally partitions the namespace on a per-directory basis, preserving server and disk locality for small directories. An optimized log-structured layout is used to store metadata and small files efficiently. We also propose two client-based storm free caching techniques: bulk namespace insertion for creation intensive workloads such as N-N check pointing, and stateless consistent metadata caching for hot spot mitigation. By combining these techniques, we have demonstrated Index FS scaled to 128 metadata servers. Experiments show our out-of-core metadata throughput out-performing existing solutions such as PVFS, Lustre, and HDFS by 50% to two orders of magnitude.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.25,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013007,Distributed file systems;bulk insertion;file system metadata;log-structured merge tree;stateless caching,Compaction;Indexes;Middleware;Receivers;Scalability;Servers;Throughput,cache storage;checkpointing;meta data;middleware,HDFS;IndexFS;Lustre;N-N check pointing;PVFS;bulk insertion;bulk namespace insertion;client-based storm free caching techniques;creation intensive workloads;disk locality;distributed file systems;file system metadata performance scaling;high-performance operations;hot spot mitigation;log-structured layout optimization;metadata scalability;middleware design;namespace partitioning;out-of-core metadata throughput;per-directory basis;preserving server;stateless caching;stateless consistent metadata caching;storage systems;table-based architecture,,8,,62,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
A Unified Programming Model for Intra- and Inter-Node Offloading on Xeon Phi Clusters,M. Noack; F. Wende; T. Steinke; F. Cordes,"Zuse Inst. Berlin, Berlin-Dahlem, Germany","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,203,214,"Standard offload programming models for the Xeon Phi, e.g. Intel LEO and OpenMP 4.0, are restricted to a single compute node and hence a limited number of coprocessors. Scaling applications across a Xeon Phi cluster/supercomputer thus requires hybrid programming approaches, usually MPI+X. In this work, we present a framework based on heterogeneous active messages (HAM-Offload) that provides the means to offload work to local and remote (co)processors using a unified offload API. Since HAM-Offload provides similar primitives as current local offload frameworks, existing applications can be easily ported to overcome the single-node limitation while keeping the convenient offload programming model. We demonstrate the effectiveness of the framework by using it to enable a real-world application from the field of molecular dynamics to use multiple local and remote Xeon Phis. The evaluation shows good scaling behavior. Compared with LEO, performance is equal for large offloads and significantly better for small offloads.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.22,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013004,,Computational modeling;Coprocessors;Data transfer;Libraries;Low earth orbit satellites;Performance evaluation;Programming,application program interfaces;coprocessors;message passing;parallel machines,HAM-offload;Intel LEO;MPI+X;OpenMP 4.0;Xeon Phi clusters;compute node;coprocessors;heterogeneous active messages;hybrid programming approaches;inter-node offloading;intra-node offloading;molecular dynamics;scaling applications;scaling behavior;standard offload programming models;supercomputer;unified offload API;unified programming model,,2,,25,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Managing DRAM Latency Divergence in Irregular GPGPU Applications,N. Chatterjee; M. O'Connor; G. H. Loh; N. Jayasena; R. Balasubramonia,,"SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,128,139,"Memory controllers in modern GPUs aggressively reorder requests for high bandwidth usage, often interleaving requests from different warps. This leads to high variance in the latency of different requests issued by the threads of a warp. Since a warp in a SIMT architecture can proceed only when all of its memory requests are returned by memory, such latency divergence causes significant slowdown when running irregular GPGPU applications. To solve this issue, we propose memory scheduling mechanisms that avoid inter-warp interference in the DRAM system to reduce the average memory stall latency experienced by warps. We further reduce latency divergence through mechanisms that coordinate scheduling decisions across multiple independent memory channels. Finally we show that carefully orchestrating the memory scheduling policy can achieve low average latency for warps, without compromising bandwidth utilization. Our combined scheme yields a 10.1% performance improvement for irregular GPGPU workloads relative to a throughput-optimized GPU memory controller.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.16,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012998,,Bandwidth;Graphics processing units;Instruction sets;Memory management;Parallel processing;Random access memory,DRAM chips;graphics processing units;storage management chips,DRAM system;SIMT architecture;average memory stall latency;bandwidth utilization;high bandwidth usage;independent memory channels;interleaving requests;interwarp interference;irregular GPGPU applications;latency divergence;memory controllers;memory requests;memory scheduling mechanisms;memory scheduling policy;scheduling decisions;throughput-optimized GPU memory controller,,12,,53,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Slim Fly: A Cost Effective Low-Diameter Network Topology,M. Besta; T. Hoefler,"ETH Zurich, Zurich, Switzerland","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,348,359,"We introduce a high-performance cost-effective network topology called Slim Fly that approaches the theoretically optimal network diameter. Slim Fly is based on graphs that approximate the solution to the degree-diameter problem. We analyze Slim Fly and compare it to both traditional and state-of the-art networks. Our analysis shows that Slim Fly has significant advantages over other topologies in latency, bandwidth, resiliency, cost, and power consumption. Finally, we propose deadlock-free routing schemes and physical layouts for large computing centres as well as a detailed cost and power model. Slim Fly enables constructing cost effective and highly resilient data enter and HPC networks that offer low latency and high bandwidth under different HPC workloads such as stencil or graph computations.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.34,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013016,,Bandwidth;Joining processes;Measurement;Network topology;Routing;System recovery;Topology,graph theory;multiprocessor interconnection networks;parallel processing,HPC network;Slim Fly;bandwidth;cost effective low-diameter network topology;deadlock-free routing scheme;latency;optimal network diameter;physical layout;power consumption,,17,,43,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
[Publisher's information],,,"SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,1078,1078,Provides a listing of current committee members and society officers.,2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.93,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013075,,,,,,0,,,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Omnisc'IO: A Grammar-Based Approach to Spatial and Temporal I/O Patterns Prediction,M. Dorier; S. Ibrahim; G. Antoniu; R. Ross,"ENS Rennes, IRISA, Rennes, France","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,623,634,"The increasing gap between the computation performance of post-petascale machines and the performance of their I/O subsystem has motivated many I/O optimizations including prefetching, caching, and scheduling techniques. In order to further improve these techniques, modeling and predicting spatial and temporal I/O patterns of HPC applications as they run has became crucial. In this paper we present Omnisc'IO, an approach that builds a grammar-based model of the I/O behavior of HPC applications and uses it to predict when future I/O operations will occur, and where and how much data will be accessed. Omnisc'IO is transparently integrated into the POSIX and MPI I/O stacks and does not require any modification in applications or higher level I/O libraries. It works without any prior knowledge of the application and converges to accurate predictions within a couple of iterations only. Its implementation is efficient in both computation time and memory footprint.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.56,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013038,Exascale;Grammar;HPC;I/O;Omnisc'IO;Prediction;Storage,Context;Grammar;Hidden Markov models;Libraries;Prediction algorithms;Predictive models;Prefetching,Unix;cache storage;grammars;input-output programs;message passing;parallel processing;scheduling;storage management,HPC applications;I/O optimizations;I/O subsystem;MPI I/O stacks;Omnisc'IO;POSIX stacks;caching techniques;grammar-based approach;post-petascale machines;prefetching techniques;scheduling techniques;spatial I/O pattern prediction;temporal I/O pattern prediction,,2,,35,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Parallel Deep Neural Network Training for Big Data on Blue Gene/Q,I. H. Chung; T. N. Sainath; B. Ramabhadran; M. Pichen; J. Gunnels; V. Austel; U. Chauhari; B. Kingsbury,"IBM T.J. Watson Res. Center, Yorktown Heights, NY, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,745,753,"Deep Neural Networks (DNNs) have recently been shown to significantly outperform existing machine learning techniques in several pattern recognition tasks. DNNs are the state-of-the-art models used in image recognition, object detection, classification and tracking, and speech and language processing applications. The biggest drawback to DNNs has been the enormous cost in computation and time taken to train the parameters of the networks - often a tenfold increase relative to conventional technologies. Such training time costs can be mitigated by the application of parallel computing algorithms and architectures. However, these algorithms often run into difficulties because of the cost of inter-processor communication bottlenecks. In this paper, we describe how to enable Parallel Deep Neural Network Training on the IBM Blue Gene/Q (BG/Q) computer system. Specifically, we explore DNN training using the data parallel Hessian-free 2nd order optimization algorithm. Such an algorithm is particularly well-suited to parallelization across a large set of loosely coupled processors. BG/Q, with its excellent inter-processor communication characteristics, is an ideal match for this type of algorithm. The paper discusses how issues regarding programming model and data-dependent imbalances are addressed. Results on large-scale speech tasks show that the performance on BG/Q scales linearly up to 4096 processes with no loss in accuracy. This allows us to train neural networks using billions of training examples in a few hours.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.66,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013048,Big Data;High Performance Computing;Speech Recognition,Neural networks;Optimization;Prefetching;Speech recognition;Synchronization;Training,Big Data;learning (artificial intelligence);neural nets;parallel architectures;pattern recognition,DNN;IBM BG/Q computer system;IBM Blue Gene/Q computer system;big data;data-parallel Hessian-free 2nd order optimization algorithm;interprocessor communication characteristics;machine learning techniques;parallel architectures;parallel computing algorithms;parallel deep neural network training;pattern recognition tasks;programming model;training time costs,,5,,31,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
[Title page i],,,"SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,i,i,Presents the title page of the proceedings record.,2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.1,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012183,,,,,,0,,,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Reciprocal Resource Fairness: Towards Cooperative Multiple-Resource Fair Sharing in IaaS Clouds,H. Liu; B. He,"Nanyang Technol. Univ., Singapore, Singapore","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,970,981,"Resource sharing in virtualized environments have been demonstrated significant benefits to improve application performance and resource/energy efficiency. However, resource sharing, especially for multiple resource types, poses several severe and challenging problems in pay-as-you-use cloud environments, such as sharing incentive, free-riding, lying and economic fairness. To address those problems, we propose Reciprocal Resource Fairness (RRF), a novel resource allocation mechanism to enable fair sharing multiple types of resource among multiple tenants in new-generation cloud environments. RRF implements two complementary and hierarchical mechanisms for resource sharing: inter-tenant resource trading and intra-tenant weight adjustment. We show that RRF satisfies several highly desirable properties to ensure fairness. Experimental results show that RRF is promising for both cloud providers and tenants. Compared to existing cloud models, RRF improves virtual machine (VM) density and cloud providers' revenue by 2.2X. For tenants, RRF improves application performance by 45% and guarantees 95% economic fairness among multiple tenants.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.84,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013066,IaaS;cloud computing;fairness;resource sharing,Dynamic scheduling;Economics;Indexes;Random access memory;Resource management;Support vector machines;Vectors,cloud computing;resource allocation;virtual machines;virtualisation,IaaS cloud;RRF;VM density;cloud model;cloud provider;cloud tenant;complementary mechanism;cooperative multiple-resource fair sharing;economic fairness;fair sharing multiple type;hierarchical mechanism;inter-tenant resource trading;intra-tenant weight adjustment;multiple resource type;pay-as-you-use cloud environment;reciprocal resource fairness;resource allocation mechanism;resource sharing;resource/energy efficiency;virtual machine density;virtualized environment,,8,,60,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
DISC: A Domain-Interaction Based Programming Model with Support for Heterogeneous Execution,M. C. Kurt; G. Agrawal,"Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,869,880,"Several emerging trends are pointing to increasing heterogeneity among nodes and/or cores in HPC systems. Existing programming models, especially for distributed memory execution, typically have been designed to facilitate high performance on homogeneous systems. This paper describes a programming model and an associated runtime system we have developed to address the above need. The main concepts in the programming model are that of a domain and interactions between the domain elements. We explain how stencil computations, unstructured grid computations, and molecular dynamics applications can be expressed using these simple concepts. We show how interprocess communication can be handled efficiently at runtime just from the knowledge of domain interaction, for different types of applications. Subsequently, we develop techniques for the runtime system to automatically partition and re-partition the work among heterogeneous processors or nodes.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.76,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013058,heterogeneous support;load balancing;programming model,Computational modeling;Data structures;Load modeling;Parallel programming;Program processors;Runtime,parallel programming,DISC;HPC systems;distributed memory execution;domain elements;domain-interaction based programming model;heterogeneity;heterogeneous execution;heterogeneous nodes;heterogeneous processors;high performance computing;interprocess communication;molecular dynamics applications;runtime system;stencil computations;unstructured grid computations,,1,,42,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Dissecting On-Node Memory Access Performance: A Semantic Approach,A. GimÌ©nez; T. Gamblin; B. Rountree; A. Bhatele; I. Jusufi; P. T. Bremer; B. Hamann,"Dept. of Comput. Sci., Univ. of California, Davis, Davis, CA, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,166,176,"Optimizing memory access is critical for performance and power efficiency. CPU manufacturers have developed sampling-based performance measurement units (PMUs) that report precise costs of memory accesses at specific addresses. However, this data is too low-level to be meaningfully interpreted and contains an excessive amount of irrelevant or uninteresting information. We have developed a method to gather fine-grained memory access performance data for specific data objects and regions of code with low overhead and attribute semantic information to the sampled memory accesses. This information provides the context necessary to more effectively interpret the data. We have developed a tool that performs this sampling and attribution and used the tool to discover and diagnose performance problems in real-world applications. Our techniques provide useful insight into the memory behaviour of applications and allow programmers to understand the performance ramifications of key design decisions: domain decomposition, multi-threading, and data motion within distributed memory systems.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.19,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013001,,Context;Hardware;Kernel;Libraries;Program processors;Semantics;Topology,distributed memory systems;multi-threading;storage management,CPU manufacturers;PMU;attribute semantic information;code regions;data motion;data objects;design decisions;distributed memory systems;domain decomposition;fine-grained memory access performance data;memory access optimization;memory behaviour;multithreading;on-node memory access performance;performance ramifications;power efficiency;sampled memory accesses;sampling-based performance measurement units;semantic approach,,2,,26,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Fast Sparse Matrix-Vector Multiplication on GPUs for Graph Applications,A. Ashari; N. Sedaghati; J. Eisenlohr; S. Parthasarath; P. Sadayappan,"Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,781,792,"Sparse matrix-vector multiplication (SpMV) is a widely used computational kernel. The most commonly used format for a sparse matrix is CSR (Compressed Sparse Row), but a number of other representations have recently been developed that achieve higher SpMV performance. However, the alternative representations typically impose a significant preprocessing overhead. While a high preprocessing overhead can be amortized for applications requiring many iterative invocations of SpMV that use the same matrix, it is not always feasible -- for instance when analyzing large dynamically evolving graphs. This paper presents ACSR, an adaptive SpMV algorithm that uses the standard CSR format but reduces thread divergence by combining rows into groups (bins) which have a similar number of non-zero elements. Further, for rows in bins that span a wide range of non zero counts, dynamic parallelism is leveraged. A significant benefit of ACSR over other proposed SpMV approaches is that it works directly with the standard CSR format, and thus avoids significant preprocessing overheads. A CUDA implementation of ACSR is shown to outperform SpMV implementations in the NVIDIA CUSP and cuSPARSE libraries on a set of sparse matrices representing power-law graphs. We also demonstrate the use of ACSR for the analysis of dynamic graphs, where the improvement over extant approaches is even higher.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.69,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013051,ACSR;CSR;GPU;HYB;SpMV,Heuristic algorithms;Instruction sets;Kernel;Parallel processing;Sparse matrices;Standards;Vectors,graph theory;graphics processing units;mathematics computing;matrix multiplication;parallel architectures,ACSR;CSR format;CUDA implementation;GPUs;NVIDIA CUSP libraries;SpMV approach;adaptive SpMV algorithm;compressed sparse row;computational kernel;cuSPARSE libraries;dynamic graphs;dynamic parallelism;fast sparse matrix-vector multiplication;graph applications;iterative invocations;power-law graphs;thread divergence,,16,,28,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
A Volume Integral Equation Stokes Solver for Problems with Variable Coefficients,D. Malhotra; A. Gholami; G. Biros,"Univ. of Texas at Austin, Austin, TX, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,92,102,"We present a novel numerical scheme for solving the Stokes equation with variable coefficients in the unit box. Our scheme is based on a volume integral equation formulation. Compared to finite element methods, our formulation decouples the velocity and pressure, generates velocity fields that are by construction divergence free to high accuracy and its performance does not depend on the order of the basis used for discretization. In addition, we employ a novel adaptive fast multipole method for volume integrals to obtain a scheme that is algorithmically optimal. Our scheme supports non-uniform discretizations and is spectrally accurate. To increase per node performance, we have integrated our code with both NVIDIA and Intel accelerators. In our largest scalability test, we solved a problem with 20 billion unknowns, using a 14-order approximation for the velocity, on 2048 nodes of the Stampede system at the Texas Advanced Computing Center. We achieved 0.656 peta FLOPS for the overall code (23% efficiency) and one peta FLOPS for the volume integrals (33% efficiency). As an application example, we simulate Stokes ow in a porous medium with highly complex pore structure using a penalty formulation to enforce the no slip condition.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.13,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012995,,Chebyshev approximation;Convergence;Convolution;Equations;Geometry;Integral equations;Mathematical model,approximation theory;integral equations;parallel machines,14-order approximation;Intel accelerator;NVIDIA;Stampede system;Stokes equation;adaptive fast multipole method;nonuniform discretization;penalty formulation;peta FLOPS;pore structure;porous medium;pressure;variable coefficient;velocity;volume integral equation Stokes solver,,0,,27,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Application Centric Energy-Efficiency Study of Distributed Multi-Core and Hybrid CPU-GPU Systems,B. Cumming; G. Fourestey; O. Fuhrer; T. Gysi; M. Fatica; T. C. Schulthess,"Swiss Nat. Supercomput. Center, ETH Zurich, Lugano, Switzerland","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,819,829,"We study the energy used by a production-level regional climate and weather simulation code on a distributed memory system with hybrid CPU-GPU nodes. The code is optimised for both processor architectures, for which we investigate both time and energy to solution. Operational constraints for time to solution can be met with both processor types, although on different numbers of nodes. Energy to solution is a factor 3 lower with GPUs, but strong scaling can be pushed to larger node counts with CPUs to minimize time to solution. Our data shows that an affine relationship exists between energy and node hours consumed by the simulation. We use this property to devise a simple and practical methodology for optimising for energy efficiency that can be applied to other applications, which we demonstrate with the HPCG benchmark. We conclude with a discussion about the relationship to the commonly-used GF/Watt metric.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.72,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013054,,Atmospheric modeling;Benchmark testing;Computational modeling;Graphics processing units;Meteorology;Power demand;Production,distributed memory systems;energy conservation;graphics processing units;power aware computing;weather forecasting,GF-Watt metric;HPCG benchmark;application centric energy-efficiency;distributed memory system;distributed multicore CPU-GPU systems;hybrid CPU-GPU systems;processor architectures;production-level regional climate simulation code;production-level regional weather simulation code,,1,,26,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Anton 2: Raising the Bar for Performance and Programmability in a Special-Purpose Molecular Dynamics Supercomputer,D. E. Shaw; J. P. Grossman; J. A. Bank; B. Batson; J. A. Butts; J. C. Chao; M. M. Deneroff; R. O. Dror; A. Even; C. H. Fenton; A. Forte; J. Gagliardo; G. Gill; B. Greskamp; C. R. Ho; D. J. Ierardi; L. Iserovich; J. S. Kuskin; R. H. Larson; T. Layman; L. S. Lee; A. K. Lerer; C. Li; D. Killebrew; K. M. Mackenzie; S. Y. H. Mok; M. A. Moraes; R. Mueller; L. J. Nociolo; J. L. Peticolas; T. Quan; D. Ramot; J. K. Salmon; D. P. Scarpazza; U. B. Schafer; N. Siddique; C. W. Snyder; J. Spengler; P. T. P. Tang; M. Theobald; H. Toma; B. Towles; B. Vitale; S. C. Wang; C. Young,"D.E. Shaw Res., New York, NY, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,41,53,"Anton 2 is a second-generation special-purpose supercomputer for molecular dynamics simulations that achieves significant gains in performance, programmability, and capacity compared to its predecessor, Anton 1. The architecture of Anton 2 is tailored for fine-grained event-driven operation, which improves performance by increasing the overlap of computation with communication, and also allows a wider range of algorithms to run efficiently, enabling many new software-based optimizations. A 512-node Anton 2 machine, currently in operation, is up to ten times faster than Anton 1 with the same number of nodes, greatly expanding the reach of all-atom bio molecular simulations. Anton 2 is the first platform to achieve simulation rates of multiple microseconds of physical time per day for systems with millions of atoms. Demonstrating strong scaling, the machine simulates a standard 23,558-atom benchmark system at a rate of 85 ë_s/day -- 180 times faster than any commodity hardware platform or general-purpose supercomputer.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.9,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012191,,Arrays;Biological system modeling;Computational modeling;Flexible printed circuits;Hardware;Pipelines;Random access memory,digital simulation;parallel machines;physics computing,Anton 2 architecture;all-atom bio molecular simulations;fine-grained event-driven operation;molecular dynamics simulations;performance improvement;software-based optimizations;special-purpose molecular dynamics supercomputer,,19,,55,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Fast Iterative Graph Computation: A Path Centric Approach,P. Yuan; W. Zhang; C. Xie; H. Jin; L. Liu; K. Lee,"Sch. of Comput. Sci. & Technol., Huazhong Univ. of Sci. & Technol., Wuhan, China","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,401,412,"Large scale graph processing represents an interesting challenge due to the lack of locality. This paper presents Path Graph for improving iterative graph computation on graphs with billions of edges. Our system design has three unique features: First, we model a large graph using a collection of tree-based partitions and use an path-centric computation rather than vertex-centric or edge-centric computation. Our parallel computation model significantly improves the memory and disk locality for performing iterative computation algorithms. Second, we design a compact storage that further maximize sequential access and minimize random access on storage media. Third, we implement the path-centric computation model by using a scatter/gather programming model, which parallels the iterative computation at partition tree level and performs sequential updates for vertices in each partition tree. The experimental results show that the path-centric approach outperforms vertex centric and edge-centric systems on a number of graph algorithms for both in-memory and out-of-core graphs.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.38,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013020,Computing model;Graph;Iterative computation;Path;Storage,,iterative methods;mathematics computing;parallel processing;storage media;trees (mathematics),Path Graph;compact storage design;edge-centric system;in-memory graph;iterative graph computation;large scale graph processing;out-of-core graph;parallel computation model;partition tree level;path centric approach;path-centric computation;path-centric computation model;random access minimization;scatter-gather programming model;sequential access maximization;sequential updates;storage media;tree-based partitions;vertex centric system,,5,,36,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Efficient Implementation of Many-Body Quantum Chemical Methods on the Intelå¨ Xeon Phi Coprocessor,E. AprÌÊ; M. Klemm; K. Kowalski,,"SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,674,684,This paper presents the implementation and performance of the highly accurate CCSD(T) quantum chemistry method on the Intel<sup>å¨</sup> Xeon Phi coprocessor within the context of the NWChem computational chemistry package. The widespread use of highly correlated methods in electronic structure calculations is contingent upon the interplay between advances in theory and the possibility of utilizing the ever-growing computer power of emerging heterogeneous architectures. We discuss the design decisions of our implementation as well as the optimizations applied to the compute kernels and data transfers between host and coprocessor. We show the feasibility of adopting the Intel<sup>å¨</sup> Many Integrated Core Architecture and the Intel Xeon Phi coprocessor for developing efficient computational chemistry modeling tools. Remarkable scalability is demonstrated by benchmarks. Our solution scales up to a total of 62560 cores with the concurrent utilization of Intel<sup>å¨</sup> Xeon<sup>å¨</sup> processors and Intel Xeon Phi coprocessors.,2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.60,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013042,Chemistry;distributed architectures;parallel algorithms,Computational modeling;Computer architecture;Coprocessors;Graphics processing units;Tensile stress;Vectors,chemistry computing;coprocessors;electronic structure;quantum chemistry,CCSD(T) quantum chemistry;Intel Xeon phi coprocessor;NWChem computational chemistry package;electronic structure calculation;many integrated core architecture;many-body quantum chemical method,,3,,53,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Scaling MapReduce Vertically and Horizontally,I. El-Helw; R. Hofman; H. E. Bal,"Dept. of Comput. Sci., Vrije Univ. Amsterdam, Amsterdam, Netherlands","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,525,535,"Glass wing is a MapReduce framework that uses OpenCL to exploit multi-core CPUs and accelerators. However, compute device capabilities may vary significantly and require targeted optimization. Similarly, the availability of resources such as memory, storage and interconnects can severely impact overall job performance. In this paper, we present and analyze how MapReduce applications can improve their horizontal and vertical scalability by using a well controlled mixture of coarse- and fine-grained parallelism. Specifically, we discuss the Glass wing pipeline and its ability to overlap computation, communication, memory transfers and disk access. Additionally, we show how Glass wing can adapt to the distinct capabilities of a variety of compute devices by employing fine-grained parallelism. We experimentally evaluated the performance of five MapReduce applications and show that Glass wing outperforms Hadoop on a 64-node multi-core CPU cluster by factors between 1.2 and 4, and factors from 20 to 30 on a 23-node GPU cluster. Similarly, we show that Glass wing is at least 1.5 times faster than GPMR on the GPU cluster.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.48,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013030,Heterogeneous;MapReduce;OpenCL;Scalability,Graphics processing units;Instruction sets;Kernel;Parallel processing;Performance evaluation;Pipelines;Scalability,data handling;graphics processing units;multiprocessing systems;parallel processing;pipeline processing,GPMR;GPU cluster;Glasswing pipeline;Hadoop;MapReduce applications;MapReduce framework;OpenCL;accelerators;coarse-grained parallelism;disk access;fine-grained parallelism;horizontal MapReduce scaling;horizontal scalability;multicore CPU;multicore CPU cluster;vertical MapReduce scaling;vertical scalability,,4,,33,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
MSL: A Synthesis Enabled Language for Distributed Implementations,Z. Xu; S. Kamil; A. Solar-Lezama,,"SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,311,322,"This paper demonstrates how ideas from generative programming and software synthesis can help support the development of bulk-synchronous distributed memory kernels. These ideas are realized in a new language called MSL, a C-like language that combines synthesis features with high level notations for array manipulation and bulk-synchronous parallelism to simplify the semantic analysis required for synthesis. The paper shows that by leveraging these high level notations, it is possible to scale the synthesis and automated bug-finding technologies that underlie MSL to realistic computational kernels. Specifically, we demonstrate the methodology through case studies implementing non-trivial distributed kernels -- both regular and irregular -- from the NAS parallel benchmarks. We show that our approach can automatically infer many challenging details from these benchmarks and can enable high level implementation ideas to be reused between similar kernels. We also demonstrate that these high level notations map easily to low level C code and show that the performance of this generated code matches that of handwritten Fortran.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.31,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013013,,Arrays;Benchmark testing;Generators;Kernel;Programming;Semantics;Synthesizers,C language;distributed memory systems;operating system kernels;parallel programming;program compilers;program debugging;storage management,C-like language;Fortran;MSL language;NAS parallel benchmark;array manipulation;automated bug-finding technology;bulk-synchronous distributed memory kernels;bulk-synchronous parallelism;code generation;computational kernels;distributed implementation;generative programming;high level notations;low level C code;nontrivial distributed kernels;semantic analysis;software synthesis;synthesis enabled language;synthesis features,,3,,53,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Using an Adaptive HPC Runtime System to Reconfigure the Cache Hierarchy,E. Totoni; J. Torrellas; L. V. Kale,"Dept. of Comput. Sci., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,1047,1058,"The cache hierarchy often consumes a large portion of a processor's energy. To save energy in HPC environments, this paper proposes software-controlled reconfiguration of the cache hierarchy with an adaptive runtime system. Our approach addresses the two major limitations associated with other methods that reconfigure the caches: predicting the application's future and finding the best cache hierarchy configuration. Our approach uses formal language theory to express the application's pattern and help predict its future. Furthermore, it uses the prevalent Single Program Multiple Data (SPMD) model of HPC codes to find the best configuration in parallel quickly. Our experiments using cycle-level simulations indicate that 67% of the cache energy can be saved with only a 2.4% performance penalty on average. Moreover, we demonstrate that, for some applications, switching to a software-controlled reconfigurable streaming buffer configuration can improve performance by up to 30% and save 75% of the cache energy.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.90,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013072,,Adaptive systems;Hardware;Kernel;Prefetching;Runtime;Sparse matrices;Supercomputers,cache storage;formal languages;parallel processing;power aware computing,HPC codes;HPC environment;SPMD model;adaptive HPC runtime system;application pattern;cache energy saving;cache hierarchy reconfiguration;cycle-level simulation;formal language theory;parallel configuration;performance penalty;processor energy;single program multiple data model;software-controlled reconfigurable streaming buffer configuration;software-controlled reconfiguration,,4,,46,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Nonblocking Epochs in MPI One-Sided Communication,J. A. Zounmevo; X. Zhao; P. Balaji; W. Gropp; A. Afsahi,"Queen's Univ., Kingston, ON, Canada","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,475,486,"The synchronization model of the MPI one-sided communication paradigm can lead to serialization and latency propagation. For instance, a process can propagate non-RMA communication-related latencies to remote peers waiting in their respective epoch-closing routines in matching epochs. In this work, we discuss six latency issues that were documented for MPI-2.0 and show how they evolved in MPI-3.0. Then, we propose entirely nonblocking RMA synchronizations that allow processes to avoid waiting even in epoch-closing routines. The proposal provides contention avoidance in communication patterns that require back to back RMA epochs. It also fixes the latency propagation issues. Moreover, it allows the MPI progress engine to orchestrate aggressive schedulings to cut down the overall completion time of sets of epochs without introducing memory consistency hazards. Our test results show noticeable performance improvements for a lower-upper matrix decomposition as well as an application pattern that performs massive atomic updates.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.44,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013026,MPI;RMA;latency propagation;nonblocking synchronizations;one-sided,Delays;Engines;Hazards;Proposals;Semantics;Standards;Synchronization,matrix decomposition;message passing,MPI one-sided communication;MPI-2.0;MPI-3.0;application pattern;atomic updates;communication patterns;contention avoidance;epoch-closing routines;latency issues;latency propagation issues;lower-upper matrix decomposition;matching epochs;nonRMA communication-related latencies;nonblocking RMA synchronizations;nonblocking epochs;serialization;synchronization model,,0,,22,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Understanding Soft Error Resiliency of Blue Gene/Q Compute Chip through Hardware Proton Irradiation and Software Fault Injection,C. Y. Cher; M. S. Gupta; P. Bose; K. P. Muller,"IBM T. J. Watson Res. Center, Yorktown Heights, NY, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,587,596,"Soft Error Resiliency is a major concern for Petascale high performance computing (HPC) systems. Blue Gene/Q (BG/Q) is the third generation of IBM's massively parallel, energy efficient Blue Gene series of supercomputers. The principal goal of this work is to understand the interaction between Blue-Gene/Q's hardware resiliency features and high-performance applications through proton irradiation of a real chip, and software resiliency inherent in these applications through application-level fault injection (AFI) experiments. From the proton irradiation experiments we derived that the mean time between correctable errors at sea level of the SRAM-based register files and Level-1 caches for a system similar to the scale of Sequoia system. From the AFI experiments, we characterized relative vulnerability among the applications in both general purpose and floating point register files. We categorized and quantified the failure outcomes, and discovered characteristics in the applications that lead to many masking improvement opportunities.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.53,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013035,chip irradiation;co-design;fault injection;high-performance applications;soft error rate,Circuit faults;Hardware;Particle beams;Protons;Radiation effects;Registers;Software,SRAM chips;cache storage;floating point arithmetic;mainframes;microprocessor chips;parallel machines;radiation hardening (electronics);software fault tolerance,AFI experiments;BlueGene/Q compute chip;BlueGene/Q hardware resiliency features;HPC systems;Level-1 caches;SRAM-based register files;Sequoia system;application-level fault injection experiments;correctable errors;floating point register files;hardware proton irradiation;petascale high performance computing systems;soft error resiliency;software fault injection;software resiliency;third generation IBM massively parallel energy efficient Blue Gene series supercomputers,,0,,27,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Table of contents,,,"SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,v,xiv,The following topics are dealt with: high-performance computing; networking; file storage; ACM Gordon Bell finalist; application scaling; application heterogeneity; microarchitecture; performance measurement; performance accelerators; file systems; earth sciences; space sciences; compiler analysis; compiler optimization; parallel algorithms; Big Data analysis; high-performance genomics; MPI; cloud computing; graph algorithms; hardware vulnerability; hardware recovery; I/O; dynamic optimization; quantum simulations; resilience analysis; machine learning; data analytics; numerical kernels; power efficiency; data locality; load balancing; optimized checkpointing; sparse solvers; large-scale visualization; and memory system energy efficiency.,2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.5,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012186,,,Big Data;checkpointing;cloud computing;data analysis;data visualisation;file organisation;internetworking;learning (artificial intelligence);message passing;parallel processing;performance evaluation;power aware computing;program compilers;quantum computing;resource allocation,ACM Gordon Bell finalist;Big Data analysis;I/O;MPI;application heterogeneity;application scaling;cloud computing;compiler analysis;compiler optimization;data analytics;data locality;dynamic optimization;earth sciences;file storage;file systems;graph algorithms;hardware recovery;hardware vulnerability;high-performance computing;high-performance genomics;large-scale visualization;load balancing;machine learning;memory system energy efficiency;microarchitecture;networking;numerical kernels;optimized checkpointing;parallel algorithms;performance accelerators;performance measurement;power efficiency;quantum simulations;resilience analysis;space sciences;sparse solvers,,0,,,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Optimization of a Multilevel Checkpoint Model with Uncertain Execution Scales,S. Di; L. Bautista-Gome; F. Cappello,"INRIA, Sophia-Antipolis, France","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,907,918,"Future extreme-scale systems are expected to experience different types of failures affecting applications with different failure scales, from transient uncorrectable memory errors in processes to massive system outages. In this paper, we propose a multilevel checkpoint model by taking into account uncertain execution scales (different numbers of processes/cores). The contribution is threefold: (1) we provide an in-depth analysis on why it is difficult to derive the optimal checkpoint intervals for different checkpoint levels and optimize the number of cores simultaneously, (2) we devise a novel method that can quickly obtain an optimized solution -- the first successful attempt in multilevel checkpoint models with uncertain scales, and (3) we perform both large scale real experiments and extreme-scale numerical simulation to validate the effectiveness of our design. The experiments confirm that our optimized solution outperforms other state of-the-art solutions by 4.3 -- 88% on wall-clock length.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.79,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013061,,Analytical models;Approximation algorithms;Computational modeling;Equations;Heating;Mathematical model;Optimization,checkpointing;multiprocessing systems;numerical analysis;optimisation,checkpoint levels;extreme-scale numerical simulation;extreme-scale systems;failure scales;massive system outages;multilevel checkpoint model;optimal checkpoint intervals;optimization;processes/cores;transient uncorrectable memory errors;uncertain execution scales;wall-clock length,,5,,38,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
A System Software Approach to Proactive Memory-Error Avoidance,C. H. A. Costa; Y. Park; B. S. Rosenburg; C. Y. Cher; K. D. Ryu,"IBM T. J. Watson Res. Center, Yorktown Heights, NY, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,707,718,"Today's HPC systems use two mechanisms to address main-memory errors. Error-correcting codes make correctable errors transparent to software, while checkpoint/restart (CR) enables recovery from uncorrectable errors. Unfortunately, CR overhead will be enormous at exascale due to the high failure rate of memory. We propose a new OS-based approach that proactively avoids memory errors using prediction. This scheme exposes correctable error information to the OS, which migrates pages and off lines unhealthy memory to avoid application crashes. We analyze memory error patterns in extensive logs from a BG/P system and show how correctable error patterns can be used to identify memory likely to fail. We implement a proactive memory management system on BG/Q by extending the firmware and Linux. We evaluate our approach with a realistic workload and compare our overhead against CR. We show improved resilience with negligible performance overhead for applications.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.63,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013045,Memory Structures;Operating Systems;Reliability;and Fault-Tolerance,Algorithm design and analysis;Correlation;Error analysis;Error correction codes;Memory management;Monitoring;Prediction algorithms,Linux;checkpointing;error correction codes;firmware;parallel processing;storage management,BG-P system;CR;HPC systems;Linux;OS-based approach;checkpoint-restart;correctable error patterns;error-correcting codes;firmware;memory error pattern analysis;page migration;proactive memory management system;proactive memory-error avoidance;system software approach,,0,,31,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Efficient Shared-Memory Implementation of High-Performance Conjugate Gradient Benchmark and its Application to Unstructured Matrices,J. Park; M. Smelyanskiy; K. Vaidyanathan; A. Heinecke; D. D. Kalamkar; X. Liu; M. M. A. Patwary; Y. Lu; P. Dubey,"Parallel Comput. Lab., Intel Corp., Santa Clara, CA, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,945,955,"A new sparse high performance conjugate gradient benchmark (HPCG) has been recently released to address challenges in the design of sparse linear solvers for the next generation extreme-scale computing systems. Key computation, data access, and communication pattern in HPCG represent building blocks commonly found in today's HPC applications. While it is a well known challenge to efficiently parallelize Gauss-Seidel smoother, the most time-consuming kernel in HPCG, our algorithmic and architecture-aware optimizations deliver 95% and 68% of the achievable bandwidth on Xeon and Xeon Phi, respectively. Based on available parallelism, our Xeon Phi shared-memory implementation of Gauss-Seidel smoother selectively applies block multi-color reordering. Combined with MPI parallelization, our implementation balances parallelism, data access locality, CG convergence rate, and communication overhead. Our implementation achieved 580 TFLOPS (82% parallelization efficiency) on Tianhe-2 system, ranking first on the most recent HPCG list in July 2014. In addition, we demonstrate that our optimizations not only benefit HPCG original dataset, which is based on structured 3D grid, but also a wide range of unstructured matrices.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.82,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013064,,Benchmark testing;Convergence;Equations;Parallel processing;Sparse matrices;Synchronization;Vectors,conjugate gradient methods;iterative methods;matrix algebra;message passing;optimisation;parallel processing;shared memory systems,3D grid;CG convergence rate;Gauss-Seidel smoother parallelization;HPC applications;HPCG;MPI parallelization;TFLOPS;Tianhe-2 system;Xeon Phi shared-memory implementation;algorithmic optimizations;architecture-aware optimizations;block multicolor reordering;communication overhead;communication pattern;data access locality;high performance conjugate gradient benchmark;next generation extreme-scale computing systems;parallelism;sparse linear solvers;unstructured matrices,,10,,39,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Pipelining Computational Stages of the Tomographic Reconstructor for Multi-Object Adaptive Optics on a Multi-GPU System,A. Charara; H. Ltaief; D. Gratadour; D. Keyes; A. Sevin; A. Abdelfattah; E. Gendron; C. Morel; F. Vidal,,"SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,262,273,"The European Extremely Large Telescope project (E-ELT) is one of Europe's highest priorities in ground-based astronomy. ELTs are built on top of a variety of highly sensitive and critical astronomical instruments. In particular, a new instrument called MOSAIC has been proposed to perform multi-object spectroscopy using the Multi-Object Adaptive Optics (MOAO) technique. The core implementation of the simulation lies in the intensive computation of a tomographic reconstruct or (TR), which is used to drive the deformable mirror in real time from the measurements. A new numerical algorithm is proposed (1) to capture the actual experimental noise and (2) to substantially speed up previous implementations by exposing more concurrency, while reducing the number of floating-point operations. Based on the Matrices Over Runtime System at Exascale numerical library (MORSE), a dynamic scheduler drives all computational stages of the tomographic reconstruct or simulation and allows to pipeline and to run tasks out-of order across different stages on heterogeneous systems, while ensuring data coherency and dependencies. The proposed TR simulation outperforms asymptotically previous state-of-the-art implementations up to 13-fold speedup. At more than 50000 unknowns, this appears to be the largest-scale AO problem submitted to computation, to date, and opens new research directions for extreme scale AO simulations.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.27,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013009,Computational Astronomy;Dense Linear Algebra;Dynamic Scheduler;GPU Computing;Multi-Objects Adaptive Optics,Computational modeling;Covariance matrices;Libraries;Runtime;Telescopes;Tomography,adaptive optics;astronomical image processing;astronomical telescopes;computerised tomography;floating point arithmetic;graphics processing units;pipeline processing;scheduling,E-ELT;European extremely large telescope project;MOAO technique;MORSE;MOSAIC;Matrices Over Runtime System at Exascale numerical library;TR simulation;astronomical instruments;computational stage pipelining;data coherency;data dependencies;dynamic scheduler;floating-point operations;ground-based astronomy;heterogeneous systems;largest-scale AO problem;multiGPU system;multiobject adaptive optics;multiobject spectroscopy;numerical algorithm;tomographic reconstructor simulation,,1,,35,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
A User-Friendly Approach for Tuning Parallel File Operations,R. McLay; D. James; S. Liu; J. Cazes; W. Barth,"Texas Adv. Comput. Center, Univ. of Texas at Austin, Austin, TX, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,229,236,"The Lustre file system provides high aggregated I/O bandwidth and is in widespread use throughout the HPC community. Here we report on work (1) developing a model for understanding collective parallel MPI write operations on Lustre, and (2) producing a library that optimizes parallel write performance in a user-friendly way. We note that a system's default stripe count is rarely a good choice for parallel I/O, and that performance depends on a delicate balance between the number of stripes and the actual (not requested) number of collective writers. Unfortunate combinations of these parameters may degrade performance considerably. For the programmer, however, it's all about the stripe count: an informed choice of this single parameter allows MPI to assign writers in a way that achieves near-optimal performance. We offer recommendations for those who wish to tune performance manually and describe the easy-to-use T3PIO library that manages the tuning automatically.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.24,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013006,,Arrays;Bandwidth;Benchmark testing;Communities;Libraries;Tuning;Writing,application program interfaces;file organisation;input-output programs;message passing;parallel processing,HPC;I/O bandwidth;Lustre file system;parallel MPI write operation tuning;user-friendly approach,,1,,29,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
High-Performance Computation of Distributed-Memory Parallel 3D Voronoi and Delaunay Tessellation,T. Peterka; D. Morozov; C. Phillips,"Argonne Nat. Lab., Argonne, IL, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,997,1007,"Computing a Voronoi or Delaunay tessellation from a set of points is a core part of the analysis of many simulated and measured datasets: N-body simulations, molecular dynamics codes, and LIDAR point clouds are just a few examples. Such computational geometry methods are common in data analysis and visualization, but as the scale of simulations and observations surpasses billions of particles, the existing serial and shared memory algorithms no longer suffice. A distributed-memory scalable parallel algorithm is the only feasible approach. The primary contribution of this paper is a new parallel Delaunay and Voronoi tessellation algorithm that automatically determines which neighbor points need to be exchanged among the sub domains of a spatial decomposition. Other contributions include periodic and wall boundary conditions, comparison of our method using two popular serial libraries, and application to numerous science datasets.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.86,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013068,Delaunay tessellation;Voronoi;computational geometry,Computational geometry;Data models;Face;Heuristic algorithms;Libraries;Parallel algorithms;Three-dimensional displays,computational geometry;data analysis;data visualisation;distributed memory systems;mesh generation;parallel algorithms,computational geometry methods;data analysis;data visualization;distributed-memory parallel 3D Delaunay tessellation;distributed-memory parallel 3D Voronoi tessellation;distributed-memory scalable parallel algorithm;high-performance computation,,3,,35,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Scalable Kernel Fusion for Memory-Bound GPU Applications,M. Wahib; N. Maruyama,"CREST, RIKEN Adv. Inst. for Comput. Sci., Kobe, Japan","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,191,202,"GPU implementations of HPC applications relying on finite difference methods can include tens of kernels that are memory-bound. Kernel fusion can improve performance by reducing data traffic to off-chip memory, kernels that share data arrays are fused to larger kernels where on-chip cache is used to hold the data reused by instructions originating from different kernels. The main challenges are a) searching for the optimal kernel fusions while constrained by data dependencies and kernels' precedences and b) effectively applying kernel fusion to achieve speedup. This paper introduces a problem definition and proposes a scalable method for searching the space of possible kernel fusions to identify optimal kernel fusions for large problems. The paper also proposes a codeless performance upper-bound projection model to achieve effective fusions. Results show that using the proposed scalable method for kernel fusion improved the performance of two real-world applications containing tens of kernels by 1.35x and 1.2x.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.21,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013003,,Arrays;Graphics processing units;Instruction sets;Kernel;Meteorology;Optimization;System-on-chip,cache storage;finite difference methods;graphics processing units;parallel processing;performance evaluation,HPC applications;codeless performance upper-bound projection model;data arrays;data dependencies;data traffic;finite difference methods;kernel precedences;memory-bound GPU applications;memory-bound kernels;off-chip memory;on-chip cache;optimal kernel fusions;scalable kernel fusion,,7,1,31,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Recycled Error Bits: Energy-Efficient Architectural Support for Floating Point Accuracy,R. Nathan; B. Anthonio; S. L. Lu; H. Naeimi; D. J. Sorin; X. Sun,,"SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,117,127,"In this work, we provide energy-efficient architectural support for floating point accuracy. For each floating point addition performed, we ""recycle"" that operation's rounding error. We make this error architecturally visible such that it can be used, whenever desired, by software. We also design a compiler pass that allows software to automatically use this feature. Experimental results on physical hardware show that software that exploits architecturally recycled error bits can (a) achieve accuracy comparable to a 64-bit FPU with performance and energy that are comparable to a 32-bit FPU, and (b) achieve accuracy comparable to an all-software scheme for 128-bit accuracy with far better performance and energy usage.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.15,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012997,innovative hardware/software co-design;linear and nonlinear systems;numerical methods,Accuracy;Assembly;Benchmark testing;Hardware;Instruments;Registers;Software,floating point arithmetic;hardware-software codesign;performance evaluation;program compilers;recycling,FPU;all-software scheme;architecturally recycled error bits;compiler pass;energy-efficient architectural support;floating point accuracy;floating point addition;operation rounding error;recycled error bits,,1,,25,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Structure Slicing: Extending Logical Regions with Fields,M. Bauer; S. Treichler; E. Slaughter; A. Aiken,"Stanford Univ., Stanford, CA, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,845,856,"Applications on modern supercomputers are increasingly limited by the cost of data movement, but mainstream programming systems have few abstractions for describing the structure of a program's data. Consequently, the burden of managing data movement, placement, and layout currently falls primarily upon the programmer. To address this problem we previously proposed a data model based on logical regions and described Legion, a programming system incorporating logical regions. In this paper, we present structure slicing, which incorporates fields into the logical region data model. We show that structure slicing enables Legion to automatically infer task parallelism from field non-interference, decouple the specification of data usage from layout, and reduce the overall amount of data moved. We demonstrate that structure slicing enables both strong and weak scaling of three Legion applications including S3D, a production combustion simulation that uses logical regions with thousands of fields, with speedups of up to 3.68X over a vectorized CPU-only Fortran implementation and 1.88X over an independently hand-tuned OpenACC code.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.74,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013056,,Arrays;Indexes;Layout;Optimization;Parallel processing;Program processors;Runtime,FORTRAN;combustion;data models;digital simulation;mainframes;parallel machines;production engineering computing,Legion;OpenACC code;S3D;data layout;data movement;data placement;data usage specification;field noninterference;logical region data model;logical region extension;mainstream programming systems;production combustion simulation;structure slicing;supercomputers;task parallelism;vectorized CPU-only Fortran implementation,,2,,27,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Maximizing Throughput on a Dragonfly Network,N. Jain; A. Bhatele; X. Ni; N. J. Wright; L. V. Kale,"Dept. of Comput. Sci., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,336,347,"Interconnection networks are a critical resource for large supercomputers. The dragonfly topology, which provides a low network diameter and large bisection bandwidth, is being explored as a promising option for building multi-Petaflop's and Exaflop's systems. Unlike the extensively studied torus networks, the best choices of message routing and job placement strategies for the dragonfly topology are not well understood. This paper aims at analyzing the behavior of a machine built using a dragonfly network for various routing strategies, job placement policies, and application communication patterns. Our study is based on a novel model that predicts traffic on individual links for direct, indirect, and adaptive routing strategies. We analyze results for individual communication patterns and some common parallel job workloads. The predictions presented in this paper are for a 100+ Petaflop's prototype machine with 92,160 high radix routers and 8.8 million cores.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.33,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013015,dragonfly networks;job placement;modeling;prediction;routing,Adaptation models;Bandwidth;Network topology;Predictive models;Routing;Throughput;Topology,multiprocessor interconnection networks;parallel machines;telecommunication network routing;telecommunication network topology,Petaflop prototype machine;application communication pattern;dragonfly topology;interconnection network;job placement policy;routing strategy;supercomputer;throughput maximization,,6,,28,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Author index,,,"SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,1071,1077,Presents an index of the authors whose articles are published in the conference proceedings record.,2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.92,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013074,,,,,,0,,,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Correctness Field Testing of Production and Decommissioned High Performance Computing Platforms at Los Alamos National Laboratory,S. E. Michalak; W. N. Rust; J. T. Dal; R. J. Dubois; D. H. Dubois,"Stat. Sci. Group, Los Alamos Nat. Lab., Los Alamos, NM, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,609,619,"Silent Data Corruption (SDC) can threaten the integrity of scientific calculations performed on high performance computing (HPC) platforms and other systems. To characterize this issue, correctness field testing of HPC platforms at Los Alamos National Laboratory was performed. This work presents results for 12 platforms, including over 1,000 node-years of computation performed on over 8,750 compute nodes and over 260 petabytes of data transfers involving nearly 6,000 compute nodes, and relevant lessons learned. Incorrect results characteristic of transient errors and of intermittent errors were observed. These results are a key underpinning to resilience efforts as they provide signatures of incorrect results observed under field conditions. Five incorrect results consistent with a transient error mechanism were observed, suggesting that the effects of transient errors could be mitigated. However, the observed numbers of incorrect results consistent with an intermittent error mechanism suggest that intermittent errors could substantially effect computational correctness.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.55,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013037,Cluster computing;HPC cluster;Linpack;field testing;high performance computing;interconnect testing;intermittent error;resilience;silent data corruption;soft error;transient error,Computer architecture;Data transfer;High performance computing;Production;SDRAM;Testing;Transient analysis,natural sciences computing;parallel processing,HPC platforms;Los Alamos National Laboratory;SDC;correctness field testing;decommissioned high performance computing platform;intermittent error mechanism;production high performance computing platform;scientific calculations;silent data corruption;transient error mechanism,,2,,31,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Petascale High Order Dynamic Rupture Earthquake Simulations on Heterogeneous Supercomputers,A. Heinecke; A. Breuer; S. Rettenberger; M. Bader; A. A. Gabriel; C. Pelties; A. Bode; W. Barth; X. K. Liao; K. Vaidyanathan; M. Smelyanskiy; P. Dubey,"Dept. of Inf., Tech. Univ. Munchen, Munich, Germany","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,3,14,"We present an end-to-end optimization of the innovative Arbitrary high-order DERivative Discontinuous Galerkin (ADER-DG) software SeisSol targeting Intelå¨ Xeon Phi coprocessor platforms, achieving unprecedented earthquake model complexity through coupled simulation of full frictional sliding and seismic wave propagation. SeisSol exploits unstructured meshes to flexibly adapt for complicated geometries in realistic geological models. Seismic wave propagation is solved simultaneously with earthquake faulting in a multiphysical manner leading to a heterogeneous solver structure. Our architecture aware optimizations deliver up to 50% of peak performance, and introduce an efficient compute-communication overlapping scheme shadowing the multiphysics computations. SeisSol delivers near-optimal weak scaling, reaching 8.6 DP-PFLOPS on 8,192 nodes of the Tianhe-2 supercomputer. Our performance model projects reaching 18 -- 20 DP-PFLOPS on the full Tianhe-2 machine. Of special relevance to modern civil engineering needs, our pioneering simulation of the 1992 Landers earthquake shows highly detailed rupture evolution and ground motion at frequencies up to 10 Hz.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.6,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012188,ADER-DG;SeisSol;dynamic rupture;earthquake simulation;heterogeneous supercomputers;hybrid parallelization;petascale performance,Computational modeling;Earthquakes;Jacobian matrices;Kernel;Optimization;Seismic waves;Stress,Galerkin method;computational geometry;earthquake engineering;earthquakes;fracture;geophysics computing;mainframes;mesh generation;parallel machines;parallel processing;seismic waves;seismology;wave propagation,ADER-DG software;DP-PFLOPS;Intel Xeon Phi coprocessor platforms;Landers earthquake;SeisSol software;Tianhe-2 supercomputer;arbitrary high-order derivative discontinuous Galerkin software;architecture-aware optimizations;civil engineering;complicated geometries;compute-communication overlapping scheme shadowing;earthquake faulting;earthquake model;end-to-end optimization;full-frictional sliding;ground motion;heterogeneous solver structure;heterogeneous supercomputers;multiphysics computations;near-optimal weak scaling;performance model;petascale high-order dynamic rupture earthquake simulations;realistic geological models;rupture evolution;seismic wave propagation;unstructured meshes,,11,,50,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
NUMARCK: Machine Learning Algorithm for Resiliency and Checkpointing,Z. Chen; S. W. Son; W. Hendrix; A. Agrawal; W. K. Liao; A. Choudhary,"Electr. Eng. & Comput. Sci. Dept., Northwestern Univ., Evanston, IL, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,733,744,"Data check pointing is an important fault tolerance technique in High Performance Computing (HPC) systems. As the HPC systems move towards exascale, the storage space and time costs of check pointing threaten to overwhelm not only the simulation but also the post-simulation data analysis. One common practice to address this problem is to apply compression algorithms to reduce the data size. However, traditional lossless compression techniques that look for repeated patterns are ineffective for scientific data in which high-precision data is used and hence common patterns are rare to find. This paper exploits the fact that in many scientific applications, the relative changes in data values from one simulation iteration to the next are not very significantly different from each other. Thus, capturing the distribution of relative changes in data instead of storing the data itself allows us to incorporate the temporal dimension of the data and learn the evolving distribution of the changes. We show that an order of magnitude data reduction becomes achievable within guaranteed user-defined error bounds for each data point. We propose NUMARCK, North western University Machine learning Algorithm for Resiliency and Check pointing, that makes use of the emerging distributions of data changes between consecutive simulation iterations and encodes them into an indexing space that can be concisely represented. We evaluate NUMARCK using two production scientific simulations, FLASH and CMIP5, and demonstrate a superior performance in terms of compression ratio and compression accuracy. More importantly, our algorithm allows users to specify the maximum tolerable error on a per point basis, while compressing the data by an order of magnitude.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.65,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013047,,Approximation algorithms;Approximation methods;Checkpointing;Computational modeling;Data models;Error analysis;Machine learning algorithms,checkpointing;data analysis;iterative methods;learning (artificial intelligence);parallel processing;software fault tolerance,HPC system;NUMARCK;Northwestern University machine learning algorithm for resiliency and check pointing;data analysis;fault tolerance technique;high performance computing;simulation iteration,,3,,26,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
CYPRESS: Combining Static and Dynamic Analysis for Top-Down Communication Trace Compression,J. Zhai; J. Hu; X. Tang; X. Ma; W. Chen,"Tsinghua Univ., Beijing, China","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,143,153,"Communication traces are increasingly important, both for parallel applications' performance analysis/optimization, and for designing next-generation HPC systems. Meanwhile, the problem size and the execution scale on supercomputers keep growing, producing prohibitive volume of communication traces. To reduce the size of communication traces, existing dynamic compression methods introduce large compression overhead with the job scale. We propose a hybrid static-dynamic method that leverages information acquired from static analysis to facilitate more effective and efficient dynamic trace compression. Our proposed scheme, Cypress, extracts a program communication structure tree at compile time using inter-procedural analysis. This tree naturally contains crucial iterative computing features such as the loop structure, allowing subsequent runtime compression to ""fill in"", in a ""top-down"" manner, event details into the known communication template. Results show that Cypress reduces intra-process and inter-process compression overhead up to 5ÌÑ and 9ÌÑ respectively over state-of-the-art dynamic methods, while only introducing very low compiling overhead.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.17,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012999,High Performance Computing;Message Passing;Performance Analysis;Trace Compression,Algorithm design and analysis;Asynchronous communication;Data structures;Educational institutions;Libraries;Performance analysis;Runtime,parallel machines;program diagnostics;software performance evaluation;trees (mathematics),CYPRESS;communication template;compile time;dynamic analysis;dynamic compression methods;dynamic trace compression;execution scale;hybrid static-dynamic method;interprocedural analysis;interprocess compression overhead;intraprocess compression overhead;iterative computing features;loop structure;next-generation HPC systems;parallel application performance analysis;parallel application performance optimization;program communication structure;static analysis;supercomputers;top-down communication trace compression,,3,,31,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
A Computation- and Communication-Optimal Parallel Direct 3-Body Algorithm,P. Koanantakool; K. Yelick,"Comput. Sci. Div., Univ. of California, Berkeley, Berkeley, CA, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,363,374,"Traditional particle simulation methods are used to calculate pair wise potentials, but some problems require 3-body potentials that calculate over triplets of particles. A direct calculation of 3-body interactions involves O(n<sup>3</sup>) interactions, but has significant redundant computations that occur in a nested loop formulation. In this paper we explore algorithms for 3-body computations that simultaneously optimize three criteria: computation minimization through symmetries, communication optimality, and load balancing. We present a new 3-body algorithm that is both communication and computation optimal. Its optional replication factor, c, saves c<sup>3</sup> in latency (number of messages) and c<sup>2</sup> in bandwidth (volume), with bounded load imbalance. We also consider the k-body case and discuss an algorithm that is optimal if there is a cut off distance of less than 1/3 of the domain. The 3-body algorithm demonstrates 99% efficiency on tens of thousands of cores, showing strong scaling properties with order of magnitude speedups over the niíöve algorithm.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.35,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013017,communication-avoiding algorithms;n-body;parallel algorithms;particle methods,Approximation algorithms;Bandwidth;Clustering algorithms;Force;Heuristic algorithms;Program processors;Three-dimensional displays,parallel algorithms;resource allocation,3-body computations;3-body interactions;bounded load imbalance;communication optimality;communication-optimal parallel direct 3-body algorithm;computation minimization;computation-optimal parallel direct 3-body algorithm;k-body case;load balancing;nested loop formulation;optional replication factor;particle simulation methods,,1,,29,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Two-Choice Randomized Dynamic I/O Scheduler for Object Storage Systems,D. Dai; Y. Chen; D. Kimpe; R. Ross,"Comput. Sci. Dept., Texas Tech Univ., Lubbock, TX, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,635,646,"Object storage is considered a promising solution for next-generation (exascale) high-performance computing platform because of its flexible and high-performance object interface. However, delivering high burst-write throughput is still a critical challenge. Although deploying more storage servers can potentially provide higher throughput, it can be ineffective because the burst-write throughput can be limited by a small number of stragglers (storage servers that are occasionally slower than others). In this paper, we propose a two-choice randomized dynamic I/O scheduler that schedules the concurrent burst-write operations in a balanced way to avoid stragglers and hence achieve high throughput. The contributions in this study are threefold. First, we propose a two-choice randomized dynamic I/O scheduler with collaborative probe and preassign strategies. Second, we design and implement a redirect table and metadata maintainer to address the metadata management challenge introduced by dynamic I/O scheduling. Third, we evaluate the proposed scheduler with both simulation tests and experimental tests in an HPC cluster. The evaluation results confirm the scalability and performance benefits of the proposed I/O scheduler.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.57,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013039,,Collaboration;Dynamic scheduling;Probes;Servers;Synchronization;Throughput;Time factors,dynamic scheduling;input-output programs;parallel processing;storage management,HPC cluster;collaborative probe;dynamic I/O scheduling;exascale high-performance computing platform;high burst-write throughput;metadata maintainer;metadata management;object storage systems;redirect table;storage servers;stragglers;two-choice randomized dynamic I/O scheduler,,4,,37,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
FAST: Near Real-Time Searchable Data Analytics for the Cloud,Y. Hua; H. Jiang; D. Feng,"Wuhan Nat. Lab. for Optoelectron., Huazhong Univ. of Sci. & Technol., Wuhan, China","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,754,765,"With the explosive growth in data volume and complexity and the increasing need for highly efficient searchable data analytics, existing cloud storage systems have largely failed to offer an adequate capability for real-time data analytics. Since the true value or worth of data heavily depends on how efficiently data analytics can be carried out on the data in (near-) real-time, large fractions of data end up with their values being lost or significantly reduced due to the data staleness. To address this problem, we propose a near-real-time and cost-effective searchable data analytics methodology, called FAST. The idea behind FAST is to explore and exploit the semantic correlation within and among datasets via correlation-aware hashing and manageable flat-structured addressing to significantly reduce the processing latency, while incurring acceptably small loss of data-search accuracy. The near-real-time property of FAST enables rapid identification of correlated files and the significant narrowing of the scope of data to be processed. FAST supports several types of data analytics, which can be implemented in existing searchable storage systems. We conduct a real-world use case in which children reported missing in an extremely crowded environment (e.g., A highly popular scenic spot on a peak tourist day) are identified in a timely fashion by analyzing 60 million images using FAST. Extensive experimental results demonstrate the efficiency and efficacy of FAST in the performance improvements and energy savings.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.67,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013049,Cloud storage;data analytics;real-time performance;semantic correlation,Complexity theory;Correlation;Data analysis;Feature extraction;Real-time systems;Semantics;Vectors,cloud computing;file organisation,FAST;cloud storage system;correlation-aware hashing;real-time searchable data analytics;semantic correlation,,4,,56,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Lattice QCD with Domain Decomposition on Intelå¨ Xeon Phi Co-Processors,S. Heybrock; B. JoÌ_; D. D. Kalamkar; M. Smelyanskiy; K. Vaidyanathan; T. Wettig; P. Dubey,"Inst. for Theor. Phys., Univ. of Regensburg, Regensburg, Germany","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,69,80,"The gap between the cost of moving data and the cost of computing continues to grow, making it ever harder to design iterative solvers on extreme-scale architectures. This problem can be alleviated by alternative algorithms that reduce the amount of data movement. We investigate this in the context of Lattice Quantum Chromo dynamics and implement such an alternative solver algorithm, based on domain decomposition, on Intel<sup>å¨</sup> Xeon Phi co-processor (KNC) clusters. We demonstrate close-to-linear on-chip scaling to all 60 cores of the KNC. With a mix of single- and half-precision the domain-decomposition method sustains 400-500 Gflop/s per chip. Compared to an optimized KNC implementation of a standard solver [1], our full multi-node domain-decomposition solver strong-scales to more nodes and reduces the time-to-solution by a factor of 5.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.11,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012993,Domain decomposition;G.1.3 [Numerical Analysis]: Numerical Linear Algebra Sparse;Intelå¨ Xeon Phi coprocessor;Lattice QCD Categories and subject descriptors: D.3.4 [Programming Languages]: Processors Optimization;and very la;structured,Gold;Jacobian matrices;Lattices;Layout;Linear systems;Prefetching;Vectors,coprocessors;data handling;iterative methods;lattice theory;multiprocessing systems;physics computing;quantum chromodynamics,Intel Xeon Phi coprocessors;KNC cluster;alternative solver algorithm;close-to-linear on-chip scaling;data movement;domain decomposition;extreme-scale architecture;iterative solvers;lattice QCD;lattice quantum chromodynamics;multinode domain-decomposition solver,,4,,26,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
"A Study on Balancing Parallelism, Data Locality, and Recomputation in Existing PDE Solvers",C. Olschanowsky; M. M. Strout; S. Guzik; J. Loffeld; J. Hittinger,"Comput. Sci., Colorado State Univ., Fort Collins, CO, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,793,804,"Structured-grid PDE solver frameworks parallelize over boxes, which are rectangular domains of cells or faces in a structured grid. In the Chombo framework, the box sizes are typically 163 or 323, but larger box sizes such as 1283 would result in less surface area and therefore less storage, copying, and/or ghost cells communication overhead. Unfortunately, current on node parallelization schemes perform poorly for these larger box sizes. In this paper, we investigate 30 different inter-loop optimization strategies and demonstrate the parallel scaling advantages of some of these variants on NUMA multicore nodes. Shifted, fused, and communication-avoiding variants for 1283 boxes result in close to ideal parallel scaling and come close to matching the performance of 163 boxes on three different multicore systems for a benchmark that is a proxy for program idioms found in Computational Fluid Dynamic (CFD) codes.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.70,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013052,,Computational fluid dynamics;Equations;Kernel;Multicore processing;Optimization;Parallel processing;Schedules,grid computing;multiprocessing systems;optimisation;parallel processing;partial differential equations,CFD codes;Chombo framework;NUMA multicore nodes;PDE solvers;communication-avoiding variants;computational fluid dynamic codes;data locality;inter-loop optimization strategies;multicore systems;node parallelization schemes;parallel scaling;parallelism balancing;partial differential equation;program idioms;structured-grid PDE solver frameworks,,5,,50,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Efficient I/O and Storage of Adaptive-Resolution Data,S. Kumar; J. Edwards; P. T. Bremer; A. Knoll; C. Christensen; V. Vishwanath; P. Carns; J. A. Schmidt; V. Pascucci,"Sci. Comput. & Imaging Inst., Univ. of Utah, Salt Lake City, UT, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,413,423,"We present an efficient, flexible, adaptive-resolution I/O framework that is suitable for both uniform and Adaptive Mesh Refinement (AMR) simulations. In an AMR setting, current solutions typically represent each resolution level as an independent grid which often results in inefficient storage and performance. Our technique coalesces domain data into a unified, multiresolution representation with fast, spatially aggregated I/O. Furthermore, our framework easily extends to importance-driven storage of uniform grids, for example, by storing regions of interest at full resolution and nonessential regions at lower resolution for visualization or analysis. Our framework, which is an extension of the PIDX framework, achieves state of the art disk usage and I/O performance regardless of resolution of the data, regions of interest, and the number of processes that generated the data. We demonstrate the scalability and efficiency of our framework using the Uintah and S3D large-scale combustion codes on the Mira and Edison supercomputers.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.39,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013021,,Adaptation models;Arrays;Data models;Encoding;Indexes;Layout;Spatial resolution,data structures;grid computing;input-output programs;parallel machines,AMR setting;AMR simulation;Edison supercomputer;I/O performance;Mira supercomputer;PIDX framework;S3D large-scale combustion code;Uintah;adaptive mesh refinement simulation;adaptive-resolution I/O framework;adaptive-resolution data;data resolution;disk usage;domain data;importance-driven storage;independent grid;lower resolution;multiresolution representation;regions of interest;resolution level;uniform grid,,1,,28,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Enabling Efficient Multithreaded MPI Communication through a Library-Based Implementation of MPI Endpoints,S. Sridharan; J. Dinan; D. D. Kalamkar,,"SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,487,498,"Modern high-speed interconnection networks are designed with capabilities to support communication from multiple processor cores. The MPI endpoints extension has been proposed to ease process and thread count tradeoffs by enabling multithreaded MPI applications to efficiently drive independent network communication. In this work, we present the first implementation of the MPI endpoints interface and demonstrate the first applications running on this new interface. We use a novel library-based design that can be layered on top of any existing, production MPI implementation. Our approach uses proxy processes to isolate threads in an MPI job, eliminating threading overheads within the MPI library and allowing threads to achieve process-like communication performance. We evaluate the performance advantages of our implementation through several benchmarks and kernels. Performance results for the Lattice QCD Dslash kernel indicate that endpoints provides up to 2.9ÌÑ improvement in communication performance and 1.87ÌÑ overall performance improvement over a highly optimized hybrid MPI+OpenMP baseline on 128 processors.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.45,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013027,Endpoints;Hybrid Parallel Programming;MPI,Arrays;Context;Kernel;Libraries;Message systems;Parallel programming;Semantics,application program interfaces;message passing;multi-threading;multiprocessing systems;multiprocessor interconnection networks;software libraries,MPI endpoints extension;MPI endpoints interface;MPI job;MPI library;MPI+OpenMP baseline;high-speed interconnection network;independent network communication;lattice QCD Dslash kernel;library-based design;library-based implementation;multiple processor core;multithreaded MPI application;multithreaded MPI communication;performance evaluation;process-like communication performance;production MPI implementation;thread count tradeoff;threading overhead,,4,,26,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
FlexSlot: Moving Hadoop Into the Cloud with Flexible Slot Management,Y. Guo; J. Rao; C. Jiang; X. Zhou,"Dept. of Comput. Sci., Univ. of Colorado, Colorado Springs, CO, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,959,969,"Load imbalance is a major source of overhead in Hadoop where the uneven distribution of input data among tasks can significantly delays the job completion. Running Hadoop in a private cloud opens up opportunities for mitigating data skew with elastic resource allocation, where stragglers are expedited with more resources, yet introduces problems that often cancel out the performance gain: (1) performance interference from co running jobs may create new stragglers, (2) there exist a semantic gap between Hadoop task management and resource pool-based virtual cluster management preventing efficient resource usage. We present FlexSlot, a user-transparent task slot management scheme that automatically identifies map stragglers and resizes their slots accordingly to accelerate task execution. FlexSlot adaptively changes the number of slots on each virtual node to promote efficient usage of resource pool. Experimental results with representative benchmarks show that FlexSlot effectively reduces job completion time by 46% and achieves better resource utilization.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.83,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013065,,Acceleration;Cloud computing;Dynamic scheduling;Measurement;Memory management;Resource management;Runtime,cloud computing;data handling;parallel processing;resource allocation,FlexSlot;Hadoop task management;data distribution;data skew;flexible slot management;job completion time;load imbalance;map stragglers;performance interference;private cloud;resource allocation;resource pool-based virtual cluster management;resource usage;resource utilization;semantic gap;task execution;user-transparent task slot management scheme;virtual node,,2,,28,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
pTatin3D: High-Performance Methods for Long-Term Lithospheric Dynamics,D. A. May; J. Brown; L. L. Pourhiet,"Dept. of Earth Sci., ETH Zurich, Zu&#x0308;rich, Switzerland","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,274,284,"Simulations of long-term lithospheric deformation involve post-failure analysis of high-contrast brittle materials driven by buoyancy and processes at the free surface. Geodynamic phenomena such as subduction and continental rifting take place over millions year time scales, thus require efficient solution methods. We present pTatin3D, a geodynamics modeling package utilising the material-point-method for tracking material composition, combined with a multigrid finite-element method to solve heterogeneous, incompressible visco-plastic Stokes problems. Here we analyze the performance and algorithmic tradeoffs of pTatin3D's multigrid preconditioner. Our matrix-free geometric multigrid preconditioner trades flops for memory bandwidth to produce a time-to-solution > 2ÌÑ faster than the best available methods utilising stored matrices (plagued by memory bandwidth limitations), exploits local element structure to achieve weak scaling at 30% of FPU peak on Cray XC-30, has improved dynamic range due to smaller memory footprint, and has more consistent timing and better intra-node scalability due to reduced memory-bus and cache pressure.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.28,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013010,Stokes;geodynamics;matrix-free;multilevel preconditioners;variable viscosity;vectorization,Equations;Finite element analysis;Mathematical model;Rocks;Sparse matrices;Viscosity,Earth crust;Earth mantle;bandwidth allocation;brittleness;differential equations;finite element analysis;geophysics computing;parallel processing;viscoplasticity,Cray XC-30;cache pressure;continental rifting;geodynamic phenomena;geodynamics modeling package;high-contrast brittle materials;high-performance methods;intranode scalability;local element structure;long-term lithospheric dynamics;material composition;material-point-method;matrix-free geometric multigrid preconditioner;memory bandwidth;memory footprint;memory-bus;multigrid finite-element method;pTatin3D multigrid preconditioner;post-failure analysis;viscoplastic Stokes problems,,4,,61,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Scalable Computation of Stream Surfaces on Large Scale Vector Fields,K. Lu; H. W. Shen; T. Peterka,"Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,1008,1019,"Stream surfaces and streamlines are two popular methods for visualizing three-dimensional flow fields. While several parallel streamline computation algorithms exist, relatively little research has been done to parallelize stream surface generation. This is because load-balanced parallel stream surface computation is nontrivial, due to the strong dependency in computing the positions of the particles forming the stream surface front. In this paper, we present a new algorithm that computes stream surfaces efficiently. In our algorithm, seeding curves are divided into segments, which are then assigned to the processes. Each process is responsible for integrating the segments assigned to it. To ensure a balanced computational workload, work stealing and dynamic refinement of seeding curve segments are employed to improve the overall performance. We demonstrate the effectiveness of our parallel stream surface algorithm using several large scale flow field data sets, and show the performance and scalability on HPC systems.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.87,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013069,Algorithms;Dynamic load balancing;Flow Visualization;Parallel stream surface,Distributed databases;Heuristic algorithms;Load management;Partitioning algorithms;Runtime;Surface treatment;Vectors,data analysis;data visualisation;parallel processing;resource allocation,HPC system;flow field visualization;large scale vector field;parallel stream surface algorithm;seeding curve segment;workload balancing,,1,,29,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Optimizing Data Locality for Fork/Join Programs Using Constrained Work Stealing,J. Lifflander; S. Krishnamoorthy; L. V. Kale,"Dept. of Comput. Sci., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,857,868,"We present an approach to improving data locality across different phases of fork/join programs scheduled using work stealing. The approach consists of: (1) user-specified and automated approaches to constructing a steal tree, the schedule of steal operations, and (2) constrained work-stealing algorithms that constrain the actions of the scheduler to mirror a given steal tree. These are combined to construct work-stealing schedules that maximize data locality across computation phases while ensuring load balance within each phase. These algorithms are also used to demonstrate dynamic coarsening, an optimization to improve spatial locality and sequential overheads by combining many finer-grained tasks into coarser tasks while ensuring sufficient concurrency for locality-optimized load balance. Implementation and evaluation in Cilk demonstrate performance improvements of up to 2.5x on 80 cores. We also demonstrate that dynamic coarsening can combine the performance benefits of coarse task specification with the adaptability of finer tasks.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.75,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013057,cilk;data locality;fork/join;task granularity,Heuristic algorithms;Kernel;Optimization;Parallel processing;Runtime;Schedules;Synchronization,concurrency control;parallel programming;processor scheduling;resource allocation;tree data structures,Cilk;automated approach;coarse task specification;coarser tasks;computation phases;concurrency;constrained work stealing;constrained work-stealing algorithms;constraint scheduler actions;data locality improvement;data locality maximization;data locality optimization;dynamic coarsening;fine-grained task adaptability;fork/join program scheduling;locality-optimized load balancing;performance improvements;sequential overhead improvement;spatial locality improvement;steal operation scheduling;steal tree construction;user-specified approach;work-stealing scheduling construction,,3,,38,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Physics-Based Urban Earthquake Simulation Enhanced by 10.7 BlnDOF ÌÑ 30 K Time-Step Unstructured FE Non-Linear Seismic Wave Simulation,T. Ichimura; K. Fujita; S. Tanaka; M. Hori; M. Lalith; Y. Shizawa; H. Kobayashi,"Earthquake Res. Inst. & Dept. of Civil Eng., Univ. of Tokyo, Tokyo, Japan","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,15,26,"With the aim of dramatically improving the reliability of urban earthquake response analyses, we developed an unstructured 3-D finite-element-based MPI-OpenMP hybrid seismic wave amplification simulation code, GAMERA. On the K computer, GAMERA was able to achieve a size-up efficiency of 87.1% up to the full K computer. Next, we applied GAMERA to a physics-based urban earthquake response analysis for Tokyo. Using 294,912 CPU cores of the K computer for 11 h, 32 min, we analyzed the 3-D non-linear ground motion of a 10.7 BlnDOF problem with 30 K time steps. Finally, we analyzed the stochastic response of 13,275 building structures in the domain considering uncertainty in structural parameters using 3 h, 56 min of 80,000 CPU cores of the K computer. Although a large amount of computer resources is needed presently, such analyses can change the quality of disaster estimations and are expected to become standard in the future.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.7,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012189,Time-to-solution;scalability;unstructured FEM;urban earthquake simulation,Analytical models;Computational modeling;Computers;Earthquakes;Finite element analysis;Mathematical model;Seismic waves,application program interfaces;buildings (structures);earthquake engineering;finite element analysis;geophysics computing;message passing;seismic waves;seismology,10.7 BlnDOF problem;30 K time steps;3D finite-element;3D nonlinear ground motion;FE nonlinear seismic wave simulation;GAMERA;MPI-OpenMP hybrid seismic wave amplification simulation code;building structures;physics-based urban earthquake simulation;stochastic response;urban earthquake response analysis,,7,,41,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
The Lightweight Distributed Metric Service: A Scalable Infrastructure for Continuous Monitoring of Large Scale Computing Systems and Applications,A. Agelastos; B. Allan; J. Brandt; P. Cassella; J. Enos; J. Fullop; A. Gentile; S. Monk; N. Naksinehaboon; J. Ogden; M. Rajan; M. Showerman; J. Stevenson; N. Taerat; T. Tucker,"Sandia Nat. Labs. ABQ, Albuquerque, NM, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,154,165,"Understanding how resources of High Performance Compute platforms are utilized by applications both individually and as a composite is key to application and platform performance. Typical system monitoring tools do not provide sufficient fidelity while application profiling tools do not capture the complex interplay between applications competing for shared resources. To gain new insights, monitoring tools must run continuously, system wide, at frequencies appropriate to the metrics of interest while having minimal impact on application performance. We introduce the Lightweight Distributed Metric Service for scalable, lightweight monitoring of large scale computing systems and applications. We describe issues and constraints guiding deployment in Sandia National Laboratories' capacity computing environment and on the National Center for Supercomputing Applications' Blue Waters platform including motivations, metrics of choice, and requirements relating to the scale and specialized nature of Blue Waters. We address monitoring overhead and impact on application performance and provide illustrative profiling results.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.18,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013000,resource management;resource monitoring,Bandwidth;Instruction sets;Measurement;Memory management;Monitoring;Resource management;Sockets,parallel processing;resource allocation;software metrics,computing system monitoring;high performance computing platform;lightweight distributed metric service;resource utilization,,12,,43,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
A Communication-Optimal Framework for Contracting Distributed Tensors,S. Rajbhandari; A. Nikam; P. W. Lai; K. Stock; S. Krishnamoorthy; P. Sadayappan,"Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,375,386,"Tensor contractions are extremely compute intensive generalized matrix multiplication operations encountered in many computational science fields, such as quantum chemistry and nuclear physics. Unlike distributed matrix multiplication, which has been extensively studied, limited work has been done in understanding distributed tensor contractions. In this paper, we characterize distributed tensor contraction algorithms on torus networks. We develop a framework with three fundamental communication operators to generate communication-efficient contraction algorithms for arbitrary tensor contractions. We show that for a given amount of memory per processor, the framework is communication optimal for all tensor contractions. We demonstrate performance and scalability of the framework on up to 262,144 cores on a Blue Gene/Q supercomputer.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.36,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013018,,Chemistry;Distributed databases;Indexes;Memory management;Scalability;Tensile stress;Three-dimensional displays,mathematics computing;matrix multiplication;parallel machines;tensors,Blue Gene/Q supercomputer;communication-optimal framework;distributed tensor contraction algorithm;matrix multiplication operation;torus network,,3,,28,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Parallel Programming with Migratable Objects: Charm++ in Practice,B. Acun; A. Gupta; N. Jain; A. Langer; H. Menon; E. Mikida; X. Ni; M. Robson; Y. Sun; E. Totoni; L. Wesolowski; L. Kale,"Dept. of Comput. Sci., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,647,658,"The advent of petascale computing has introduced new challenges (e.g. Heterogeneity, system failure) for programming scalable parallel applications. Increased complexity and dynamism in science and engineering applications of today have further exacerbated the situation. Addressing these challenges requires more emphasis on concepts that were previously of secondary importance, including migratability, adaptivity, and runtime system introspection. In this paper, we leverage our experience with these concepts to demonstrate their applicability and efficacy for real world applications. Using the CHARM++ parallel programming framework, we present details on how these concepts can lead to development of applications that scale irrespective of the rough landscape of supercomputing technology. Empirical evaluation presented in this paper spans many miniapplications and real applications executed on modern supercomputers including Blue Gene/Q, Cray XE6, and Stampede.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.58,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013040,,Checkpointing;Computational modeling;Control systems;Load management;Program processors;Programming;Runtime,mainframes;parallel machines;parallel programming,Blue Gene/Q;CHARM++ parallel programming framework;Cray XE6;Stampede;migratable objects;petascale computing;rough landscape;runtime system introspection;scalable parallel application programming;science and engineering applications;supercomputing technology,,14,,50,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Efficient Sparse Matrix-Vector Multiplication on GPUs Using the CSR Storage Format,J. L. Greathouse; M. Daga,"AMD Res., Adv. Micro Devices Inc., USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,769,780,"The performance of sparse matrix vector multiplication (SpMV) is important to computational scientists. Compressed sparse row (CSR) is the most frequently used format to store sparse matrices. However, CSR-based SpMV on graphics processing units (GPUs) has poor performance due to irregular memory access patterns, load imbalance, and reduced parallelism. This has led researchers to propose new storage formats. Unfortunately, dynamically transforming CSR into these formats has significant runtime and storage overheads. We propose a novel algorithm, CSR-Adaptive, which keeps the CSR format intact and maps well to GPUs. Our implementation addresses the aforementioned challenges by (i) efficiently accessing DRAM by streaming data into the local scratchpad memory and (ii) dynamically assigning different numbers of rows to each parallel GPU compute unit. CSR-Adaptive achieves an average speedup of 14.7ÌÑ over existing CSR-based algorithms and 2.3ÌÑ over clSpMV cocktail, which uses an assortment of matrix formats.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.68,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013050,AMD;Sparse matrix-vector multiplication (SpMV);compressed sparse row (CSR);general purpose computation on graphics processing units (GPGPU);performance acceleration,Bandwidth;Graphics processing units;Heuristic algorithms;Instruction sets;Random access memory;Sparse matrices;Vectors,graphics processing units;mathematics computing;matrix multiplication;parallel processing;sparse matrices,CSR storage format;CSR-adaptive;CSR-based SpMV;DRAM;clSpMV cocktail;compressed sparse row;graphics processing units;local scratchpad memory;parallel GPU compute unit;sparse matrix-vector multiplication;streaming data,,25,,34,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Mapping to Irregular Torus Topologies and Other Techniques for Petascale Biomolecular Simulation,J. C. Phillips; Y. Sun; N. Jain; E. J. Bohm; L. V. KalÌ©,"Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,81,91,"Currently deployed petascale supercomputers typically use toroidal network topologies in three or more dimensions. While these networks perform well for topology-agnostic codes on a few thousand nodes, leadership machines with 20,000 nodes require topology awareness to avoid network contention for communication-intensive codes. Topology adaptation is complicated by irregular node allocation shapes and holes due to dedicated input/output nodes or hardware failure. In the context of the popular molecular dynamics program NAMD, we present methods for mapping a periodic 3-D grid of fixed-size spatial decomposition domains to 3-D Cray Gemini and 5-D IBM Blue Gene/Q toroidal networks to enable hundred-million atom full machine simulations, and to similarly partition node allocations into compact domains for smaller simulations using multiple copy algorithms. Additional enabling techniques are discussed and performance is reported for NCSA Blue Waters, ORNL Titan, ANL Mira, TACC Stampede, and NERSC Edison.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.12,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012994,,Biological system modeling;Computational modeling;Graphics processing units;Network topology;Partitioning algorithms;Resource management;Topology,biology computing;digital simulation;mainframes;molecular biophysics;molecular dynamics method;parallel machines;topology,3D Cray Gemini toroidal networks;5D IBM Blue Gene/Q toroidal networks;ANL Mira;NCSA Blue Waters;NERSC Edison;ORNL Titan;TACC Stampede;communication-intensive codes;fixed-size spatial decomposition;full machine simulations;hardware failure;irregular node allocation shapes;irregular torus topology mapping;leadership machines;molecular dynamics program NAMD;multiple-copy algorithms;network contention;partition node allocations;periodic 3D grid;petascale biomolecular simulation;petascale supercomputers;topology adaptation;topology awareness;topology-agnostic codes;toroidal network topologies,,4,,26,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Maximizing Throughput of Overprovisioned HPC Data Centers Under a Strict Power Budget,O. Sarood; A. Langer; A. Gupta; L. Kale,"Dept. of Comput. Sci., Univ. of Illinois Urbana-Champaign, Urbana, IL, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,807,818,"Building future generation supercomputers while constraining their power consumption is one of the biggest challenges faced by the HPC community. For example, US Department of Energy has set a goal of 20 MW for an exascale (1018 flops) supercomputer. To realize this goal, a lot of research is being done to revolutionize hardware design to build power efficient computers and network interconnects. In this work, we propose a software-based online resource management system that leverages hardware facilitated capability to constrain the power consumption of each node in order to optimally allocate power and nodes to a job. Our scheme uses this hardware capability in conjunction with an adaptive runtime system that can dynamically change the resource configuration of a running job allowing our resource manager to re-optimize allocation decisions to running jobs as new jobs arrive, or a running job terminates. We also propose a performance modeling scheme that estimates the essential power characteristics of a job at any scale. The proposed online resource manager uses these performance characteristics for making scheduling and resource allocation decisions that maximize the job throughput of the supercomputer under a given power budget. We demonstrate the benefits of our approach by using a mix of jobs with different power response characteristics. We show that with a power budget of 4:75 MW, we can obtain up to 5:2X improvement in job throughput when compared with the SLURM scheduling policy that is power-unaware. We corroborate our results with real experiments on a relatively small scale cluster, in which we obtain a 1:7X improvement.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.71,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013053,,Linear programming;Mathematical model;Parallel processing;Power demand;Resource management;Throughput;Time-frequency analysis,computer centres;mainframes;parallel machines;power consumption;resource allocation;scheduling,SLURM scheduling policy;adaptive runtime system;hardware facilitated capability;network interconnects;node allocation;online resource manager;overprovisioned HPC data centers;performance modeling scheme;power 4.75 MW;power allocation;power consumption;power efficient computers;power response characteristics;resource allocation decisions;software-based online resource management system;strict power budget;supercomputers;throughput maximization,,17,,45,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
An Image-Based Approach to Extreme Scale in Situ Visualization and Analysis,J. Ahrens; S. Jourdain; P. OLeary; J. Patchett; D. H. Rogers; M. Petersen,"Los Alamos Nat. Lab., Los Alamos, NM, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,424,434,"Extreme scale scientific simulations are leading a charge to exascale computation, and data analytics runs the risk of being a bottleneck to scientific discovery. Due to power and I/O constraints, we expect in situ visualization and analysis will be a critical component of these workflows. Options for extreme scale data analysis are often presented as a stark contrast: write large files to disk for interactive, exploratory analysis, or perform in situ analysis to save detailed data about phenomena that a scientists knows about in advance. We present a novel framework for a third option - a highly interactive, image-based approach that promotes exploration of simulation results, and is easily accessed through extensions to widely used open source tools. This in situ approach supports interactive exploration of a wide range of results, while still significantly reducing data movement and storage.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.40,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013022,,Analytical models;Atmospheric modeling;Cameras;Computational modeling;Data models;Data visualization;Databases,data analysis;data visualisation;image processing;public domain software;software tools,extreme scale data analysis;extreme scale in situ visualization;extreme scale scientific simulations;interactive image-based approach;open source tools,,12,,41,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
MC-Checker: Detecting Memory Consistency Errors in MPI One-Sided Applications,Z. Chen; J. Dinan; Z. Tang; P. Balaji; H. Zhong; J. Wei; T. Huang; F. Qin,,"SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,499,510,"One-sided communication decouples data movement and synchronization by providing support for asynchronous reads and updates of distributed shared data. While such interfaces can be extremely efficient, they also impose challenges in properly performing asynchronous accesses to shared data. This paper presents MC-Checker, a new tool that detects memory consistency errors in MPI one-sided applications. MCChecker first performs online instrumentation and captures relevant dynamic events, such as one-sided communications and load/store operations. MC-Checker then performs analysis to detect memory consistency errors. When found, errors are reported along with useful diagnostic information. Experiments indicate that MC-Checker is effective at detecting and diagnosing memory consistency bugs in MPI one-sided applications, with low overhead, ranging from 24.6% to 71.1%, with an average of 45.2%.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.46,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013028,Bug Detection;MPI;One-Sided Communication,Analytical models;Computer bugs;Data models;Instruments;Load modeling;Runtime;Synchronization,application program interfaces;message passing;program debugging;program diagnostics,MC-Checker;MPI one-sided applications;data movement;data synchronization;distributed shared data;dynamic events;load/store operations;memory consistency bug diagnosis;memory consistency error detection;one-sided communication;online instrumentation,,1,,35,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Oil and Water Can Mix: An Integration of Polyhedral and AST-Based Transformations,J. Shirako; L. N. Pouchet; V. Sarkar,,"SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,287,298,"Optimizing compilers targeting modern multi-core machines require complex program restructuring to expose the best combinations of coarse- and fine-grain parallelism and data locality. The polyhedral compilation model has provided significant advancements in the seamless handling of compositions of loop transformations, thereby exposing multiple levels of parallelism and improving data reuse. However, it usually implements abstract optimization objectives, for example ""maximize data reuse"", which often does not deliver best performance, e.g., The complex loop structures generated can be detrimental to short-vector SIMD performance. In addition, several key transformations such as pipeline-parallelism and unroll-and-jam are difficult to express in the polyhedral framework. In this paper, we propose a novel optimization flow that combines polyhedral and syntactic/AST-based transformations. It generates high-performance code that contains regular loops which can be effectively vectorized, while still implementing sufficient parallelism and data reuse. It combines several transformation stages using both polyhedral and AST-based transformations, delivering performance improvements of up to 3ÌÑ over the PoCC polyhedral compiler on Intel Nehalem and IBM Power7 multicore processors.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.29,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013011,,Arrays;Data models;Nickel;Optimization;Parallel processing;Schedules;Silicon,optimising compilers;parallelising compilers,AST-based transformations;IBM Power7 multicore processor;Intel Nehalem multicore processor;PoCC polyhedral compiler;abstract optimization;coarse-grain parallelism;complex loop structures;data locality;data reuse maximization;data reusing;fine-grain parallelism;high-performance code generation;loop transformations;multicore machines;optimization flow;optimizing compilers;pipeline-parallelism;polyhedral compilation model;polyhedral framework;polyhedral-based transformations;program restructuring;seamless composition handling;short-vector SIMD performance;syntactic-based transformation;transformation stages;unroll-and-jam,,3,,38,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
In-Situ Feature Extraction of Large Scale Combustion Simulations Using Segmented Merge Trees,A. G. Landge; V. Pascucci; A. Gyulassy; J. C. Bennett; H. Kolla; J. Chen; P. T. Bremer,"SCI Inst., Univ. of Utah, Salt Lake City, UT, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,1020,1031,"The ever increasing amount of data generated by scientific simulations coupled with system I/O constraints are fueling a need for in-situ analysis techniques. Of particular interest are approaches that produce reduced data representations while maintaining the ability to redefine, extract, and study features in a post-process to obtain scientific insights. This paper presents two variants of in-situ feature extraction techniques using segmented merge trees, which encode a wide range of threshold based features. The first approach is a fast, low communication cost technique that generates an exact solution but has limited scalability. The second is a scalable, local approximation that nevertheless is guaranteed to correctly extract all features up to a predefined size. We demonstrate both variants using some of the largest combustion simulations available on leadership class supercomputers. Our approach allows state-of-the-art, feature-based analysis to be performed in-situ at significantly higher frequency than currently possible and with negligible impact on the overall simulation runtime.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.88,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013070,feature extraction;in situ analysis;merge tree computation;segmented merge tree;topological data analysis,Algorithm design and analysis;Analytical models;Bismuth;Combustion;Computational modeling;Feature extraction;Program processors,approximation theory;data structures;feature extraction;image segmentation;trees (mathematics),combustion simulations;feature-based analysis;in-situ analysis techniques;in-situ feature extraction;leadership class supercomputers;local approximation;merge tree segmentation;reduced data representations;scientific simulations;simulation runtime;system I/O constraints;threshold based features,,7,,44,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
"Real-Time Scalable Cortical Computing at 46 Giga-Synaptic OPS/Watt with ~100ÌÑ Speedup in Time-to-Solution and ~100,000ÌÑ Reduction in Energy-to-Solution",A. S. Cassidy; R. Alvarez-Icaza; F. Akopyan; J. Sawada; J. V. Arthur; P. A. Merolla; P. Datta; M. G. Tallada; B. Taba; A. Andreopoulos; A. Amir; S. K. Esser; J. Kusnitz; R. Appuswamy; C. Haymes; B. Brezzo; R. Moussalli; R. Bellofatto; C. Baks; M. Mastro; K. Schleupen; C. E. Cox; K. Inoue; S. Millman; N. Imam; E. Mcquinn; Y. Y. Nakamura; I. Vo; C. Guok; D. Nguyen; S. Lekuch; S. Asaad; D. Friedman; B. L. Jackson; M. D. Flickner; W. P. Risk; R. Manohar; D. S. Modha,,"SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,27,38,"Drawing on neuroscience, we have developed a parallel, event-driven kernel for neurosynaptic computation, that is efficient with respect to computation, memory, and communication. Building on the previously demonstrated highly optimized software expression of the kernel, here, we demonstrate True North, a co-designed silicon expression of the kernel. True North achieves five orders of magnitude reduction in energy to-solution and two orders of magnitude speedup in time-to solution, when running computer vision applications and complex recurrent neural network simulations. Breaking path with the von Neumann architecture, True North is a 4,096 core, 1 million neuron, and 256 million synapse brain-inspired neurosynaptic processor, that consumes 65mW of power running at real-time and delivers performance of 46 Giga-Synaptic OPS/Watt. We demonstrate seamless tiling of True North chips into arrays, forming a foundation for cortex-like scalability. True North's unprecedented time-to-solution, energy-to-solution, size, scalability, and performance combined with the underlying flexibility of the kernel enable a broad range of cognitive applications.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.8,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012190,,Compass;Computational modeling;Computer architecture;Kernel;Message systems;Nerve fibers,computer vision;neural chips;neural net architecture;recurrent neural nets,True North chips;codesigned silicon expression;complex recurrent neural network simulation;computer vision application;cortex-like scalability;energy to-solution;energy-to-solution;giga-synaptic OPS/Watt;magnitude reduction;magnitude speedup;neuroscience;neurosynaptic computation;parallel event-driven kernel;real-time scalable cortical computing;software expression;synapse brain-inspired neurosynaptic processor;time-to-solution;von Neumann architecture,,7,,66,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Fast Parallel Computation of Longest Common Prefixes,J. Shun,,"SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,387,398,"Suffix arrays and the corresponding longest common prefix (LCP) array have wide applications in bioinformatics, information retrieval and data compression. In this work, we propose and theoretically analyze new parallel algorithms for computing the LCP array given the suffix array as input. Most of our algorithms have a work and depth (parallel time) complexity related to the LCP values of the input. We also present a slight variation of Kaíörkkaíöinen and Sanders' skew algorithm that requires linear work and poly-logarithmic depth in the worst case. We present a comprehensive experimental study of our parallel algorithms along with existing parallel and sequential LCP algorithms. On a variety of real-world and artificial strings, we show that on a 40-core shared-memory machine our fastest algorithm is up to 2.3 times faster than the fastest existing parallel algorithm, and up to 21.8 times faster than the fastest sequential LCP algorithm.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.37,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013019,,Algorithm design and analysis;Arrays;Bioinformatics;Heuristic algorithms;Parallel algorithms;Phased arrays;Radiation detectors,computational complexity;data structures;parallel algorithms,40-core shared-memory machine;Kaíörkkaíöinen-Sanders skew algorithm;LCP array;artificial strings;longest common prefix array;parallel LCP algorithms;parallel algorithms;parallel time complexity;polylogarithmic depth;sequential LCP algorithms;suffix arrays,,5,,42,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Metascalable Quantum Molecular Dynamics Simulations of Hydrogen-on-Demand,K. I. Nomura; R. K. Kalia; A. Nakano; P. Vashishta; K. Shimamura; F. Shimojo; M. Kunaseth; P. C. Messina; N. A. Romerod,"Dept. of Comput. Sci., Univ. of Southern California, Los Angeles, CA, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,661,673,"We enabled an unprecedented scale of quantum molecular dynamics simulations through algorithmic innovations. A new lean divide-and-conquer density functional theory algorithm significantly reduces the prefactor of the O(N) computational cost based on complexity and error analyses. A globally scalable and locally fast solver hybridizes a global real-space multigrid with local plane-wave bases. The resulting weak-scaling parallel efficiency was 0.984 on 786,432 IBM Blue Gene/Q cores for a 50.3 million-atom (39.8 trillion degrees-of-freedom) system. The time-to-solution was 60-times less than the previous state-of-the art, owing to enhanced strong scaling by hierarchical band-space domain decomposition and high floating-point performance (50.5% of the peak). Production simulation involving 16,661 atoms for 21,140 time steps (or 129,208 self-consistent-field iterations) revealed a novel nanostructural design for on-demand hydrogen production from water, advancing renewable energy technologies. This metascalable (or ""design once, scale on new architectures"") algorithm is used for broader applications within a recently proposed divide-conquer-recombine paradigm.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.59,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013041,Density functional theory;Divide-and-conquer;On-demand hydrogen production,Computational efficiency;Computational modeling;Computers;Discrete Fourier transforms;Production;Quantum mechanics;Wave functions,computational complexity;density functional theory;divide and conquer methods;error analysis;hydrogen production;molecular dynamics method;parallel processing;production engineering computing,IBM Blue Gene/Q cores;algorithmic innovations;computational cost;divide-conquer-recombine paradigm;error analyses;floating-point performance;global real-space multigrid;hierarchical band-space-domain decomposition;hydrogen-on-demand;lean divide-and-conquer density functional theory algorithm;local plane-wave bases;metascalable quantum molecular dynamics simulations;nanostructural design;on-demand hydrogen production;renewable energy technologies;self-consistent-field iterations;weak-scaling parallel efficiency,,2,,87,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Parallel De Bruijn Graph Construction and Traversal for De Novo Genome Assembly,E. Georganas; A. BuluÌ¤; J. Chapman; L. Oliker; D. Rokhsar; K. Yelick,"Comput. Res. Div., Lawrence Berkeley Nat. Lab., Berkeley, CA, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,437,448,"De novo whole genome assembly reconstructs genomic sequence from short, overlapping, and potentially erroneous fragments called reads. We study optimized parallelization of the most time-consuming phases of Meraculous, a state of-the-art production assembler. First, we present a new parallel algorithm for k-mer analysis, characterized by intensive communication and I/O requirements, and reduce the memory requirements by 6.93ÌÑ. Second, we efficiently parallelize de Bruijn graph construction and traversal, which necessitates a distributed hash table and is a key component of most de novo assemblers. We provide a novel algorithm that leverages one-sided communication capabilities of the Unified Parallel C (UPC) to facilitate the requisite fine-grained parallelism and avoidance of data hazards, while analytically proving its scalability properties. Overall results show unprecedented performance and efficient scaling on up to 15,360 cores of a Cray XC30, on human genome as well as the challenging wheat genome, with performance improvement from days to seconds.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.41,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013023,,Algorithm design and analysis;Assembly;Bioinformatics;Genomics;Memory management;Program processors;Sequential analysis,Cray computers;biology computing;genomics;graph theory;input-output programs;parallel algorithms;program assemblers,Cray XC30;I/O requirement;Meraculous;UPC;data hazard avoidance;de novo assembler;de novo genome assembly;distributed hash table;fine-grained parallelism;genomic sequence;human genome;k-mer analysis;memory requirements;one-sided communication capability;optimized parallelization;parallel algorithm;parallel de Bruijn graph construction and traversal;production assembler;scalability property;unified parallel C;wheat genome,,9,,25,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Scheduling Multi-tenant Cloud Workloads on Accelerator-Based Systems,D. Sengupta; A. Goswami; K. Schwan; K. Pallavi,"Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,513,524,"Accelerator-based systems are making rapid inroads into becoming platforms of choice for high end cloud services. There is a need therefore, to move from the current model in which high performance applications explicitly and programmatically select the GPU devices on which to run, to a dynamic model where GPUs are treated as first class schedulable entities. The Strings scheduler realizes this vision by decomposing the GPU scheduling problem into a combination of load balancing and per-device scheduling. (i) Device-level scheduling efficiently uses all of a GPU's hardware resources, including its computational and data movement engines, and (ii) load balancing goes beyond obtaining high throughput, to ensure fairness through prioritizing GPU requests that have attained least service. With its methods, Strings achieves improvements in system throughput and fairness of up to 8.70ÌÑ and 13%, respectively, compared to the CUDA runtime.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.47,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013029,CUDA;GPU;Multi-tenancy;hierarchical scheduling;runtime systems;virtualization,Context;Graphics processing units;Processor scheduling;Runtime;Servers;Switches;Synchronization,cloud computing;graphics processing units;parallel processing;resource allocation;scheduling,GPU scheduling problem;accelerator-based systems;data movement engines;device-level scheduling;dynamic model;high end cloud services;high performance applications;load balancing;multitenant cloud workload scheduling;per-device scheduling;strings scheduler,,10,,43,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Compiler Techniques for Massively Scalable Implicit Task Parallelism,T. G. Armstrong; J. M. Wozniak; M. Wilde; I. T. Foster,"Dept. of Comput. Sci., Univ. of Chicago, Chicago, IL, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,299,310,"Swift/T is a high-level language for writing concise, deterministic scripts that compose serial or parallel codes implemented in lower-level programming models into large-scale parallel applications. It executes using a data-driven task parallel execution model that is capable of orchestrating millions of concurrently executing asynchronous tasks on homogeneous or heterogeneous resources. Producing code that executes efficiently at this scale requires sophisticated compiler transformations: poorly optimized code inhibits scaling with excessive synchronization and communication. We present a comprehensive set of compiler techniques for data-driven task parallelism, including novel compiler optimizations and intermediate representations. We report application benchmark studies, including unbalanced tree search and simulated annealing, and demonstrate that our techniques greatly reduce communication overhead and enable extreme scalability, distributing up to 612 million dynamically load balanced tasks per second at scales of up to 262,144 cores without explicit parallelism, synchronization, or load balancing in application code.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.30,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013012,,Data models;Load modeling;Optimization;Parallel processing;Runtime;Servers;Synchronization,concurrency control;optimising compilers;parallel programming;simulated annealing;tree searching,Swift/T;application benchmark;application code;asynchronous tasks;code optimization;communication overhead reduction;compiler optimizations;compiler transformations;data-driven task parallel execution model;data-driven task parallelism;deterministic scripts;heterogeneous resource;high-level language;homogeneous resource;intermediate representations;load balancing;lower-level programming models;parallel applications;parallel codes;scalable implicit task parallelism;serial codes;simulated annealing;unbalanced tree search,,8,,40,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
ECC Parity: A Technique for Efficient Memory Error Resilience for Multi-Channel Memory Systems,X. Jian; R. Kumar,"Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,1035,1046,"Servers and HPC systems often use a strong memory error correction code, or ECC, to meet their reliability and availability requirements. However, these ECCs often require significant capacity and/or power overheads. We observe that since memory channels are independent from one another, error correction typically needs to be performed for one channel at a time. Based on this observation, we show that instead of always storing in memory the actual ECC correction bits as do existing systems, it is sufficient to store the bitwise parity of the ECC correction bits of different channels for fault-free memory regions, and store the actual ECC correction bits only for faulty memory regions. By trading off the resultant ECC capacity overhead reduction for improved memory energy efficiency, the proposed technique reduces memory energy per instruction by 54.4% and 20.6%, respectively, compared to a commercial chip kill correct ECC and a DIMM-kill correct ECC, while incurring similar or lower capacity overheads.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.89,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013071,,Circuit faults;Error correction;Error correction codes;Layout;Memory management;Optimization;Resilience,DRAM chips;error correction codes;parallel processing;parity check codes;storage management,DIMM-kill correct ECC;ECC capacity overhead reduction;ECC correction bits;ECC parity;HPC system;availability requirement;bitwise parity;commercial chip kill correct ECC;fault-free memory region;faulty memory region;memory channel;memory energy efficiency;memory energy per instruction;memory error correction code;memory error resilience;multichannel memory system;reliability requiement,,3,1,25,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Orion: Scaling Genomic Sequence Matching with Fine-Grained Parallelization,K. Mahadik; S. Chaterji; B. Zhou; M. Kulkarni; S. Bagchi,"Sch. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,449,460,"Gene sequencing instruments are producing huge volumes of data, straining the capabilities of current database searching algorithms and hindering efforts of researchers analyzing large collections of data to obtain greater insights. In the space of parallel genomic sequence search, most of the popular software packages, like mpiBLAST, use the database segmentation approach, wherein the entire database is sharded and searched on different nodes. However this approach does not scale well with the increasing length of individual query sequences as well as the rapid growth in size of sequence databases. In this paper, we propose a fine-grained parallelism technique, called Orion, that divides the input query into an adaptive number of fragments and shards the database. Our technique achieves higher parallelism (and hence speedup) and load balancing than database sharding alone, while maintaining 100% accuracy. We show that it is 12.3X faster than mpiBLAST for solving a relevant comparative genomics problem.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.42,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013024,,Bioinformatics;DNA;Databases;Genomics;Organisms;Parallel processing,biology computing;database management systems;genomics;query processing;string matching,Orion;database segmentation;fine-grained parallelization;genomic sequence matching;query sequence,,1,,30,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Parallel Bayesian Network Structure Learning for Genome-Scale Gene Networks,S. Misra; M. Vasimuddin; K. Pamnany; S. P. Chockalingam; Y. Dong; M. Xie; M. R. Aluru; S. Aluru,"Parallel Comput. Lab., Intel Corp., Bangalore, India","SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",20150119,2014,,,461,472,"Learning Bayesian networks is NP-hard. Even with recent progress in heuristic and parallel algorithms, modeling capabilities still fall short of the scale of the problems encountered. In this paper, we present a massively parallel method for Bayesian network structure learning, and demonstrate its capability by constructing genome-scale gene networks of the model plant Arabidopsis thaliana from over 168.5 million gene expression values. We report strong scaling efficiency of 75% and demonstrate scaling to 1.57 million cores of the Tianhe-2 supercomputer. Our results constitute three and five orders of magnitude increase over previously published results in the scale of data analyzed and computations performed, respectively. We achieve this through algorithmic innovations, using efficient techniques to distribute work across all compute nodes, all available processors and coprocessors on each node, all available threads on each processor and coprocessor, and vectorization techniques to maximize single thread performance.",2167-4329;21674329,Electronic:978-1-4799-5500-8; POD:978-1-4799-5501-5,10.1109/SC.2014.43,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7013025,Bayesian networks;gene networks;parallel machine learning;systems biology,Bayes methods;Bioinformatics;Coprocessors;Genomics;Hypercubes;Instruction sets;Vectors,belief networks;biology computing;genetic algorithms;genomics;learning (artificial intelligence);parallel algorithms,NP-hard;Tianhe-2 supercomputer;algorithmic innovation;gene expression value;genome-scale gene networks;heuristic algorithm;learning Bayesian networks;model plant Arabidopsis thaliana;modeling capability;parallel Bayesian network structure learning;parallel algorithm;scaling efficiency;single thread performance;vectorization technique,,1,,23,,,16-21 Nov. 2014,,IEEE,IEEE Conference Publications
Scalable matrix computations on large scale-free graphs using 2D graph partitioning,E. G. Boman; K. D. Devine; S. Rajamanickam,"Scalable Algorithms Dept., Sandia Nat. Labs., Albuquerque, NM, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Scalable parallel computing is essential for processing large scale-free (power-law) graphs. The distribution of data across processes becomes important on distributed-memory computers with thousands of cores. It has been shown that two-dimensional layouts (edge partitioning) can have significant advantages over traditional one-dimensional layouts. However, simple 2D block distribution does not use the structure of the graph, and more advanced 2D partitioning methods are too expensive for large graphs. We propose a new two-dimensional partitioning algorithm that combines graph partitioning with 2D block distribution. The computational cost of the algorithm is essentially the same as 1D graph partitioning. We study the performance of sparse matrix-vector multiplication (SpMV) for scale-free graphs from the web and social networks using several different partitioners and both 1D and 2D data layouts. We show that SpMV run time is reduced by exploiting the graph's structure. Contrary to popular belief, we observe that current graph and hypergraph partitioners often yield relatively good partitions on scale-free graphs. We demonstrate that our new 2D partitioning method consistently outperforms the other methods considered, for both SpMV and an eigensolver, on matrices with up to 1.6 billion nonzeros using up to 16,384 cores.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503293,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877483,graph partitioning;parallel computing;scale-free graphs;sparse matrix-vector multiplication;two-dimensional distribution,Eigenvalues and eigenfunctions;Layout;Partitioning algorithms;Software;Sparse matrices;Symmetric matrices;Vectors,data handling;graph theory;matrix multiplication;parallel processing;sparse matrices;vectors,1D data layouts;1D graph partitioning;2D block distribution;2D data layouts;2D graph partitioning;SpMV;advanced 2D partitioning methods;computational cost;data distribution;distributed-memory computers;eigensolver;graph structure;hypergraph partitioners;large scale-free graph processing;one-dimensional layouts;scalable matrix computations;scalable parallel computing;social networks;sparse matrix-vector multiplication;two-dimensional layouts;two-dimensional partitioning algorithm,,13,,35,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
A large-scale cross-architecture evaluation of thread-coarsening,A. Magni; C. Dubach; M. F. P. O'Boyle,"University of Edinburgh, UK","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,11,"OpenCL has become the de-facto data parallel programming model for parallel devices in today's high-performance supercomputers. OpenCL was designed with the goal of guaranteeing program portability across hardware from different vendors. However, achieving good performance is hard, requiring manual tuning of the program and expert knowledge of each target device. In this paper we consider a data parallel compiler transformation - thread-coarsening - and evaluate its effects across a range of devices by developing a source-to-source OpenCL compiler based on LLVM. We thoroughly evaluate this transformation on 17 benchmarks and five platforms with different coarsening parameters giving over 43,000 different experiments. We achieve speedups over 9x on individual applications and average speedups ranging from 1.15x on the Nvidia Kepler GPU to 1.50x on the AMD Cypress GPU. Finally, we use statistical regression to analyse and explain program performance in terms of hardware-based performance counters.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503268,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877444,GPU;OpenCL;Regression trees;Thread coarsening,Benchmark testing;Graphics processing units;Hardware;Instruction sets;Kernel;Multicore processing;Performance evaluation,,,,14,,28,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Scalable domain decomposition preconditioners for heterogeneous elliptic problems,P. Jolivet; F. Hecht; F. Nataf; C. Prud'homme,"Lab. J. Kuntzmannn, Univ. J. Fourier, Grenoble, France","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,11,"Domain decomposition methods are, alongside multigrid methods, one of the dominant paradigms in contemporary large-scale partial differential equation simulation. In this paper, a lightweight implementation of a theoretically and numerically scalable preconditioner is presented in the context of overlapping methods. The performance of this work is assessed by numerical simulations executed on thousands of cores, for solving various highly heterogeneous elliptic problems in both 2D and 3D with billions of degrees of freedom. Such problems arise in computational science and engineering, in solid and fluid mechanics. While focusing on overlapping domain decomposition methods might seem too restrictive, it will be shown how this work can be applied to a variety of other methods, such as non-overlapping methods and abstract deflation based preconditioners. It is also presented how multilevel preconditioners can be used to avoid communication during an iterative process such as a Krylov method.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503212,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877513,Linear solvers;divide and conquer;scalability,Abstracts;Convergence;Equations;Iterative methods;Mathematical model;Sparse matrices;Vectors,mathematics computing;parallel processing;partial differential equations,Krylov method;abstract deflation based preconditioners;computational science and engineering;domain decomposition methods;fluid mechanics;heterogeneous elliptic problems;large-scale partial differential equation simulation;multigrid methods;numerical simulations;numerically scalable preconditioner;scalable domain decomposition preconditioners;solid mechanics,,1,,33,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Scalable parallel OPTICS data clustering using graph algorithmic techniques,M. M. A. Patwary; D. Palsetia; A. Agrawal; W. k. Liao; F. Manne; A. Choudhary,"Northwestern Univ., Evanston, IL, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"OPTICS is a hierarchical density-based data clustering algorithm that discovers arbitrary-shaped clusters and eliminates noise using adjustable reachability distance thresholds. Parallelizing OPTICS is considered challenging as the algorithm exhibits a strongly sequential data access order. We present a scalable parallel OPTICS algorithm (POPTICS) designed using graph algorithmic concepts. To break the data access sequentiality, POPTICS exploits the similarities between the OPTICS algorithm and PRIM's Minimum Spanning Tree algorithm. Additionally, we use the disjoint-set data structure to achieve a high parallelism for distributed cluster extraction. Using high dimensional datasets containing up to a billion floating point numbers, we show scalable speedups of up to 27.5 for our OpenMP implementation on a 40-core shared-memory machine, and up to 3,008 for our MPI implementation on a 4,096-core distributed-memory machine. We also show that the quality of the results given by POPTICS is comparable to those given by the classical OPTICS algorithm.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503255,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877482,Density-based clustering;Disjoint-set data structure;Minimum spanning tree;Union-Find algorithm,Algorithm design and analysis;Clustering algorithms;Complexity theory;Data structures;Noise;Optics;Vegetation,data mining;data structures;parallel algorithms;pattern clustering;reachability analysis;trees (mathematics),MPI;OpenMP;POPTICS;PRIM minimum spanning tree algorithm;adjustable reachability distance thresholds;arbitrary-shaped clusters;data mining technique;disjoint-set data structure;distributed cluster extraction;distributed-memory machine;floating point numbers;graph algorithmic techniques;hierarchical density-based data clustering algorithm;high dimensional datasets;noise elimination;scalable parallel OPTICS algorithm;scalable parallel OPTICS data clustering;sequential data access order;shared-memory machine,,1,,56,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Distributed-memory parallel algorithms for generating massive scale-free networks using preferential attachment model,M. Alam; M. Khan; M. V. Marathe,"Dept. of Comput. Sci., Virginia Tech, Blacksburg, VA, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Recently, there has been substantial interest in the study of various random networks as mathematical models of complex systems. As these complex systems grow larger, the ability to generate progressively large random networks becomes all the more important. This motivates the need for efficient parallel algorithms for generating such networks. Naive parallelization of the sequential algorithms for generating random networks may not work due to the dependencies among the edges and the possibility of creating duplicate (parallel) edges. In this paper, we present MPI-based distributed memory parallel algorithms for generating random scale-free networks using the preferential-attachment model. Our algorithms scale very well to a large number of processors and provide almost linear speedups. The algorithms can generate scale-free networks with 50 billion edges in 123 seconds using 768 processors.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503291,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877524,Big Data;copy model;high performance computing;parallel algorithms;preferential attachment;random networks;scale-free networks,Abstracts;Algorithm design and analysis;Radio access networks,complex networks;distributed memory systems;parallel algorithms;random processes,MPI-based distributed memory parallel algorithms;Naive parallelization;complex systems;distributed-memory parallel algorithms;massive scale-free network generation;mathematical models;preferential attachment model;random scale-free networks;sequential algorithms,,3,,28,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
General transformations for GPU execution of tree traversals,M. Goldfarb; Y. Jo; M. Kulkarni,"Sch. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"With the advent of programmer-friendly GPU computing environments, there has been much interest in offloading workloads that can exploit the high degree of parallelism available on modern GPUs. Exploiting this parallelism and optimizing for the GPU memory hierarchy is well-understood for regular applications that operate on dense data structures such as arrays and matrices. However, there has been significantly less work in the area of irregular algorithms and even less so when pointer-based dynamic data structures are involved. Recently, irregular algorithms such as Barnes-Hut and kd-tree traversals have been implemented on GPUs, yielding significant performance gains over CPU implementations. However, the implementations often rely on exploiting application-specific semantics to get acceptable performance. We argue that there are general-purpose techniques for implementing irregular algorithms on GPUs that exploit similarities in algorithmic structure rather than application-specific knowledge. We demonstrate these techniques on several tree traversal algorithms, achieving speedups of up to 38ÌÑ over 32-thread CPU versions.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503223,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877443,GPU;irregular programs;tree traversals;vectorization,Abstracts;Educational institutions;Legged locomotion;Optimization;Propulsion;Semantics;Servers,multiprocessing systems;parallel algorithms;tree data structures,Barnes-Hut tree traversal algorithm;CPU versions;GPU execution;GPU memory hierarchy;algorithmic structure;application-specific semantics;general-purpose techniques;irregular algorithms;kd-tree traversal algorithm;offloading workloads;pointer-based dynamic data structures;programmer-friendly GPU computing environments,,5,,28,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Rethinking algorithm-based fault tolerance with a cooperative software-hardware approach,D. Li; Z. Chen; P. Wu; J. S. Vetter,"Oak Ridge Nat. Lab., Oak Ridge, TN, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Algorithm-based fault tolerance (ABFT) is a highly efficient resilience solution for many widely-used scientific computing kernels. However, in the context of the resilience ecosystem, ABFT is completely opaque to any underlying hardware resilience mechanisms. As a result, some data structures are over-protected by ABFT and hardware, which leads to redundant costs in terms of performance and energy. In this paper, we rethink ABFT using an integrated view including both software and hardware with the goal of improving performance and energy efficiency of ABFT-enabled applications. In particular, we study how to coordinate ABFT and error-correcting code (ECC) for main memory, and investigate the impact of this coordination on performance, energy, and resilience for ABFT-enabled applications. Scaling tests and analysis indicate that our approach saves up to 25% for system energy (and up to 40% for dynamic memory energy) with up to 18% performance improvement over traditional approaches of ABFT with ECC.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503226,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877477,adaptive resilience;algorithm-based fault tolerance;error-correcting code,Computer architecture;Error correction codes;Fault tolerance;Fault tolerant systems;Hardware;Registers;Resilience,data structures;error correction codes;fault tolerant computing;performance evaluation;power aware computing,ABFT-enabled applications;ECC;algorithm-based fault tolerance;cooperative software-hardware approach;data structures;energy efficiency;error-correcting code;hardware resilience mechanisms;resilience ecosystem;rethinking algorithm-based fault tolerance;widely-used scientific computing kernels,,4,,42,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
"A scalable, efficient scheme for evaluation of stencil computations over unstructured meshes",J. King; R. M. Kirby,"Sci. Comput. & Imaging Inst., Univ. of Utah, Salt Lake City, UT, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Stencil computations are a common class of operations that appear in many computational scientific and engineering applications. Stencil computations often benefit from compiletime analysis, exploiting data-locality, and parallelism. Post-processing of discontinuous Galerkin (dG) simulation solutions with B-spline kernels is an example of a numerical method which requires evaluating computationally intensive stencil operations over a mesh. Previous work on stencil computations has focused on structured meshes, while giving little attention to unstructured meshes. Performing stencil operations over an unstructured mesh requires sampling of heterogeneous elements which often leads to inefficient memory access patterns and limits data locality/reuse. In this paper, we present an efficient method for performing stencil computations over unstructured meshes which increases data-locality and cache efficiency, and a scalable approach for stencil tiling and concurrent execution. We provide experimental results in the context of post-processing of dG solutions that demonstrate the effectiveness of our approach.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503214,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877512,,Abstracts;Computational modeling;Educational institutions;Method of moments;Switches,Galerkin method;cache storage;concurrency control;mathematics computing;mesh generation;splines (mathematics),B-spline kernels;cache efficiency;compile time analysis;concurrent execution;dG solution post-processing;data-locality;discontinuous Galerkin simulation solutions;heterogeneous element sampling;numerical method;parallelism;stencil computation evaluation;stencil tiling;unstructured meshes,,0,,29,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Mr. Scan: Extreme scale density-based clustering using a tree-based network of GPGPU nodes,B. Welton; E. Samanas; B. P. Miller,"Comput. Sci. Dept., Univ. of Wisconsin, Madison, WI, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,11,"Density-based clustering algorithms are a widely-used class of data mining techniques that can find irregularly shaped clusters and cluster data without prior knowledge of the number of clusters it contains. DBSCAN is the most wellknown density-based clustering algorithm. We introduce our version of DBSCAN, called Mr. Scan, which uses a hybrid parallel implementation that combines the MRNet tree-based distribution network with GPGPU-equipped nodes. Mr. Scan avoids the problems of existing implementations by effectively partitioning the point space and by optimizing DBSCAN's computation over dense data regions. We tested Mr. Scan on both a geolocated Twitter dataset and image data obtained from the Sloan Digital Sky Survey. At its largest scale, Mr. Scan clustered 6.5 billion points from the Twitter dataset on 8,192 GPU nodes on Cray Titan in 17.3 minutes. All other parallel DBSCAN implementations have only demonstrated the ability to cluster up to 100 million points.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503262,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877517,,Algorithm design and analysis;Clustering algorithms;Distributed databases;Noise;Optimization;Partitioning algorithms;Spatial indexes,data mining;graphics processing units;parallel algorithms;pattern clustering;social networking (online);trees (mathematics),Cray Titan;DBSCAN;GPGPU-equipped nodes;MRNet tree-based distribution network;Sloan digital sky survey;data mining techniques;density-based clustering algorithms;extreme scale density-based clustering;geolocated Twitter dataset;hybrid parallel implementation;irregularly shaped clusters;mr scan;tree-based network,,6,,29,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Design and performance evaluation of NUMA-aware RDMA-based end-to-end data transfer systems,Y. Ren; T. Li; D. Yu; S. Jin; T. Robertazzi,"Stony Brook Univ., Stony Brook, NY, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,10,"Data-intensive applications place stringent requirements on the performance of both back-end storage systems and frontend network interfaces. However, for ultra high-speed data transfer, for example, at 100 Gbps and higher, the effects of multiple bottlenecks along a full end-to-end path, have not been resolved efficiently. In this paper, we describe our implementation of an end-to-end data transfer software at such high-speeds. At the back-end, we construct a storage area network with the iSCSI protocols, and utilize efficient RDMA technology. At the front-end, we design network communication software to transfer data in parallel, and utilize NUMA techniques to maximize the performance of multiple network interfaces. We demonstrate that our system can deliver the full 100 Gbps end-to-end data transfer throughput. The software product is tested rigorously and demonstrated applicable to supporting various data-intensive applications that constantly move bulk data within and across data centers.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503260,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877481,Multi-Core Architecture;Network Protocols;Non-Uniform Memory Access;Remote Direct Memory Access;Storage Area Network,Abstracts;Artificial intelligence;IP networks;Laboratories;Wide area networks,computer network performance evaluation;data communication;network interfaces;protocols;storage area networks,NUMA techniques;NUMA-aware RDMA-based end-to-end data transfer systems;RDMA technology;back-end storage systems;data centers;data-intensive applications;end-to-end data transfer software;front-end network interfaces;iSCSI protocols;network interfaces;performance evaluation;software product;storage area network;ultra high-speed data transfer,,4,,29,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
An improved parallel singular value algorithm and its implementation for multicore hardware,A. Haidar; J. Kurzak; P. Luszczek,"Electr. Eng. & Comput. Sci., Univ. of Tennessee, Knoxville, TN, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"The enormous gap between the high-performance capabilities of today's CPUs and off-chip communication poses extreme challenges to the development of numerical software that is scalable and achieves high performance. In this article, we describe a successful methodology to address these challenges-starting with our algorithm design, through kernel optimization and tuning, and finishing with our programming model. All these lead to development of a scalable high-performance Singular Value Decomposition (SVD) solver. We developed a set of highly optimized kernels and combined them with advanced optimization techniques that feature fine-grain and cache-contained kernels, a task based approach, and hybrid execution and scheduling runtime, all of which significantly increase the performance of our SVD solver. Our results demonstrate a many-fold performance increase compared to currently available software. In particular, our software is two times faster than Intel's Math Kernel Library (MKL), a highly optimized implementation from the hardware vendor, when all the singular vectors are requested; it achieves a 5-fold speed-up when only 20% of the vectors are computed; and it is up to 10 times faster if only the singular values are required.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503292,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877523,Performance;Reduction to bidiagonal;Singular Value Decomposition;eigenvalues and eigenvectors;task parallelism,Eigenvalues and eigenfunctions;Heuristic algorithms;Kernel;Layout;Processor scheduling;Symmetric matrices;Vectors,multiprocessing systems;operating system kernels;parallel algorithms;singular value decomposition,MKL;Math Kernel Library;SVD solver;advanced optimization techniques;cache-contained kernels;fine-grain kernels;hardware vendor;kernel optimization;kernel tuning;multicore hardware;parallel singular value algorithm;scalable high-performance singular value decomposition solver;singular vectors,,3,,63,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Performance evaluation of Intel<sup>å¨</sup> Transactional Synchronization Extensions for high-performance computing,R. M. Yoo; C. J. Hughes; K. Lai; R. Rajwar,"Parallel Comput. Lab., Intel Labs., Santa Clara, CA, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,11,"Intel has recently introduced Intel<sup>å¨</sup> Transactional Synchronization Extensions (Intel<sup>å¨</sup> TSX) in the Intel 4th Generation Core‰ã¢ Processors. With Intel TSX, a processor can dynamically determine whether threads need to serialize through lock-protected critical sections. In this paper, we evaluate the first hardware implementation of Intel TSX using a set of high-performance computing (HPC) workloads, and demonstrate that applying Intel TSX to these workloads can provide significant performance improvements. On a set of real-world HPC workloads, applying Intel TSX provides an average speedup of 1.41x. When applied to a parallel user-level TCP/IP stack, Intel TSX provides 1.31x average bandwidth improvement on network intensive applications. We also demonstrate the ease with which we were able to apply Intel TSX to the various workloads.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503232,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877452,High-Performance Computing;Transactional Memory,Benchmark testing;Hardware;Instruction sets;Libraries;Synchronization,microprocessor chips;parallel processing;performance evaluation;synchronisation,Intel 4th Generation Core Processors;Intel TSX;Intel transactional synchronization extension;high-performance computing;high-performance computing workloads;lock-protected critical sections;parallel user-level TCP/IP stack;performance evaluation;real-world HPC workloads,,17,,30,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Using simulation to explore distributed key-value stores for extreme-scale system services,Ke Wang; A. Kulkarni; M. Lang; D. Arnold; I. Raicu,"Illinois Inst. of Technol., Chicago, IL, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Owing to the significant high rate of component failures at extreme scales, system services will need to be failure-resistant, adaptive and self-healing. A majority of HPC services are still designed around a centralized paradigm and hence are susceptible to scaling issues. Peer-to-peer services have proved themselves at scale for wide-area internet workloads. Distributed key-value stores (KVS) are widely used as a building block for these services, but are not prevalent in HPC services. In this paper, we simulate KVS for various service architectures and examine the design trade-offs as applied to HPC service workloads to support extreme-scale systems. The simulator is validated against existing distributed KVS-based services. Via simulation, we demonstrate how failure, replication, and consistency models affect performance at scale. Finally, we emphasize the general use of KVS to HPC services by feeding real HPC service workloads into the simulator and presenting a KVS-based distributed job launch prototype.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503239,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877442,Discrete Event Simulation;Extreme Scales;Key-Value Store;System Services,Abstracts;Data models;Fingers;Laboratories;Scalability;Servers;Taxonomy,Internet;discrete event simulation;fault tolerant computing;parallel processing;peer-to-peer computing;system recovery;wide area networks,HPC service workloads;KVS-based distributed job launch prototype;adaptive services;centralized paradigm;component failures;consistency models;distributed KVS-based services;distributed key-value stores;extreme-scale system services;extreme-scale systems;failure models;failure-resistant services;peer-to-peer services;replication models;self-healing services;service architectures;wide-area Internet workloads,,8,,33,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Effective sampling-driven performance tools for GPU-accelerated supercomputers,M. Chabbi; K. Murthy; M. Fagan; J. Mellor-Crummey,"Dept. of Comput. Sci., Rice Univ. Houston, Houston, TX, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Performance analysis of GPU-accelerated systems requires a system-wide view that considers both CPU and GPU components. In this paper, we describe how to extend system-wide, sampling-based performance analysis methods to GPU-accelerated systems. Since current GPUs do not support sampling, our implementation required careful coordination of instrumentation-based performance data collection on GPUs with sampling-based methods employed on CPUs. In addition, we also introduce a novel technique for analyzing systemic idleness in CPU/GPU systems. We demonstrate the effectiveness of our techniques with application case studies on Titan and Keeneland. Some of the highlights of our case studies are: 1) we improved performance for LULESH 1.0 by 30%, 2) we identified a hardware performance problem on Keeneland, 3) we identified a scaling problem in LAMMPS derived from CUDA initialization, and 4) we identified a performance problem that is caused by GPU synchronization operations that suffer delays due to blocking system calls.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503299,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877476,CPU-GPU blame shifting;Call path profiling;Heterogeneous architectures;Performance analysis,Context;Graphics processing units;Instruments;Kernel;Measurement;Radiation detectors;Tuning,graphics processing units;parallel architectures;parallel machines;performance evaluation;sampling methods;synchronisation,CPU-GPU systems;CUDA initialization;GPU synchronization operations;GPU-accelerated supercomputers;GPU-accelerated systems;Keeneland;LAMMPS;LULESH 1.0;Titan;blocking system calls;hardware performance problem;instrumentation-based performance data collection;sampling-driven performance tools;system-wide sampling-based performance analysis methods,,4,,26,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
GoldRush: Resource efficient in situ scientific data analytics using fine-grained interference aware execution,F. Zheng; H. Yu; C. Hantas; M. Wolf; G. Eisenhauer; K. Schwan; H. Abbasi; S. Klasky,"Georgia Inst. of Technol., Atlanta, GA, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Severe I/O bottlenecks on High End Computing platforms call for running data analytics in situ. Demonstrating that there exist considerable resources in compute nodes un-used by typical high end scientific simulations, we leverage this fact by creating an agile runtime, termed GoldRush, that can harvest those otherwise wasted, idle resources to efficiently run in situ data analytics. GoldRush uses fine-grained scheduling to ‰ÛÏsteal‰Û idle resources, in ways that minimize interference between the simulation and in situ analytics. This involves recognizing the potential causes of on-node resource contention and then using scheduling methods that prevent them. Experiments with representative science applications at large scales show that resources harvested on compute nodes can be leveraged to perform useful analytics, significantly improving resource efficiency, reducing data movement costs incurred by alternate solutions, and posing negligible impact on scientific simulations.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503279,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877511,Design;Measurement;Performance,Abstracts;Context;Instruction sets;Lead;Programmable logic arrays;Runtime;Switches,Big Data;data analysis;scheduling,Big Data;GoldRush system;I/O bottlenecks;data movement cost reduction;fine-grained interference aware execution;fine-grained scheduling method;high end computing platforms;high end scientific simulations;idle resources;on-node resource contention;resource efficient in situ scientific data analytics,,9,,48,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Swendsen-Wang multi-cluster algorithm for the 2D/3D Ising model on Xeon Phi and GPU,F. Wende; T. Steinke,"Zuse Inst. Berlin, Berlin-Dahlem, Germany","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Simulations of the critical Ising model by means of local update algorithms suffer from critical slowing down. One way to partially compensate for the influence of this phenomenon on the runtime of simulations is using increasingly faster and parallel computer hardware. Another approach is using algorithms that do not suffer from critical slowing down, such as cluster algorithms. This paper reports on the Swendsen-Wang multi-cluster algorithm on Intel Xeon Phi coprocessor 5110P, Nvidia Tesla M2090 GPU, and x86 multi-core CPU.We present shared memory versions of the said algorithm for the simulation of the two- and three-dimensional Ising model. We use a combination of local cluster search and global label reduction by means of atomic hardware primitives. Further, we describe an MPI version of the algorithm on Xeon Phi and CPU, respectively. Significant performance improvements over known implementations of the Swendsen-Wang algorithm are demonstrated.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503254,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877516,CUDA;GPGPU;Ising model;Many-core processors;Swendsen-Wang multi-cluster algorithm;Xeon Phi;graph algorithms;performance evaluation,Clustering algorithms;Computational modeling;Computers;Graphics processing units;Instruction sets;Lattices;Solid modeling,Ising model;application program interfaces;graphics processing units;message passing;parallel algorithms;shared memory systems,2D-3D Ising model;GPU;Intel Xeon Phi coprocessor 5110P;MPI version;Nvidia Tesla M2090 GPU;Swendsen-Wang multicluster algorithm;atomic hardware primitives;global label reduction;local cluster search;local update algorithms;parallel computer hardware;shared memory versions;three-dimensional Ising model;two-dimensional Ising model;x86 multicore CPU,,2,,28,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
SDQuery DSI: Integrating data management support with a wide area data transfer protocol,Yu Su; Yi Wang; G. Agrawal; R. Kettimuthu,"Comput. Sci. & Eng, Ohio State Univ., Columbus, OH, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"In many science areas where datasets need to be transferred or shared, rapid growth in dataset size, coupled with much slower increases in wide area data transfer bandwidths, is making it extremely hard for scientists to analyze the data. This paper addresses the current limitations by developing SDQuery DSI, a GridFTP plug-in that supports flexible server-side data subsetting. An existing GridFTP server is able to dynamically load this tool to support new functionality. Different queries types (query over dimensions, coordinates and values) are supported by our tool. A number of optimizations, like parallel indexing, performance model for data subsetting, and parallel streaming are also applied. We compare our SDQuery DSI with GridFTP default File DSI in different network environments, and show that our method can achieve better efficiency in almost all cases.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877480,I/O performance tuning;data management;indexing;query processing;wide area networks,Abstracts;Filtering;Indexing;Photonics;Ports (Computers);Protocols;Security,database indexing;grid computing;query processing;wide area networks,GridFTP server;SDQuery DSI;data management support;data subsetting;flexible server-side data subsetting;parallel indexing;parallel streaming;performance model;wide area data transfer protocol,,1,,33,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Detection of false sharing using machine learning,S. Jayasena; S. Amarasinghe; A. Abeyweera; G. Amarasinghe; H. De Silva; S. Rathnayake; Xiaoqiao Meng; Yanbin Liu,"Dept. of Comput. Sci. & Eng., Univ. of Moratuwa, Moratuwa, Sri Lanka","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,9,"False sharing is a major class of performance bugs in parallel applications. Detecting false sharing is difficult as it does not change the program semantics. We introduce an efficient and effective approach for detecting false sharing based on machine learning. We develop a set of mini-programs in which false sharing can be turned on and off. We then run the mini-programs both with and without false sharing, collect a set of hardware performance event counts and use the collected data to train a classifier. We can use the trained classifier to analyze data from arbitrary programs for detection of false sharing. Experiments with the PARSEC and Phoenix benchmarks show that our approach is indeed effective. We detect published false sharing regions in the benchmarks with zero false positives. Our performance penalty is less than 2%. Thus, we believe that this is an effective and practical method for detecting false sharing.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503269,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877463,False Sharing;Machine Learning;Performance Events,Accuracy;Arrays;Hardware;Instruction sets;Training;Training data;Vectors,learning (artificial intelligence);parallel programming;program debugging,PARSEC;Phoenix benchmarks;data analysis;false sharing detection;hardware performance event counts;machine learning;miniprograms;performance bugs;program semantics;trained classifier,,4,,33,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Petascale WRF simulation of hurricane sandy: Deployment of NCSA's cray XE6 blue waters,P. Johnsen; M. Straka; M. Shapiro; A. Norton; T. Galarneau,"Performance Eng. Group, Cray, Inc., St. Paul, MN, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,7,"The National Center for Atmospheric Research (NCAR) Weather Research and Forecasting (WRF) model has been employed on the largest yet storm prediction model using real data of over 4 billion points to simulate the landfall of Hurricane Sandy. Using an unprecedented 13,680 nodes (437,760 cores) of the Cray XE6 ‰ÛÏBlue Waters‰Û at NCSA at the University of Illinois, researchers achieved a sustained rate of 285 Tflops while simulating an 18-hour forecast. A grid of size 9120ÌÑ9216ÌÑ48 (1.4Tbytes of input) was used, with horizontal resolution of 500 meters and a 2-second time step. 86 Gbytes of forecast data was written every 6 forecast hours at a rate of up to 2 Gbytes/second and collaboratively post-processed and displayed using the Vapor suite at NCAR. Opportunities to enhance scalability in the source code, run-time, and operating system realms were exploited. The output of this numerical model is now under study for model validation.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503231,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877496,Cray;High Performance Computing;NCAR;NCSA;WRF;forecast;hurricane;prediction;simulation;storm;weather,Atmospheric modeling;Computational modeling;Computer architecture;Manganese;Reliability;SDRAM;Weather forecasting,geophysics computing;operating systems (computers);parallel machines;source code (software);storms;weather forecasting,NCSA cray XE6 blue waters;National Center for Atmospheric Research Weather Research and Forecasting model;Petascale WRF simulation;forecast data;hurricane sandy;numerical model;operating system;source code;storm prediction model;time 18 hour;vapor suite,,2,,14,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
A computationally efficient algorithm for the 2D covariance method,O. Green; Y. Birk,"Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"The estimated covariance matrix is a building block for many algorithms, including signal and image processing. The Covariance Method is an estimator for the covariance matrix, favored both as an estimator and in view of the convenient properties of the matrix that it produces. However, the considerable computational requirements limit its use. We present a novel computation algorithm for the covariance method, which dramatically reduces the computational complexity (both ALU operations and memory access) relative to previous algorithms. It has a small memory footprint, is highly parallelizable and requires no synchronization among compute threads. On a 40-core X86 system, we achieve 1200X speedup relative to a straightforward single-core implementation; even on a single core, 35X speedup is achieved.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503218,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877522,Covariance Method;Estimation;Inclusion-Exclusion Principle;Parallel algorithms,Covariance matrices;Image processing;Indexes;Memory management;Partitioning algorithms;Synchronization;Synthetic aperture radar,computational complexity;covariance matrices;parallel algorithms,2D covariance method;40-core X86 system;computational complexity;computationally efficient algorithm;estimated covariance matrix;image processing;signal processing;small memory footprint,,0,,19,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Hybrid MPI: Efficient message passing for multi-core systems,A. Friedley; G. Bronevetsky; T. Hoefler; A. Lumsdaine,"Indiana Univ., Bloomington, IN, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,11,"Multi-core shared memory architectures are ubiquitous in both High-Performance Computing (HPC) and commodity systems because they provide an excellent trade-off between performance and programmability. MPI's abstraction of explicit communication across distributed memory is very popular for programming scientific applications. Unfortunately, OS-level process separations force MPI to perform unnecessary copying of messages within shared memory nodes. This paper presents a novel approach that transparently shares memory across MPI processes executing on the same node, allowing them to communicate like threaded applications. While prior work explored thread-based MPI libraries, we demonstrate that this approach is impractical and performs poorly in practice. We instead propose a novel process-based approach that enables shared memory communication and integrates with existing MPI libraries and applications without modifications. Our protocols for shared memory message passing exhibit better performance and reduced cache footprint. Communication speedups of more than 26% are demonstrated for two applications.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503294,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877451,,Libraries;Memory management;Message passing;Message systems;Protocols;Random access memory;Receivers,application program interfaces;distributed shared memory systems;memory architecture;message passing;multiprocessing systems;parallel processing,HPC;OS-level process separations;commodity systems;distributed memory;high-performance computing;hybrid MPI process;multicore shared memory architectures;multicore systems;process-based approach;reduced cache footprint;shared memory communication;shared memory message passing;shared memory nodes;thread-based MPI libraries,,4,,28,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
SPBC: Leveraging the characteristics of MPI HPC applications for scalable checkpointing,T. Ropars; T. V. Martsinkevich; A. Guermouche; A. Schiper; F. Cappello,"Ecole Polytech. Fed. de Lausanne (EPFL), Lausanne, Switzerland","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"The high failure rate expected for future supercomputers requires the design of new fault tolerant solutions. Most checkpointing protocols are designed to work with any message-passing application but sudder from scalability issues at extreme scale. We take a different approach: We identify a property common to many HPC applications, namely channel-determinism, and introduce a new partial order relation, called always-happens-before relation, between events of such applications. Leveraging these two concepts, we design a protocol that combines an unprecedented set of features. Our protocol called SPBC combines in a hierarchical way coordinated checkpointing and message logging. It is the first protocol that provides failure containment without logging any information reliably apart from process checkpoints, and this, without penalizing recovery performance. Experiments run with a representative set of HPC workloads demonstrate a good performance of our protocol during both, failure-free execution and recovery.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503271,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877441,Algorithms;Reliability,Checkpointing;Fault tolerance;Fault tolerant systems;Libraries;Payloads;Protocols,application program interfaces;checkpointing;failure analysis;message passing;parallel processing;protocols;software fault tolerance;software reliability,HPC workloads;MPI HPC;SPBC protocol;always-happens-before relation;channel-determinism;checkpointing protocols;failure containment;fault tolerant solutions;hierarchical way coordinated checkpointing;high failure rate;message logging;message-passing;partial order relation;scalable checkpointing;supercomputers,,5,,31,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Precimonious: Tuning assistant for floating-point precision,C. Rubio-GonzÌÁlez; Cuong Nguyen; Hong Diep Nguyen; J. Demmel; W. Kahan; K. Sen; D. H. Bailey; C. Iancu; D. Hough,"EECS Dept., UC, Berkeley, CA, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Given the variety of numerical errors that can occur, floating-point programs are difficult to write, test and debug. One common practice employed by developers without an advanced background in numerical analysis is using the highest available precision. While more robust, this can degrade program performance significantly. In this paper we present Precimonious, a dynamic program analysis tool to assist developers in tuning the precision of floating-point programs. Precimonious performs a search on the types of the floating-point program variables trying to lower their precision subject to accuracy constraints and performance goals. Our tool recommends a type instantiation that uses lower precision while producing an accurate enough answer without causing exceptions. We evaluate Precimonious on several widely used functions from the GNU Scientific Library, two NAS Parallel Benchmarks, and three other numerical programs. For most of the programs analyzed, Precimonious reduces precision, which results in performance improvements as high as 41%.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503296,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877460,delta-debugging algorithm;dynamic program analysis;floating-point arithmetic;mixed precision;program optimization,Accuracy;Algorithm design and analysis;Heuristic algorithms;Numerical analysis;Partitioning algorithms;Performance analysis;Tuning,floating point arithmetic;mathematics computing;numerical analysis;program debugging;program diagnostics;program testing,GNU Scientific Library;NAS parallel benchmarks;PRECIMONIOUS;dynamic program analysis tool;floating-point program precision tuning;floating-point program variables;numerical analysis;numerical errors;numerical programs,,9,,29,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Tera-scale 1D FFT with low-communication algorithm and Intel<sup>å¨</sup> Xeon Phi‰ã¢ coprocessors,Jongsoo Park; G. Bikshandi; K. Vaidyanathan; P. T. P. Tang; P. Dubey; Daehyun Kim,"Parallel Comput. Lab., USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"This paper demonstrates the first tera-scale performance of Intel<sup>å¨</sup> Xeon Phi‰ã¢ coprocessors on 1D FFT computations. Applying a disciplined performance programming methodology of sound algorithm choice, valid performance model, and well-executed optimizations, we break the tera-flop mark on a mere 64 nodes of Xeon Phi and reach 6.7 TFLOPS with 512 nodes, which is 1.5ÌÑ than achievable on a same number of Intel<sup>å¨</sup> Xeon<sup>å¨</sup> nodes. It is a challenge to fully utilize the compute capability presented by many-core wide-vector processors for bandwidth-bound FFT computation. We leverage a new algorithm, Segment-of-Interest FFT, with low inter-node communication cost, and aggressively optimize data movements in node-local computations, exploiting caches. Our coordination of low communication algorithm and massively parallel architecture for scalable performance is not limited to running FFT on Xeon Phi; it can serve as a reference for other bandwidth-bound computations and for emerging HPC systems that are increasingly communication limited.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503242,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877467,Bandwidth Optimizations;Communication-Avoiding Algorithms;FFT;Wide-Vector Many-Core Processors;Xeon Phi,Abstracts;Demodulation;Optimization;Program processors,coprocessors;fast Fourier transforms;multiprocessing systems;parallel architectures,HPC systems;Intel Xeon Phi coprocessors;TFLOPS;bandwidth-bound FFT computation;data movement optimization;disciplined performance programming methodology;low communication algorithm;low inter-node communication cost;low-communication algorithm;many-core wide-vector processors;node-local computations;parallel architecture;segment-of-interest FFT;tera-scale 1D FFT;tera-scale performance,,5,,33,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Integrating dynamic pricing of electricity into energy aware scheduling for HPC systems,X. Yang; Z. Zhou; S. Wallace; Z. Lan; W. Tang; S. Coghlan; M. E. Papka,"Illinois Inst. of Technol., Chicago, IL, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,11,The research literature to date mainly aimed at reducing energy consumption in HPC environments. In this paper we propose a job power aware scheduling mechanism to reduce HPC's electricity bill without degrading the system utilization. The novelty of our job scheduling mechanism is its ability to take the variation of electricity price into consideration as a means to make better decisions of the timing of scheduling jobs with diverse power profiles. We verified the effectiveness of our design by conducting trace-based experiments on an IBM Blue Gene/P and a cluster system as well as a case study on Argonne's 48-rack IBM Blue Gene/Q system. Our preliminary results show that our power aware algorithm can reduce electricity bill of HPC systems as much as 23%.,2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503264,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877493,Electricity Bill;Job Scheduling;Power;System Utilization,Electricity;Hardware;Power demand;Pricing;Production;Resource management;Supercomputers,energy consumption;parallel processing;power aware computing;pricing;scheduling,Argonne'48-rack IBM Blue Gene/Q system;HPC electricity bill reduction;HPC systems;IBM Blue Gene/P;cluster system;electricity dynamic pricing;energy aware scheduling;energy consumption reduction;job power aware scheduling mechanism,,11,,30,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
CooMR: Cross-task coordination for efficient data management in MapReduce programs,Xiaobing Li; Yandong Wang; Yizheng Jiao; Cong Xu; Weikuan Yu,"Dept. of Comput. Sci. & Software Eng., Auburn Univ., Auburn, AL, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,11,"Hadoop is a widely adopted open source implementation of MapReduce programming model for big data processing. It represents system resources as available map and reduce slots and assigns them to various tasks. This execution model gives little regard to the need of cross-task coordination on the use of shared system resources on a compute node, which results in task interference. In addition, the existing Hadoop merge algorithm can cause excessive I/O. In this study, we undertake an effort to address both issues. Accordingly, we have designed a cross-task coordination framework called CooMR for efficient data management in MapReduce programs. CooMR consists of three component schemes including cross-task opportunistic memory sharing and log-structured I/O consolidation, which are designed to facilitate task coordination, and the key-based in-situ merge (KISM) algorithm which is designed to enable the sorting/merging of Hadoop intermediate data without actually moving the <;key, value> pairs. Our evaluation demonstrates that CooMR is able to increase task coordination, improve system resource utilization, and significantly speed up the execution time of MapReduce programs.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877475,,Abstracts;Arrays;Software engineering,Big Data;data handling;distributed programming;public domain software;sorting,Big Data processing;CooMR;Hadoop intermediate data;Hadoop merge algorithm;KISM algorithm;MapReduce programming model;compute node;cross-task coordination framework;cross-task opportunistic memory sharing;data management;excessive I/O;key-based in-situ merge algorithm;log-structured I/O consolidation;shared system resources;system resource utilization;task interference,,0,,24,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Exploring power behaviors and trade-offs of in-situ data analytics,M. Gamell; I. Rodero; M. Parashar; J. C. Bennett; H. Kolla; J. Chen; P. T. Bremer; A. G. Landge; A. Gyulassy; P. McCormick; S. Pakin; V. Pascucci; S. Klasky,"Rutgers University, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"As scientific applications target exascale, challenges related to data and energy are becoming dominating concerns. For example, coupled simulation workflows are increasingly adopting in-situ data processing and analysis techniques to address costs and overheads due to data movement and I/O. However it is also critical to understand these overheads and associated trade-offs from an energy perspective. The goal of this paper is exploring data-related energy/performance trade-offs for end-to-end simulation workflows running at scale on current high-end computing systems. Specifically, this paper presents: (1) an analysis of the data-related behaviors of a combustion simulation workflow with an insitu data analytics pipeline, running on the Titan system at ORNL; (2) a power model based on system power and data exchange patterns, which is empirically validated; and (3) the use of the model to characterize the energy behavior of the workflow and to explore energy/performance tradeoffs on current as well as emerging systems.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503303,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877510,,Analytical models;Blades;Computational modeling;Computer architecture;Data models;Mathematical model;Radiation detectors,combustion;data analysis;physics computing,ORNL;Titan system;combustion simulation workflow;data analytics pipeline;data exchange patterns;data-related behaviors;power model,,9,,74,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
A framework for load balancing of Tensor Contraction expressions via dynamic task partitioning,Pai-Wei Lai; K. Stock; S. Rajbhandari; S. Krishnamoorthy; P. Sadayappan,"Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,10,"In this paper, we introduce the Dynamic Load-balanced Tensor Contractions (DLTC), a domain-specific library for efficient task parallel execution of tensor contraction expressions, a class of computation encountered in quantum chemistry and physics. Our framework decomposes each contraction into smaller unit of tasks, represented by an abstraction referred to as iterators. We exploit an extra level of parallelism by having tasks across independent contractions executed concurrently through a dynamic load balancing runtime. We demonstrate the improved performance, scalability, and flexibility for the computation of tensor contraction expressions on parallel computers using examples from Coupled Cluster (CC) methods.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503290,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877446,Tensor contraction;domain-specific language;dynamic load balancing;task scheduling library,Computational modeling;Indexes;Libraries;Load management;Memory management;Program processors;Tensile stress,parallel processing;quantum chemistry;resource allocation;tensors,CC methods;DLTC;coupled cluster methods;domain-specific library;dynamic load balancing runtime;dynamic load-balanced tensor contractions;dynamic task partitioning;load balancing;parallel computers;quantum chemistry;task parallel execution;tensor contraction expression computation;tensor contraction expressions,,3,,24,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Kinetic turbulence simulations at extreme scale on leadership-class systems,B. Wang; S. Ethier; W. Tang; T. Williams; K. Z. Ibrahim; K. Madduri; S. Williams; L. Oliker,"Princeton Inst. of Comput. Sci. & Eng., Princeton Univ., Princeton, NJ, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Reliable predictive simulation capability addressing confinement properties in magnetically confined fusion plasmas is critically-important for ITER, a 20 billion dollar international burning plasma device under construction in France. The complex study of kinetic turbulence, which can severely limit the energy confinement and impact the economic viability of fusion systems, requires simulations at extreme scale for such an unprecedented device size. Our newly optimized, global, ab initio particle-in-cell code solving the nonlinear equations underlying gyrokinetic theory achieves excellent performance with respect to ‰ÛÏtime to solution‰Û at the full capacity of the IBM Blue Gene/Q on 786,432 cores of Mira at ALCF and recently of the 1,572,864 cores of Sequoia at LLNL. Recent multithreading and domain decomposition optimizations in the new GTC-P code represent critically important software advances for modern, low memory per core systems by enabling routine simulations at unprecedented size (130 million grid points ITER-scale) and resolution (65 billion particles).",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503258,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877515,,Abstracts;Bandwidth;Clocks;Lead;Magnetic confinement;Magnetomechanical effects;Prefetching,Tokamak devices;ab initio calculations;multi-threading;nonlinear equations;physics computing;plasma kinetic theory;plasma nonlinear processes;plasma simulation;plasma toroidal confinement;plasma turbulence,ALCF;GTC-P code;IBM Blue Gene/Q;ITER;LLNL;Mira;Sequoia;ab initio particle-in-cell code;burning plasma device;confinement properties;domain decomposition optimizations;energy confinement;fusion systems;gyrokinetic theory;kinetic turbulence simulations;leadership-class systems;magnetically confined fusion plasmas;multithreading;nonlinear equations;predictive simulation capability;time to solution,,3,,25,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Efficient data partitioning model for heterogeneous graphs in the cloud,Kisung Lee; L. Liu,"Georgia Inst. of Technol., Atlanta, GA, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"As the size and variety of information networks continue to grow in many scientific and engineering domains, we witness a growing demand for efficient processing of large heterogeneous graphs using a cluster of compute nodes in the Cloud. One open issue is how to effectively partition a large graph to process complex graph operations efficiently. In this paper, we present VB-Partitioner - a distributed data partitioning model and algorithms for efficient processing of graph operations over large-scale graphs in the Cloud. Our VB-Partitioner has three salient features. First, it introduces vertex blocks (VBs) and extended vertex blocks (EVBs) as the building blocks for semantic partitioning of large graphs. Second, VB-Partitioner utilizes vertex block grouping algorithms to place those vertex blocks that have high correlation in graph structure into the same partition. Third, VB-Partitioner employs a VB-partition guided query partitioning model to speed up the parallel processing of graph pattern queries by reducing the amount of inter-partition query processing. We conduct extensive experiments on several real-world graphs with millions of vertices and billions of edges. Our results show that VB-Partitioner significantly outperforms the popular random block-based data partitioner in terms of query latency and scalability over large-scale graphs.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503302,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877479,big data processing;cloud computing;heterogeneous graph;partitioning,Abstracts;Engines;Pipelines,Big Data;cloud computing;graph theory;parallel processing;query processing,EVB;VB-partition guided query partitioning model;VB-partitioner;complex graph operations;compute node cluster;data partitioning model;distributed data partitioning model;extended vertex blocks;graph operations;graph pattern queries;graph structure;heterogeneous graphs;information networks;interpartition query processing;large-scale graphs;parallel processing;query latency;query scalability;random block-based data partitioner;vertex block grouping algorithms,,7,,21,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Enabling comprehensive data-driven system management for large computational facilities,J. C. Browne; R. L. DeLeon; C. D. Lu; M. D. Jones; S. M. Gallo; A. Ghadersohi; A. K. Patra; W. L. Barth; J. Hammond; T. R. Furlani; R. T. McLay,"Center for Comput. Res., SUNY at Buffalo, Buffalo, NY, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,11,"This paper presents a tool chain, based on the open source tool TACC_Stats, for systematic and comprehensive job level resource use measurement for large cluster computers, and its incorporation into XDMoD, a reporting and analytics framework for resource management that targets meeting the information needs of users, application developers, systems administrators, systems management and funding managers. Accounting, scheduler and event logs are integrated with system performance data from TACC_Stats. TACC_Stats periodically records resource use including many hardware counters for each job running on each node. Furthermore, system level metrics are obtained through aggregation of the node (job) level data. Analysis of this data generates many types of standard and custom reports and even a limited predictive capability that has not previously been available for open-source, Linux-based software systems. This paper presents case studies of information that can be applied for effective resource management. We believe this system to be the first fully comprehensive system for supporting the information needs of all stakeholders in open-source software based HPC systems.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503230,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877519,,Abstracts;Bandwidth;Market research;Performance evaluation;Servers;Sockets;Standards,Linux;public domain software;resource allocation;workstation clusters,Linux-based software systems;TACC_Stats;XDMoD;cluster computers;computational facilities;data-driven system management;funding managers;job level resource;open source tool;open-source software based HPC systems;resource management;system administrators;system level metrics;system management;systematic resource;tool chain,,5,,28,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
On the usefulness of object tracking techniques in performance analysis,G. Llort; H. Servat; J. GonzÌÁlez; J. GimÌ©nez; J. Labarta,"Barcelona Supercomput. Center, Univ. Politec. de Catalunya, Barcelona, Spain","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,11,"Understanding the behavior of a parallel application is crucial if we are to tune it to achieve its maximum performance. Yet the behavior the application exhibits may change over time and depend on the actual execution scenario: particular inputs and program settings, the number of processes used, or hardware-specific problems. So beyond the details of a single experiment a far more interesting question arises: how does the application behavior respond to changes in the execution conditions? In this paper, we demonstrate that object tracking concepts from computer vision have huge potential to be applied in the context of performance analysis. We leverage tracking techniques to analyze how the behavior of a parallel application evolves through multiple scenarios where the execution conditions change. This method provides comprehensible insights on the influence of different parameters on the application behavior, enabling us to identify the most relevant code regions and their performance trends.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503267,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877462,Performance analysis;clustering;object tracking;tracing,Clustering algorithms;Market research;Object recognition;Object tracking;Performance analysis,computer vision;object tracking;parallel processing,code regions;computer vision;hardware-specific problems;object tracking techniques;parallel application behavior;performance analysis,,3,,29,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Solving the compressible Navier-Stokes equations on up to 1.97 million cores and 4.1 trillion grid points,I. Bermejo-Moreno; J. Bodart; J. Larsson; B. M. Barney; J. W. Nichols; S. Jones,"Center for Turbulence Res., Stanford Univ., Stanford, CA, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,10,"We present weak and strong scaling studies as well as performance analyses of the Hybrid code, a finite-difference solver of the compressible Navier-Stokes equations on structured grids used for the direct numerical simulation of isotropic turbulence and its interaction with shock waves. Parallelization is achieved through MPI, emphasizing the use of nonblocking communication with concurrent computation. The simulations, scaling and performance studies were done on the Sequoia, Vulcan and Vesta Blue Gene/Q systems, the first two accounting for a total of 1,966,080 cores when used in combination. The maximum number of grid points simulated was 4.12 trillion, with a memory usage of approximately 1.6 PB. We discuss the use of hyperthreading, which significantly improves the parallel performance of the code on this architecture.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503265,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877495,Compressible turbulence;Direct Numerical Simulation;Navier-Stokes;high-performance computing;shock waves,Abstracts;Electric shock;Government;Laboratories;Quantum cascade lasers,Navier-Stokes equations;application program interfaces;compressible flow;finite difference methods;flow simulation;message passing;multiprocessing systems;parallel processing;physics computing;shock waves;turbulence,MPI;Sequoia Blue Gene/Q systems;Vesta Blue Gene/Q systems;Vulcan Blue Gene/Q systems;compressible Navier-Stokes equations;direct numerical simulation;finite-difference solver;grid points;hybrid code;hyperthreading;isotropic turbulence;nonblocking communication;parallel performance;performance analysis;shock waves;structured grids,,2,,19,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
A distributed dynamic load balancer for iterative applications,H. Menon; L. KalÌ©,"Dept. of Comput. Sci., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,11,"For many applications, computation load varies over time. Such applications require dynamic load balancing to improve performance. Centralized load balancing schemes, which perform the load balancing decisions at a central location, are not scalable. In contrast, fully distributed strategies are scalable but typically do not produce a balanced work distribution as they tend to consider only local information. This paper describes a fully distributed algorithm for load balancing that uses partial information about the global state of the system to perform load balancing. This algorithm, referred to as GrapevineLB, consists of two stages: global information propagation using a lightweight algorithm inspired by epidemic [21] algorithms, and work unit transfer using a randomized algorithm. We provide analysis of the algorithm along with detailed simulation and performance comparison with other load balancing strategies. We demonstrate the effectiveness of GrapevineLB for adaptive mesh refinement and molecular dynamics on up to 131,072 cores of BlueGene/Q.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503284,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877448,distributed load balancer;epidemic algorithm;load balancing,Abstracts;Algorithm design and analysis;Erbium;Load modeling;Logic gates;Pipelines,distributed algorithms;iterative methods;molecular dynamics method;randomised algorithms;resource allocation,BlueGene/Q;GrapevineLB;adaptive mesh refinement;centralized load balancing schemes;computation load;distributed algorithm;distributed dynamic load balancer;distributed strategies;epidemic algorithms;global information propagation;iterative applications;lightweight algorithm;load balancing decisions;load balancing strategies;molecular dynamics;partial information;randomized algorithm;work unit transfer,,4,,35,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Parallel reduction to Hessenberg form with Algorithm-Based Fault Tolerance,Y. Jia; G. Bosilca; P. Luszczek; J. J. Dongarra,"Univ. of Tennessee, Knoxville, TN, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,11,"This paper studies the resilience of a two-sided factorization and presents a generic algorithm-based approach capable of making two-sided factorizations resilient. We establish the theoretical proof of the correctness and the numerical stability of the approach in the context of a Hessenberg Reduction (HR) and present the scalability and performance results of a practical implementation. Our method is a hybrid algorithm combining an Algorithm Based Fault Tolerance (ABFT) technique with diskless checkpointing to fully protect the data. We protect the trailing and the initial part of the matrix with checksums, and protect finished panels in the panel scope with diskless checkpoints. Compared with the original HR (the ScaLA-PACK PDGEHRD routine) our fault-tolerant algorithm introduces very little overhead, and maintains the same level of scalability. We prove that the overhead shows a decreasing trend as the size of the matrix or the size of the process grid increases.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503249,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877521,Algorithm-based fault tolerance;Dense linear algebra;Hessenberg reduction;Parallel numerical libraries;ScaLAPACK,Algorithm design and analysis;Checkpointing;Eigenvalues and eigenfunctions;Fault tolerance;Fault tolerant systems;Libraries;Prediction algorithms,checkpointing;mathematics computing;matrix decomposition;numerical stability;parallel algorithms;software fault tolerance,ABFT technique;HR;Hessenberg reduction;algorithm-based fault tolerance;data protection;diskless checkpointing;generic algorithm-based approach;hybrid algorithm;numerical stability;parallel reduction;two-sided factorization,,1,,46,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Globalizing selectively: Shared-memory efficiency with address-space separation,N. Mahajan; U. Pitambare; A. Chauhan,"Indiana Univ., Bloomington, IN, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"It has become common for MPI-based applications to run on shared-memory machines. However, MPI semantics do not allow leveraging shared memory fully for communication between processes from within the MPI library. This paper presents an approach that combines compiler transformations with a specialized runtime system to achieve zero-copy communication whenever possible by proving certain properties statically and globalizing data selectively by altering the allocation and deallocation of communication buffers. The runtime system provides dynamic optimization, when such proofs are not possible statically, by copying data only when there are write-write or read-write conflicts. We implemented a prototype compiler, using ROSE, and evaluated it on several benchmarks. Our system produces code that performs better than MPI in most cases and no worse than MPI, tuned for shared memory, in all cases.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503275,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877450,,Cloning;Libraries;Programming;Receivers;Resource management;Runtime;Semantics,application program interfaces;globalisation;message passing;program compilers;shared memory systems,MPI library;MPI semantics;MPI-based applications;ROSE;address-space separation;communication buffer deallocation;compiler transformations;data selectively globalization;prototype compiler;read-write conflicts;shared-memory efficiency;shared-memory machines;write-write conflicts;zero-copy communication,,0,,16,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
ACR: Automatic checkpoint/restart for soft and hard error protection,X. Ni; E. Meneses; N. Jain; L. V. KalÌ©,"Dept. of Comput. Sci., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"As machines increase in scale, many researchers have predicted that failure rates will correspondingly increase. Soft errors do not inhibit execution, but may silently generate incorrect results. Recent trends have shown that soft error rates are increasing, and hence they must be detected and handled to maintain correctness. We present a holistic methodology for automatically detecting and recovering from soft or hard faults with minimal application intervention. This is demonstrated by ACR: an automatic checkpoint/restart framework that performs application replication and automatically adapts the checkpoint period using online information about the current failure rate. ACR performs an application- and user-oblivious recovery. We empirically test ACR by injecting failures that follow different distributions for five applications and show low overhead when scaled to 131,072 cores. We also analyze the interaction between soft and hard errors and propose three recovery schemes that explore the trade-off between performance and reliability requirements.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503266,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877440,Fault-tolerance;checkpoint/restart;redundancy;silent data corruption,Checkpointing;Computer crashes;Fault tolerant systems;Redundancy;Resilience,checkpointing;error correction;fault tolerant computing,ACR;application-recovery;automatic checkpoint-restart framework;checkpoint period;failure rates;hard error protection;online information;soft error protection;user-oblivious recovery,,12,,30,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Exploring portfolio scheduling for long-term execution of scientific workloads in IaaS clouds,K. Deng; J. Song; K. Ren; A. Iosup,"Sch. of Comput., Nat. Univ. of Defense Technol., Changsha, China","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Long-term execution of scientific applications often leads to dynamic workloads and varying application requirements. When the execution uses resources provisioned from IaaS clouds, and thus consumption-related payment, efficient and online scheduling algorithms must be found. Portfolio scheduling, which selects dynamically a suitable policy from a broad portfolio, may provide a solution to this problem. However, selecting online the right policy from possibly tens of alternatives remains challenging. In this work, we introduce an abstract model to explore this selection problem. Based on the model, we present a comprehensive portfolio scheduler that includes tens of provisioning and allocation policies. We propose an algorithm that can enlarge the chance of selecting the best policy in limited time, possibly online. Through trace-based simulation, we evaluate various aspects of our portfolio scheduler, and find performance improvements from 7% to 100% in comparison with the best constituent policies and high improvement for bursty workloads.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503244,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877488,IaaS Cloud;Portfolio Scheduling;Resource Provisioning;Scientific Workloads,Abstracts;Computers;Databases;Silicon,cloud computing;digital simulation;natural sciences computing;resource allocation;scheduling,IaaS clouds;abstract model;allocation policy;comprehensive portfolio scheduler;consumption-related payment;long-term execution;online scheduling algorithms;portfolio scheduling;provisioning policy;scientific applications;scientific workloads;trace-based simulation,,5,,45,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
A scalable parallel algorithm for dynamic range-limited n-tuple computation in many-body molecular dynamics simulation,M. Kunaseth; R. K. Kalia; A. Nakano; K. i. Nomura; P. Vashishta,"Dept. of Comput. Sci., Univ. of Southern California, Los Angeles, CA, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Recent advancements in reactive molecular dynamics (MD) simulations based on many-body interatomic potentials necessitate efficient dynamic n-tuple computation, where a set of atomic n-tuples within a given spatial range is constructed at every time step. Here, we develop a computation-pattern algebraic framework to mathematically formulate general n-tuple computation. Based on translation/reflection-invariant properties of computation patterns within this framework, we design a shift-collapse (SC) algorithm for cell-based parallel MD. Theoretical analysis quantifies the compact n-tuple search space and small communication cost of SC-MD for arbitrary n, which are reduced to those in best pair-computation approaches (e.g. eighth-shell method) for n = 2. Benchmark tests show that SC-MD outperforms our production MD code at the finest grain, with 9.7-and 5.1-fold speedups on Intel-Xeon and BlueGene/Q clusters. SC-MD also exhibits excellent strong scalability.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503235,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877504,Dynamic range-limited n-tuple computation;Molecular dynamics;Parallel computing,Algorithm design and analysis;Biological system modeling;Computational modeling;Force;Heuristic algorithms;Program processors,algebra;molecular dynamics method;multiprocessing systems;parallel algorithms;physics computing,Blue Gene/Q clusters;Intel-Xeon;MD simulations;SC algorithm;atomic n-tuples;cell-based parallel MD;compact n-tuple search space;computation-pattern algebraic framework;dynamic range-limited n-tuple computation;eighth-shell method;many-body interatomic potentials;many-body molecular dynamics simulation;production MD code;reactive molecular dynamics simulations;scalable parallel algorithm;shift-collapse algorithm;small communication cost;translation-reflection-invariant properties,,0,,30,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Accelerating sparse matrix-vector multiplication on GPUs using bit-representation-optimized schemes,Wai Teng Tang; Wen Jun Tan; R. Ray; Yi Wen Wong; Weiguang Chen; Shyh-hao Kuo; R. S. M. Goh; S. J. Turner; Weng-Fai Wong,"Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"The sparse matrix-vector (SpMV) multiplication routine is an important building block used in many iterative algorithms for solving scientific and engineering problems. One of the main challenges of SpMV is its memory-boundedness. Although compression has been proposed previously to improve SpMV performance on CPUs, its use has not been demonstrated on the GPU because of the serial nature of many compression and decompression schemes. In this paper, we introduce a family of bit-representation-optimized (BRO) compression schemes for representing sparse matrices on GPUs. The proposed schemes, BRO-ELL, BRO-COO, and BRO-HYB, perform compression on index data and help to speed up SpMV on GPUs through reduction of memory traffic. Furthermore, we formulate a BRO-aware matrix reodering scheme as a data clustering problem and use it to increase compression ratios. With the proposed schemes, experiments show that average speedups of 1.5ÌÑ compared to ELLPACK and HYB can be achieved for SpMV on GPUs.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503234,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877459,GPU;Sparse matrix format;data compression;matrix-vector multiplication;memory bandwidth;parallelism,Abstracts;Acceleration;Educational institutions;Indexes;Instruction sets;Optimization,graphics processing units;iterative methods;mathematics computing;matrix multiplication;pattern clustering;sparse matrices,BRO compression schemes;BRO-COO;BRO-ELL;BRO-HYB;BRO-aware matrix reordering scheme;GPU;SpMV;bit-representation-optimized compression schemes;bit-representation-optimized schemes;data clustering problem;decompression schemes;engineering problems;iterative algorithms;memory traffic reduction;memory-boundedness;scientific problems;sparse matrix-vector multiplication routine,,4,,27,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Radiative signature of the relativistic Kelvin-Helmholtz Instability,M. Bussmann; H. Burau; T. E. Cowan; A. Debus; A. Huebl; G. Juckeland; T. Kluge; W. E. Nagel; R. Pausch; F. Schmitt; U. Schramm; J. Schuchart; R. Widera,"Helmholtz-Zentrum Dresden - Rossendorf, Dresden, Germany","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"We present a particle-in-cell simulation of the relativistic Kelvin-Helmholtz Instability (KHI) that for the first time delivers angularly resolved radiation spectra of the particle dynamics during the formation of the KHI. This enables studying the formation of the KHI with unprecedented spatial, angular and spectral resolution. Our results are of great importance for understanding astrophysical jet formation and comparable plasma phenomena by relating the particle motion observed in the KHI to its radiation signature. The innovative methods presented here on the implementation of the particle-in-cell algorithm on graphic processing units can be directly adapted to any many-core parallelization of the particle-mesh method. With these methods we see a peak performance of 7.176 PFLOP/s (double-precision) plus 1.449 PFLOP/s (single-precision), an efficiency of 96% when weakly scaling from 1 to 18432 nodes, an efficiency of 68.92% and a speed up of 794 (ideal: 1152) when strongly scaling from 16 to 18432 nodes.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2504564,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877438,,Computational modeling;Graphics processing units;Instruction sets;Kernel;Magnetic fields;Plasmas;Spatial resolution,Kelvin-Helmholtz instability;plasma simulation;relativistic plasmas,angular resolution;angular-resolved radiation spectra;astrophysical jet formation;graphic processing units;particle dynamics;particle motion;particle-in-cell algorithm;particle-in-cell simulation;particle-mesh method;plasma phenomena;relativistic Kelvin-Helmholtz Instability;spatial resolution;spectral resolution,,4,,68,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Enabling highly-scalable remote memory access programming with MPI-3 one sided,R. Gerstenberger; M. Besta; T. Hoefler,"Dept. of Comput. Sci., ETH Zurich, Zurich, Switzerland","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Modern interconnects offer remote direct memory access (RDMA) features. Yet, most applications rely on explicit message passing for communications albeit their unwanted overheads. The MPI-3.0 standard defines a programming interface for exploiting RDMA networks directly, however, it's scalability and practicability has to be demonstrated in practice. In this work, we develop scalable bufferless protocols that implement the MPI-3.0 specification. Our protocols support scaling to millions of cores with negligible memory consumption while providing highest performance and minimal overheads. To arm programmers, we provide a spectrum of performance models for all critical functions and demonstrate the usability of our library and models with several application studies with up to half a million processes. We show that our design is comparable to, or better than UPC and Fortran Coarrays in terms of latency, bandwidth, and message rate. We also demonstrate application performance improvements with comparable programming complexity.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503286,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877486,Performance,Hardware;Libraries;Memory management;Programming;Protocols;Radiation detectors;Synchronization,application program interfaces;memory protocols;message passing,Fortran Coarrays;MPI-3 one sided standard;RDMA networks;UPC;explicit message passing;highly-scalable remote memory access programming;memory consumption;performance models;programming interface;remote direct memory access;scalable bufferless protocols,,7,,41,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Compiling affine loop nests for distributed-memory parallel architectures,U. Bondhugula,"Dept. of Comput. Sci. & Autom., Indian Inst. of Sci., Bangalore, India","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"We present new techniques for compilation of arbitrarily nested loops with affine dependences for distributed-memory parallel architectures. Our framework is implemented as a source-level transformer that uses the polyhedral model, and generates parallel code with communication expressed with the Message Passing Interface (MPI) library. Compared to all previous approaches, ours is a significant advance either (1) with respect to the generality of input code handled, or (2) efficiency of communication code, or both. We provide experimental results on a cluster of multicores demonstrating its effectiveness. In some cases, code we generate outperforms manually parallelized codes, and in another case is within 25% of it. To the best of our knowledge, this is the first work reporting end-to-end fully automatic distributed-memory parallelization and code generation for input programs and transformation techniques as general as those we allow.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503289,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877466,Polyhedral model;affine loop nests;code generation;distributed-memory;parallelization,Indexes;Libraries;Parallel processing;Program processors;Silicon;Synchronization;Vectors,affine transforms;application program interfaces;distributed memory systems;message passing;multiprocessing systems;parallel architectures;program compilers,MPI library;affine loop nest compilation;arbitrarily nested loop compilation;code generation;communication code;distributed-memory parallel architectures;end-to-end fully automatic distributed-memory parallelization;message passing interface library;multicore cluster;parallel code;polyhedral model;source-level transformer,,4,,38,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Guide-copy: Fast and silent migration of virtual machine for datacenters,J. Kim; D. Chae; J. Kim; J. Kim,"Dept. of Comput. Sci. & Eng., POSTECH, Pohang, South Korea","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Cloud infrastructure providers deploy Dynamic Resource Management (DRM) to minimize the cost of datacenter operation, while maintaining the Service Level Agreement (SLA). Such DRM schemes depend on the capability to migrate virtual machine (VM) images. However, existing migration techniques are not suitable for highly utilized clouds due to their latency and bandwidth critical memory transfer mechanisms. In this paper, we propose guide-copy migration, a novel VM migration scheme to provide a fast and silent migration, which works nicely under highly utilized clouds. The guide-copy migration transfers only the memory pages accessed at the destination node in the near future by running a guide version of the VM at the source node and a migrated VM at the destination node simultaneously during the migration. The guide-copy migration's highly accurate and low-bandwidth memory transfer mechanism enables a fast and silent VM migration to maintain the SLA of all VMs in the cloud.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877499,Operating Systems;Process Migration;Virtual Machines,Abstracts;Continuous wavelet transforms,cloud computing;computer centres;contracts;resource allocation;virtual machines,DRM;SLA;VM migration scheme;datacenters;dynamic resource management;fast migration;guide-copy migration;highly utilized clouds;low-bandwidth memory transfer mechanism;service level agreement;silent migration;virtual machine,,1,,37,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Enabling fair pricing on HPC systems with node sharing,A. D. Breslow; A. Tiwari; M. Schulz; L. Carrington; L. Tang; J. Mars,"Univ. of California, San Diego, La Jolla, CA, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Co-location, where multiple jobs share compute nodes in large-scale HPC systems, has been shown to increase aggregate throughput and energy efficiency by 10 to 20%. However, system operators disallow co-location due to fair-pricing concerns, i.e., a pricing mechanism that considers performance interference from co-running jobs. In the current pricing model, application execution time determines the price, which results in unfair prices paid by the minority of users whose jobs suffer from co-location. This paper presents POPPA, a runtime system that enables fair pricing by delivering precise online interference detection and facilitates the adoption of supercomputers with co-locations. POPPA leverages a novel shutter mechanism - a cyclic, fine-grained interference sampling mechanism to accurately deduce the interference between co-runners - to provide unbiased pricing of jobs that share nodes. POPPA is able to quantify inter-application interference within 4% mean absolute error on a variety of co-located benchmark and real scientific workloads.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503256,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877470,Chip Multiprocessor;Contention;Online Pricing;Resource Sharing;Supercomputer Accounting,Abstracts;Logic gates;Pricing;Supercomputers,parallel machines;parallel processing,HPC systems;POPPA;fair pricing;fine-grained interference sampling mechanism;interference reduction;node sharing;runtime system;shutter mechanism;supercomputers,,2,,65,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
"Toward millions of file system IOPS on low-cost, commodity hardware",D. Zheng; R. Burns; A. S. Szalay,"Dept. of Comput. Sci., Johns Hopkins Univ., Baltimore, MD, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"We describe a storage system that removes I/O bottlenecks to achieve more than one million IOPS based on a user-space file abstraction for arrays of commodity SSDs. The file abstraction refactors I/O scheduling and placement for extreme parallelism and non-uniform memory and I/O. The system includes a set-associative, parallel page cache in the user space. We redesign page caching to eliminate CPU overhead and lock-contention in non-uniform memory architecture machines. We evaluate our design on a 32 core NUMA machine with four, eight-core processors. Experiments show that our design delivers 1.23 million 512-byte read IOPS. The page cache realizes the scalable IOPS of Linux asynchronous I/O (AIO) and increases user-perceived I/O performance linearly with cache hit rates. The parallel, set-associative cache matches the cache hit rates of the global Linux page cache under real workloads.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503225,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877502,Data-intensive computing;low cost;millions of IOPS;page cache optimization;solid-state storage devices,Hardware;Instruction sets;Kernel;Linux;Message systems;Performance evaluation,Linux;cache storage;input-output programs,32 core NUMA machine;AIO;CPU overhead;IO bottlenecks;IO scheduling;Linux asynchronous IO;cache hit rates;commodity SSD;extreme parallelism;file abstraction;file system IOPS;global Linux page cache;lock-contention;low-cost commodity hardware;nonuniform memory architecture machines;parallel page cache;storage system;user space;user-perceived IO performance;user-space file abstraction,,4,,34,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Predicting application performance using supervised learning on communication features,N. Jain; A. Bhatele; M. P. Robson; T. Gamblin; L. V. Kale,"Dept. of Comput. Sci., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Task mapping on torus networks has traditionally focused on either reducing the maximum dilation or average number of hops per byte for messages in an application. These metrics make simplified assumptions about the cause of network congestion, and do not provide accurate correlation with execution time. Hence, these metrics cannot be used to reasonably predict or compare application performance for different mappings. In this paper, we attempt to model the performance of an application using communication data, such as the communication graph and network hardware counters. We use supervised learning algorithms, such as randomized decision trees, to correlate performance with prior and new metrics. We propose new hybrid metrics that provide high correlation with application performance, and may be useful for accurate performance prediction. For three different communication patterns and a production application, we demonstrate a very strong correlation between the proposed metrics and the execution time of these codes.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503263,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877528,contention;modeling;prediction;supervised learning;task mapping;torus networks,Benchmark testing;Correlation;Decision trees;Hardware;Measurement;Predictive models;Radiation detectors,decision trees;learning (artificial intelligence);multiprocessing systems;multiprocessor interconnection networks;performance evaluation,application performance prediction;communication data;communication features;communication graph;communication patterns;network congestion;network hardware counters;production application;randomized decision trees;supervised learning algorithms;task mapping;torus networks,,7,,17,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Coordinated energy management in heterogeneous processors,I. Paul; V. Ravi; S. Manne; M. Arora; S. Yalamanchili,"Adv. Micro Devices, Inc., Boxborough, MA, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"This paper examines energy management in a heterogeneous processor consisting of an integrated CPU-GPU for high-performance computing (HPC) applications. Energy management for HPC applications is challenged by their uncompromising performance requirements and complicated by the need for coordinating energy management across distinct core types - a new and less understood problem. We examine the intra-node CPU-GPU frequency sensitivity of HPC applications on tightly coupled CPU-GPU architectures as the first step in understanding power and performance optimization for a heterogeneous multi-node HPC system. The insights from this analysis form the basis of a coordinated energy management scheme, called DynaCo, for integrated CPU-GPU architectures. We implement DynaCo on a modern heterogeneous processor and compare its performance to a state-of-the-art power- and performance-management algorithm. DynaCo improves measured average energy-delay squared (ED^2) product by up to 30% with less than 2% average performance loss across several exascale and other HPC workloads.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503227,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877492,Energy management;High-performance computing,Central Processing Unit;Energy management;Frequency measurement;Graphics processing units;Kernel;Sensitivity,graphics processing units;multiprocessing systems;parallel architectures;performance evaluation;power aware computing,DynaCo;HPC workloads;average energy-delay squared product;coordinated energy management scheme;heterogeneous multinode HPC system;heterogeneous processors;high-performance computing;integrated CPU-GPU;intra-node CPU-GPU frequency sensitivity;performance optimization;performance-management algorithm;power-management algorithm;tightly coupled CPU-GPU architectures,,7,,43,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
There goes the neighborhood: Performance degradation due to nearby jobs,A. Bhatele; K. Mohror; S. H. Langer; K. E. Isaacs,"Lawrence Livermore Nat. Lab., Livermore, CA, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Predictable performance is important for understanding and alleviating application performance issues; quantifying the effects of source code, compiler, or system software changes; estimating the time required for batch jobs; and determining the allocation requests for proposals. Our experiments show that on a Cray XE system, the execution time of a communication-heavy parallel application ranges from 28% faster to 41% slower than the average observed performance. Blue Gene systems, on the other hand, demonstrate no noticeable run-to-run variability. In this paper, we focus on Cray machines and investigate potential causes for performance variability such as OS jitter, shape of the allocated partition, and interference from other jobs sharing the same network links. Reducing such variability could improve overall throughput at a computer center and save energy costs.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503247,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877474,communication performance;interference;resource management;system noise;torus networks,Interference;Laser beams;Message passing;Resource management;Shape;Three-dimensional displays;Topology,Cray computers;parallel processing;performance evaluation,Blue Gene systems;Cray XE system;Cray machines;OS jitter;allocation requests;batch jobs;communication-heavy parallel application;compiler;computer center;energy cost saving;network links;performance degradation;performance variability;predictable performance;source code;system software changes;time estimation,,20,,17,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
"Low-power, low-storage-overhead chipkill correct via multi-line error correction",X. Jian; H. Duwe; J. Sartori; V. Sridharan; R. Kumar,"Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Due to their large memory capacities, many modern servers require chipkill correct, an advanced type of memory error detection and correction, to meet their reliability requirements. However, existing chipkill-correct solutions incur high power or storage overheads, or both because they use dedicated error-correction resources per codeword to perform error correction. This requires high overhead for correction and results in high overhead for error detection. We propose a novel chipkill-correct solution, multi-line error correction, that uses resources shared across multiple lines in memory for error correction to reduce the overhead of both error detection and correction. Our evaluations show that the proposed solution reduces memory power by a mean of 27%, and up to 38% with respect to commercial solutions, at a cost of 0.4% increase in storage overhead and minimal impact on reliability.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503243,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877457,,Error correction;Error correction codes;Maximum likelihood decoding;Maximum likelihood detection;Memory management;Reliability;Servers,DRAM chips;error correction;error detection;integrated circuit reliability;low-power electronics;storage management,DRAM device;dedicated error-correction resources per codeword;low-power low-storage-overhead chipkill correct solution;memory error detection;multiline error correction;reliability;storage overheads,,6,,26,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Assessing the effects of data compression in simulations using physically motivated metrics,D. Laney; S. Langer; C. Weber; P. Lindstrom; A. Wegener,"Lawrence Livermore Lab., Lawrence, KS, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"This paper examines whether lossy compression can be used effectively in physics simulations as a possible strategy to combat the expected data-movement bottleneck in future high performance computing architectures. We show that, for the codes and simulations we tested, compression levels of 3-5X can be applied without causing significant changes to important physical quantities. Rather than applying signal processing error metrics, we utilize physics-based metrics appropriate for each code to assess the impact of compression. We evaluate three different simulation codes: a Lagrangian shock-hydrodynamics code, an Eulerian higher-order hydrodynamics turbulence modeling code, and an Eulerian coupled laser-plasma interaction code. We compress relevant quantities after each time-step to approximate the effects of tightly coupled compression and study the compression rates to estimate memory and disk-bandwidth reduction. We find that the error characteristics of compression algorithms must be carefully considered in the context of the underlying physics being modeled.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503283,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877509,data compression;high performance computing,Abstracts;Correlation;Electric shock;Encoding;Gas lasers;Laser beams;Lead,data compression;digital simulation;hydrodynamics;parallel processing;physics computing;turbulence,Eulerian coupled laser-plasma interaction code;Eulerian higher-order hydrodynamics turbulence modeling code;Lagrangian shock-hydrodynamics code;compression algorithms;compression impact assessment;compression rates;data compression effects;disk-bandwidth reduction;error characteristics;high performance computing architectures;lossy compression;physically motivated metrics;physics simulations;physics-based metrics;simulation codes;tightly coupled compression,,4,,35,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
11 PFLOP/s simulations of cloud cavitation collapse,D. Rossinelli; B. Hejazialhosseini; P. Hadjidoukas; C. Bekas; A. Curioni; A. Bertsch; S. Futral; S. J. Schmidt; N. A. Adams; P. Koumoutsakos,"Dept. of Comput. Sci., ETH Zurich, Zurich, Switzerland","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,13,"We present unprecedented, high throughput simulations of cloud cavitation collapse on 1.6 million cores of Sequoia reaching 55% of its nominal peak performance, corresponding to 11 PFLOP/s. The destructive power of cavitation reduces the lifetime of energy critical systems such as internal combustion engines and hydraulic turbines, yet it has been harnessed for water purification and kidney lithotripsy. The present two-phase flow simulations enable the quantitative prediction of cavitation using 13 trillion grid points to resolve the collapse of 15'000 bubbles. We advance by one order of magnitude the current state-of-the-art in terms of time to solution, and by two orders the geometrical complexity of the flow. The software successfully addresses the challenges that hinder the effective solution of complex flows on contemporary supercomputers, such as limited memory bandwidth, I/O bandwidth and storage capacity. The present work redefines the frontier of high performance computing for fluid dynamics simulations.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2504565,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877436,,Bandwidth;Computational modeling;Equations;Kernel;Mathematical model;Supercomputers,bubbles;cavitation;cloud computing;computational fluid dynamics;flow simulation;mainframes;parallel machines;parallel processing;two-phase flow,I/O bandwidth;PFLOP/s simulations;cloud cavitation collapse;contemporary supercomputers;destructive power;energy critical systems;fluid dynamics simulations;geometrical complexity;high performance computing;hydraulic turbines;internal combustion engines;kidney lithotripsy;storage capacity;two-phase flow simulations;water purification,,2,,81,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Scalable parallel graph partitioning,S. Kirmani; P. Raghavan,"Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,10,"We consider partitioning a graph in parallel using a large number of processors. Parallel multilevel partitioners, such as Pt-Scotch and ParMetis, produce good quality partitions but their performance scales poorly. Coordinate bisection schemes such as those in Zoltan, which can be applied only to graphs with coordinates, scale well but partition quality is often compromised. We seek to address this gap by developing a scalable parallel scheme which imparts coordinates to a graph through a lattice-based multilevel embedding. Partitions are computed with a parallel formulation of a geometric scheme that has been shown to provide provably good cuts on certain classes of graphs. We analyze the parallel complexity of our scheme and we observe speed-ups and cut-sizes on large graphs. Our results indicate that our method is substantially faster than ParMetis and Pt-Scotch for hundreds to thousands of processors, while producing high quality cuts.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503280,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877484,,Force;Lattices;Particle separators;Partitioning algorithms;Program processors;Smoothing methods;Strips,graph theory;multiprocessing systems;parallel processing,ParMetis;Pt-Scotch;Zoltan;coordinate bisection schemes;geometric scheme;lattice-based multilevel embedding;parallel complexity;parallel multilevel partitioners;partition quality;scalable parallel graph partitioning scheme,,4,,27,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Semi-automatic restructuring of offloadable tasks for many-core accelerators,N. Ravi; Y. Yang; T. Bao; S. Chakradhar,"NEC Labs. America, Princeton, NJ, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Work division between the processor and accelerator is a common theme in modern heterogenous computing. Recent efforts (such as LEO and OpenAcc) provide directives that allow the developer to mark code regions in the original application from which offloadable tasks can be generated by the compiler. Auto-tuners and runtime schedulers work with the options (i.e., offloadable tasks) generated at compile time, which is limited by the directives specified by the developer. There is no provision for offload restructuring.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503285,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877445,,Coprocessors;Data transfer;Indexes;Low earth orbit satellites;Microwave integrated circuits;Runtime;Semantics,multiprocessing systems;parallel architectures;parallel languages;processor scheduling;program compilers,CPU;EOC;Intel Xeon Phi architecture;LEO;MIC architecture;NAS parallel benchmarks;SpecOMP;accelerator memory;auto-tuners;code regions;directive-based languages;elastic offload compiler;heterogeneous computing;language extensions for offload;many-core accelerators;offloadable tasks;parallel loop;processor;relaxed semantics;runtime schedulers;semiautomatic restructuring;sub-offload;super-offload,,0,,48,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Parallel design and performance of nested filtering factorization preconditioner,L. Qu; L. Grigori; F. Nataf,"Lab. de Rech. en Inf., Univ. Paris Sud 11, Orsay, France","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"We present the parallel design and performance of the nested filtering factorization preconditioner (NFF), which can be used for solving linear systems arising from the discretization of a system of PDEs on unstructured grids. NFF has limited memory requirements, and it is based on a two level recursive decomposition that exploits a nested block arrow structure of the input matrix, obtained priorly by using graph partitioning techniques. It also allows to preserve several directions of interest of the input matrix to alleviate the effect of low frequency modes on the convergence of iterative methods. For a boundary value problem with highly heterogeneous coefficients, discretized on three-dimensional grids with 64 millions unknowns and 447 millions nonzero entries, we show experimentally that NFF scales up to 2048 cores of Genci's Bull system (Curie), and it is up to 2.6 times faster than the domain decomposition preconditioner Restricted Additive Schwarz implemented in PETSc.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503287,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877514,,Approximation methods;Equations;Mathematical model;Matrix decomposition;Particle separators;Program processors;Vectors,boundary-value problems;filtering theory;graph theory;iterative methods;linear algebra;mathematics computing;parallel processing;partial differential equations,Genci's Bull system;NFF scales;PDE;PETSc;boundary value problem;domain decomposition preconditioner;graph partitioning techniques;heterogeneous coefficients;iterative methods;level recursive decomposition;linear systems;memory requirements;nested filtering factorization preconditioner performance;parallel design;restricted additive Schwarz;three-dimensional grids;unstructured grids,,0,,21,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
On fast parallel detection of strongly connected components (SCC) in small-world graphs,S. Hong; N. C. Rodia; K. Olukotun,"Oracle Labs., Redwood Shores, CA, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,11,"Detecting strongly connected components (SCCs) in a directed graph is a fundamental graph analysis algorithm that is used in many science and engineering domains. Traditional approaches in parallel SCC detection, however, show limited performance and poor scaling behavior when applied to large real-world graph instances. In this paper, we investigate the shortcomings of the conventional approach and propose a series of extensions that consider the fundamental properties of real-world graphs, e.g. the small-world property. Our scalable implementation offers excellent performance on diverse, small-world graphs resulting in a 5.01ÌÑ to 29.41ÌÑ parallel speedup over the optimal sequential algorithm with 16 cores and 32 hardware threads.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503246,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877525,graph algorithms;multicore;parallel algorithms;small-world graphs;strongly connected components (SCC),Algorithm design and analysis;Arrays;Color;Image color analysis;Parallel processing;Partitioning algorithms,directed graphs;parallel algorithms,directed graph;fast parallel detection;fundamental graph analysis algorithm;large real-world graph instances;optimal sequential algorithm;parallel SCC detection;parallel algorithms;small-world graphs;small-world property;strongly connected component detection,,5,,30,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Using automated performance modeling to find scalability bugs in complex codes,A. Calotoiu; T. Hoefler; M. Poke; F. Wolf,"German Res. Sch. for Simulation Sci., RWTH Aachen Univ., Aachen, Germany","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Many parallel applications suffer from latent performance limitations that may prevent them from scaling to larger machine sizes. Often, such scalability bugs manifest themselves only when an attempt to scale the code is actually being made-a point where remediation can be difficult. However, creating analytical performance models that would allow such issues to be pinpointed earlier is so laborious that application developers attempt it at most for a few selected kernels, running the risk of missing harmful bottlenecks. In this paper, we show how both coverage and speed of this scalability analysis can be substantially improved. Generating an empirical performance model automatically for each part of a parallel program, we can easily identify those parts that will reduce performance at larger core counts. Using a climate simulation as an example, we demonstrate that scalability bugs are not confined to those routines usually chosen as kernels.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503277,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877478,Scalasca;performance analysis;performance modeling;scalability,Analytical models;Biological system modeling;Computational modeling;Computer bugs;Kernel;Measurement;Scalability,parallel programming;program debugging;software performance evaluation;software reliability,analytical performance models;automated performance modeling;climate simulation;complex codes;core counts;machine sizes;parallel program;scalability analysis;scalability bugs,,4,,42,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
The Science DMZ: A network design pattern for data-intensive science,E. Dart; L. Rotman; B. Tierney; M. Hester; J. Zurawski,"Energy Sci. Network, Lawrence Berkeley Nat. Lab., Berkeley, CA, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,10,"The ever-increasing scale of scientific data has become a significant challenge for researchers that rely on networks to interact with remote computing systems and transfer results to collaborators worldwide. Despite the availability of high-capacity connections, scientists struggle with inadequate cyberinfrastructure that cripples data transfer performance, and impedes scientific progress. The Science DMZ paradigm comprises a proven set of network design patterns that collectively address these problems for scientists. We explain the Science DMZ model, including network architecture, system configuration, cybersecurity, and performance tools, that creates an optimized network environment for science. We describe use cases from universities, supercomputing centers and research laboratories, highlighting the effectiveness of the Science DMZ model in diverse operational settings. In all, the Science DMZ model is a solid platform that supports any science workflow, and flexibly accommodates emerging network technologies. As a result, the Science DMZ vastly improves collaboration, accelerating scientific discovery.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503245,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877518,Design;Measurement;Performance;Reliability,Data transfer;Monitoring;Packet loss;Security;Throughput;Wide area networks,data communication;natural sciences computing;scientific information systems,cyberinfrastructure;cybersecurity;data transfer performance;data-intensive science;high-capacity connections;network design pattern;network design patterns;performance tools;remote computing systems;science DMZ paradigm;scientific data;scientific discovery;supercomputing centers;system configuration;universities,,16,,17,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Location-aware cache management for many-core processors with deep cache hierarchy,Jongsoo Park; R. M. Yoo; D. S. Khudia; C. J. Hughes; Daehyun Kim,"Parallel Comput. Lab., Intel Corp., Santa Clara, CA, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"As cache hierarchies become deeper and the number of cores on a chip increases, managing caches becomes more important for performance and energy. However, current hardware cache management policies do not always adapt optimally to the applications behavior: e.g., caches may be polluted by data structures whose locality cannot be captured by the caches, and producer-consumer communication incurs multiple round trips of coherence messages per cache line transferred. We propose load and store instructions that carry hints regarding into which cache(s) the accessed data should be placed. Our instructions allow software to convey locality information to the hardware, while incurring minimal hardware cost and not affecting correctness. Our instructions provide a 1.07ÌÑ speedup and a 1.24ÌÑ energy efficiency boost, on average, according to simulations on a 64-core system with private L1 and L2 caches. With a large shared L3 cache added, the benefits increase, providing 1.33ÌÑ energy reduction on average.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503224,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877453,Energy-Efficient Memory Hierarchy;Producer-Consumer Communication;Reuse Distance;Streaming Memory Accesses,Abstracts;Coherence;Protocols;Streaming media;Tin,cache storage;data structures;mobile computing;multiprocessing systems;performance evaluation;power aware computing,64-core system;data structures;deep cache hierarchy;hardware cache management policies;locality information;location-aware cache management;many-core processors;private L1 cache;private L2 cache;producer-consumer communication,,3,,51,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Parallelizing the execution of sequential scripts,Zhao Zhang; D. S. Katz; T. G. Armstrong; J. M. Wozniak; I. Foster,"Dept. of Comput. Sci., Univ. of Chicago, Chicago, IL, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Scripting is often used in science to create applications via the composition of existing programs. Parallel scripting systems allow the creation of such applications, but each system introduces the need to adopt a somewhat specialized programming model. We present an alternative scripting approach, AMFS Shell, that lets programmers express parallel scripting applications via minor extensions to existing sequential scripting languages, such as Bash, and then execute them in-memory on large-scale computers. We define a small set of commands between the scripts and a parallel scripting runtime system, so that programmers can compose their scripts in a familiar scripting language. The underlying AMFS implements both collective (fast file movement) and functional (transformation based on content) file management. Tasks are handled by AMFS's built-in execution engine. AMFS Shell is expressive enough for a wide range of applications, and the framework can run such applications efficiently on large-scale computers.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503222,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877464,Many-task computing;Parallel scripting;Shared file system,Computers;Educational institutions;Engines;Parallel processing;Programming;Runtime;Servers,authoring languages;file organisation;parallel programming,AMFS shell;Bash;execution parallelization;file management;large-scale computers;parallel scripting runtime system;parallel scripting systems;sequential scripting languages;sequential scripts;specialized programming model,,4,,35,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Optimization of cloud task processing with checkpoint-restart mechanism,S. Di; Y. Robert; F. Vivien; D. Kondo; C. L. Wang; F. Cappello,"Argonne Nat. Lab., Argonne, IL, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"In this paper, we aim at optimizing fault-tolerance techniques based on a checkpointing/restart mechanism, in the context of cloud computing. Our contribution is three-fold. (1) We derive a fresh formula to compute the optimal number of checkpoints for cloud jobs with varied distributions of failure events. Our analysis is not only generic with no assumption on failure probability distribution, but also attractively simple to apply in practice. (2) We design an adaptive algorithm to optimize the impact of checkpointing regarding various costs like checkpointing/restart overhead. (3) We evaluate our optimized solution in a real cluster environment with hundreds of virtual machines and Berkeley Lab Checkpoint/Restart tool. Task failure events are emulated via a production trace produced on a large-scale Google data center. Experiments confirm that our solution is fairly suitable for Google systems. Our optimized formula outperforms Young's formula by 3-10 percent, reducing wall-clock lengths by 50-100 seconds per job on average.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503217,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877497,BLCR;Checkpoint-Restart Mechanism;Cloud Computing;Google;Optimal Checkpointing Interval,Checkpointing;Cloud computing;Clouds;Fault tolerance;Fault tolerant systems;Google;Probability distribution,cloud computing;failure analysis;software fault tolerance;software reliability;statistical distributions;virtual machines,Berkeley lab checkpoint-restart tool;adaptive algorithm;checkpoint-restart mechanism;cloud computing;cloud task processing optimization;failure event distributions;failure probability distribution;large-scale Google data center;optimizing fault-tolerance techniques;production trace;real cluster environment;task failure events;virtual machines,,15,,31,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
A data-centric profiler for parallel programs,X. Liu; J. Mellor-Crummey,"Dept. of Comput. Sci., Rice Univ., Houston, TX, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"It is difficult to manually identify opportunities for enhancing data locality. To address this problem, we extended the HPCToolkit performance tools to support data-centric profiling of scalable parallel programs. Our tool uses hardware counters to directly measure memory access latency and attributes latency metrics to both variables and instructions. Different hardware counters provide insight into different aspects of data locality (or lack thereof). Unlike prior tools for data-centric analysis, our tool employs scalable measurement, analysis, and presentation methods that enable it to analyze the memory access behavior of scalable parallel programs with low runtime and space overhead. We demonstrate the utility of HPCToolkit's new data-centric analysis capabilities with case studies of five well-known benchmarks. In each benchmark, we identify performance bottlenecks caused by poor data locality and demonstrate non-trivial performance optimizations enabled by this guidance.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503297,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877461,Data-centric profiling;data locality;scalable profiler,Context;Instruction sets;Monitoring;Phasor measurement units;Resource management,data analysis;optimisation;parallel programming,HPCToolkit data-centric analysis capabilities;HPCToolkit performance tools;data locality;data-centric profiler;hardware counters;latency metrics;memory access latency;parallel programs;performance optimizations;scalable parallel programs,,3,,39,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
A framework for hybrid parallel flow simulations with a trillion cells in complex geometries,C. Godenschwager; F. Schornbaum; M. Bauer; H. KÌ¦stler; U. RÌ_de,"Friedrich-Alexander-Univ. Erlangen-Nurnberg, Erlangen, Germany","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"waLBerla is a massively parallel software framework for simulating complex flows with the lattice Boltzmann method (LBM). Performance and scalability results are presented for SuperMUC, the world's fastest x86-based supercomputer ranked number 6 on the Top500 list, and JUQUEEN, a Blue Gene/Q system ranked as number 5. We reach resolutions with more than one trillion cells and perform up to 1.93 trillion cell updates per second using 1.8 million threads. The design and implementation of waLBerla is driven by a careful analysis of the performance on current petascale supercomputers. Our fully distributed data structures and algorithms allow for efficient, massively parallel simulations on these machines. Elaborate node level optimizations and vectorization using SIMD instructions result in highly optimized compute kernels for the single- and two-relaxation-time LBM. Excellent weak and strong scaling is achieved for a complex vascular geometry of the human coronary tree.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503273,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877468,,Computational modeling;Geometry;Kernel;Lattice Boltzmann methods;Load modeling;Supercomputers,data structures;flow simulation;lattice Boltzmann methods;mainframes;optimisation;parallel machines;parallel programming,Blue Gene/Q system;JUQUEEN;SIMD instructions;SuperMUC;Top500 list;WALBERLA;complex geometries;complex vascular geometry;data structures;human coronary tree;hybrid parallel flow simulation framework;lattice Boltzmann method;massively parallel software framework;petascale supercomputers;single-relaxation-time LBM;two-relaxation-time LBM;x86-based supercomputer,,1,,38,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Petascale direct numerical simulation of turbulent channel flow on up to 786K cores,M. Lee; N. Malaya; R. D. Moser,"Dept. of Mech. Eng., Univ. of Texas at Austin, Austin, TX, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,11,"We present results of performance optimization for direct numerical simulation (DNS) of wall bounded turbulent flow (channel flow). DNS is a technique in which the fluid flow equations are solved without subgrid modeling. Of particular interest are high Reynolds number (Re) turbulent flows over walls, because of their importance in technological applications. Simulating high Re turbulence is a challenging computational problem, due to the high spatial and temporal resolution requirements.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503298,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877494,Data transpose;MPI alltoall;Parallel FFT;Petascale;Turbulence,Abstracts;Benchmark testing;Equations;Mathematical model;Mechanical engineering;Numerical simulation;Vectors,boundary layer turbulence;channel flow;computational fluid dynamics;external flows;flow simulation;numerical analysis,DNS;channel flow;computational fluid dynamics;fluid flow equations;grid modeling;high Reynolds number;performance optimization;petascale direct numerical simulation;turbulent flow over walls;wall bounded turbulent flow,,2,,28,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Load-balanced pipeline parallelism,M. Kamruzzaman; S. Swanson; D. M. Tullsen,"Comput. Sci. & Eng., Univ. of California, San Diego, La Jolla, CA, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Accelerating a single thread in current parallel systems remains a challenging problem, because sequential threads do not naturally take advantage of the additional cores. Recent work shows that automatic extraction of pipeline parallelism is an effective way to speed up single thread execution. However, two problems remain challenging - load balancing and inter-thread communication. This work shows new mechanism to exploit pipeline parallelism that naturally solves the load balancing and communication problems. This compiler-based technique automatically extracts the pipeline stages and executes them in a data parallel fashion, using token-based chunked synchronization to handle sequential stages. This technique provides linear speedup for several applications, and outperforms prior techniques to exploit pipeline parallelism by as much as 50%.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503295,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877447,chip multiprocessors;compilers;load-balancing;locality;pipeline parallelism,Instruction sets;Load management;Pipeline processing;Pipelines;Synchronization,multiprocessing systems;parallel processing;pipeline processing;program compilers;resource allocation;synchronisation,compiler-based technique;data parallel fashion;inter-thread communication;load-balanced pipeline parallelism;parallel systems;pipeline stage automatic extraction;sequential stages;sequential threads;single thread acceleration;token-based chunked synchronization,,0,,26,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Insights for exascale IO APIs from building a petascale IO API,J. Lofstead; R. Ross,"Sandia Nat. Labs., Albuquerque, NM, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Near the dawn of the petascale era, IO libraries had reached a stability in their function and data layout with only incremental changes being incorporated. The shift in technology, particularly the scale of parallel file systems and the number of compute processes, prompted revisiting best practices for optimal IO performance. Among other efforts like PLFS, the project that led to ADIOS, the ADaptable IO System, was motivated by both the shift in technology and the historical requirement, for optimal IO performance, to change how simulations performed IO depending on the platform. To solve both issues, the ADIOS team, along with consultation with other leading IO experts, sought to build a new IO platform based on the assumptions inherent in the petascale hardware platforms. This paper helps inform the design of future IO platforms with a discussion of lessons learned as part of the process of designing and building ADIOS.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503238,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877520,,Arrays;Hardware;Layout;Libraries;Performance evaluation;Standards;Writing,application program interfaces;file organisation;multiprocessing systems;parallel processing,ADIOS;IO libraries;PLFS;adaptable IO system;data layout;exascale IO APIs;optimal IO performance;parallel file systems;petascale IO API;petascale hardware platforms,,3,,64,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Distributed wait state tracking for runtime MPI deadlock detection,T. Hilbrich; B. R. de Supinski; W. E. Nagel; J. Protze; C. Baier; M. S. MÌ_ller,"Tech. Univ. Dresden, Dresden, Germany","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"The widely used Message Passing Interface (MPI) with its multitude of communication functions is prone to usage errors. Runtime error detection tools aid in the removal of these errors. We develop MUST as one such tool that provides a wide variety of automatic correctness checks. Its correctness checks can be run in a distributed mode, except for its deadlock detection. This limitation applies to a wide range of tools that either use centralized detection algorithms or a timeout approach. In order to provide scalable and distributed deadlock detection with detailed insight into deadlock situations, we propose a model for MPI blocking conditions that we use to formulate a distributed algorithm. This algorithm implements scalable MPI deadlock detection in MUST. Stress tests at up to 4,096 processes demonstrate the scalability of our approach. Finally, overhead results for a complex benchmark suite demonstrate an average runtime increase of 34% at 2,048 processes.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503237,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877449,,Algorithm design and analysis;Distributed algorithms;Runtime;Scalability;Semantics;Standards;System recovery,application program interfaces;benchmark testing;concurrency control;distributed algorithms;error detection;message passing,MPI blocking conditions;MUST;centralized detection algorithms;communication functions;complex benchmark suite;correctness checks;deadlock situations;distributed algorithm;distributed deadlock detection;distributed wait state tracking;message passing interface;runtime MPI deadlock detection;runtime error detection tools;scalable MPI deadlock detection;timeout approach,,3,,30,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Cost-effective cloud HPC resource provisioning by building Semi-Elastic virtual clusters,S. Niu; J. Zhai; X. Ma; X. Tang; W. Chen,"Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing, China","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Recent studies have found cloud environments increasingly appealing for executing HPC applications, including tightly coupled parallel simulations. While public clouds offer elastic, on-demand resource provisioning and pay-as-you-go pricing, individual users setting up their on-demand virtual clusters may not be able to take full advantage of common cost-saving opportunities, such as reserved instances. In this paper, we propose a Semi-Elastic Cluster (SEC) computing model for organizations to reserve and dynamically resize a virtual cloud-based cluster. We present a set of integrated batch scheduling plus resource scaling strategies uniquely enabled by SEC, as well as an online reserved instance provisioning algorithm based on job history. Our trace-driven simulation results show that such a model has a 61.0% cost saving than individual users acquiring and managing cloud resources without causing longer average job wait time. Meanwhile, the overhead of acquiring/maintaining shared cloud instances is shown to take only a few seconds.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877489,Cloud computing;job scheduling;resource provisioning,Abstracts;Bismuth;Hardware;Portfolios;Program processors,batch processing (computers);cloud computing;parallel processing;pattern clustering;scheduling,SEC computing model;average job wait time;cloud environments;cloud resource management;cost-effective cloud HPC resource provisioning;elastic on-demand resource provisioning;integrated batch scheduling;job history;on-demand virtual clusters;online reserved instance provisioning algorithm;pay-as-you-go pricing;resource scaling strategy;semielastic virtual clusters;tightly coupled parallel simulations;trace-driven simulation;virtual cloud-based cluster,,0,,38,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
2HOT: An improved parallel hashed oct-tree N-Body algorithm for cosmological simulation,M. S. Warren,"Theor. Div., Los Alamos Nat. Lab., Los Alamos, NM, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"We report on improvements made over the past two decades to our adaptive treecode N-body method (HOT). A mathematical and computational approach to the cosmological N-body problem is described, with performance and scalability measured up to 256k (2<sup>18</sup>) processors. We present error analysis and scientific application results from a series of more than ten 69 billion (4096<sup>3</sup>) particle cosmological simulations, accounting for 4 ÌÑ 10<sup>20</sup> floating point operations. These results include the first simulations using the new constraints on the standard model of cosmology from the Planck satellite. Our simulations set a new standard for accuracy and scientific throughput, while meeting or exceeding the computational efficiency of the latest generation of hybrid TreePM N-body methods.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503220,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877505,Computational Cosmology;Fast Multipole Method;N-body,Abstracts;Ear;Equations,N-body problems;N-body simulations (astronomical);cosmology;octrees;parallel processing,2HOT;Planck satellite;adaptive treecode N-body method;computational approach;computational efficiency;cosmological N-body problem;cosmological simulation;error analysis;floating point operations;hybrid TreePM N-body methods;mathematical approach;parallel hashed oct-tree N-body algorithm;particle cosmological simulations;performance measurement;scalability measurement;scientific application;scientific throughput;standard cosmology model,,2,,77,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
HACC: Extreme scaling and performance across diverse architectures,S. Habib; V. Morozov; N. Frontiere; H. Finkel; A. Pope; K. Heitmann,"Argonne Nat. Lab., Argonne, IL, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,10,"Supercomputing is evolving towards hybrid and accelerator-based architectures with millions of cores. The HACC (Hardware/Hybrid Accelerated Cosmology Code) framework exploits this diverse landscape at the largest scales of problem size, obtaining high scalability and sustained performance. Developed to satisfy the science requirements of cosmological surveys, HACC melds particle and grid methods using a novel algorithmic structure that flexibly maps across architectures, including CPU/GPU, multi/many-core, and Blue Gene systems. We demonstrate the success of HACC on two very different machines, the CPU/GPU system Titan and the BG/Q systems Sequoia and Mira, attaining unprecedented levels of scalable performance. We demonstrate strong and weak scaling on Titan, obtaining up to 99.2% parallel efficiency, evolving 1.1 trillion particles. On Sequoia, we reach 13.94 PFlops (69.2% of peak) and 90% parallel efficiency on 1,572,864 cores, with 3.6 trillion particles, the largest cosmological benchmark yet performed. HACC design concepts are applicable to several other supercomputer applications.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2504566,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877439,,Accuracy;Adaptation models;Computer architecture;Force;Graphics processing units;Laboratories;Slabs,cosmology;multiprocessing systems;parallel architectures;performance evaluation;physics computing,Blue Gene systems;HACC;Mira BG/Q systems;PFlops;Sequoia BG/Q systems;Titan CPU-GPU system;accelerator-based architectures;algorithmic structure;cosmological benchmark;diverse architectures;grid methods;hardware-hybrid accelerated cosmology code framework;hybrid-based architectures;many-core system;multicore system;supercomputing,,5,,37,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
MVAPICH-PRISM: A proxy-based communication framework using InfiniBand and SCIF for Intel MIC clusters,S. Potluri; D. Bureddy; K. Hamidouche; A. Venkatesh; K. Kandalla; H. Subramoni; D. K. Panda,"Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,11,"Xeon Phi, based on the Intel Many Integrated Core (MIC) architecture, packs up to 1TFLOPs of performance on a single chip while providing x86_64 compatibility. On the other hand, InfiniBand is one of the most popular choices of interconnect for supercomputing systems. The software stack on Xeon Phi allows processes to directly access an InfiniBand HCA on the node and thus, provides a low latency path for internode communication. However, drawbacks in the state-of-the-art chipsets like Sandy Bridge limit the bandwidth available for these transfers. In this paper, we propose MVAPICH-PRISM, a novel proxy-based framework to optimize the communication performance on such systems. We present several designs and evaluate them using micro-benchmarks and application kernels. Our designs improve internode latency between Xeon Phi processes by up to 65% and internode bandwidth by up to five times. Our designs improve the performance of MPI_Alltoall operation by up to 65%, with 256 processes. They improve the performance of a 3D Stencil communication kernel and the P3DFFT library by 56% and 22% with 1,024 and 512 processes, respectively.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503288,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877487,Clusters;InfiniBand;MIC;MPI;PCIe;RDMA,Bandwidth;Computer architecture;Coprocessors;Libraries;Microwave integrated circuits;Peer-to-peer computing;Program processors,application program interfaces;message passing;multiprocessing systems;multiprocessor interconnection networks;parallel programming,1TFLOPs;3D Stencil communication kernel;InfiniBand HCA;Intel MIC clusters;Intel many integrated core architecture;MIC architecture;MPI_Alltoall operation;MVAPICH-PRISM;P3DFFT library;SCIF;Sandy Bridge;Xeon Phi processes;application kernels;chipsets;communication performance optimization;internode communication;low latency path;microbenchmarks;proxy-based communication framework;proxy-based framework;single chip;software stack;supercomputing systems;x86_64 compatibility,,11,,21,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Characterization and modeling of PIDX parallel I/O for performance optimization,S. Kumar; A. Saha; V. Vishwanath; P. Carns; J. A. Schmidt; G. Scorzelli; H. Kolla; R. Grout; R. Latham; R. Ross; M. E. Papka; J. Chen; V. Pascucci,"Scientific Computing and Imaging Institute, University of Utah, Salt Lake City, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Parallel I/O library performance can vary greatly in response to user-tunable parameter values such as aggregator count, file count, and aggregation strategy. Unfortunately, manual selection of these values is time consuming and dependent on characteristics of the target machine, the underlying file system, and the dataset itself. Some characteristics, such as the amount of memory per core, can also impose hard constraints on the range of viable parameter values. In this work we address these problems by using machine learning techniques to model the performance of the PIDX parallel I/O library and select appropriate tunable parameter values. We characterize both the network and I/O phases of PIDX on a Cray XE6 as well as an IBM Blue Gene/P system. We use the results of this study to develop a machine learning model for parameter space exploration and performance prediction.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503252,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877500,I/O & Network Characterization;Performance Modeling,Adaptation models;Data visualization;Laboratories;Libraries;Memory management;Predictive models;Throughput,input-output programs;learning (artificial intelligence);parallel processing;software libraries;software performance evaluation,Cray XE6;I/O phases;IBM Blue Gene/P system;PIDX parallel I/O characterization;PIDX parallel I/O modeling;aggregation strategy;aggregator count;file count;file system;hard constraints;machine learning techniques;network phases;parallel I/O library performance prediction;parameter space exploration;performance optimization;target machine;user-tunable parameter value selection,,2,,42,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
ACIC: Automatic cloud I/O configurator for HPC applications,M. Liu; Y. Jin; J. Zhai; Y. Zhai; Q. Shi; X. Ma; W. Chen,"Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing, China","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"The cloud has become a promising alternative to traditional HPC centers or in-house clusters. This new environment highlights the I/O bottleneck problem, typically with top-of-the-line compute instances but sub-par communication and I/O facilities. It has been observed that changing cloud I/O system configurations leads to significant variation in the performance and cost efficiency of I/O intensive HPC applications. However, storage system configuration is tedious and error-prone to do manually, even for experts. This paper proposes ACIC, which takes a given application running on a given cloud platform, and automatically searches for optimized I/O system configurations. ACIC utilizes machine learning models to perform black-box performance/cost predictions. To tackle the high-dimensional parameter exploration space unique to cloud platforms, we enable affordable, reusable, and incremental training guided by Plackett and Burman Matrices. Results with four representative applications indicate that ACIC consistently identifies near-optimal configurations among a large group of candidate settings.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503216,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877471,Cloud Computing;Modeling;Performance;Storage,Benchmark testing;Cloud computing;Predictive models;Servers;Space exploration;Training;Training data,cloud computing;input-output programs;learning (artificial intelligence);parallel processing,ACIC;HPC applications;automatic cloud I/O configurator;cloud platform;high-dimensional parameter exploration space;machine learning models;optimized I/O system configurations,,4,,56,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Physics-based seismic hazard analysis on petascale heterogeneous supercomputers,Y. Cui; E. Poyraz; K. B. Olsen; J. Zhou; K. Withers; S. Callaghan; J. Larkin; C. Guest; D. Choi; A. Chourasia; Z. Shi; S. M. Day; P. J. Maechling; T. H. Jordan,"Univ. of California, San Diego, La Jolla, CA, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"We have developed a highly scalable and efficient GPU-based finite-difference code (AWP) for earthquake simulation that implements high throughput, memory locality, communication reduction and communication / computation overlap and achieves linear scalability on Cray XK7 Titan at ORNL and NCSA's Blue Waters system. We simulate realistic 0-10 Hz earthquake ground motions relevant to building engineering design using high-performance AWP. Moreover, we show that AWP provides a speedup by a factor of 110 in key strain tensor calculations critical to probabilistic seismic hazard analysis (PSHA). These performance improvements to critical scientific application software, coupled with improved co-scheduling capabilities of our workflow-managed systems, make a statewide hazard model a goal reachable with existing supercomputers. The performance improvements of GPU-based AWP are expected to save millions of core-hours over the next few years as physics-based seismic hazard analysis is developed using heterogeneous petascale supercomputers.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503300,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877503,CyberShake;GPU;SCEC;earthquake ground motions;hybrid heterogeneous;seismic hazard analysis;weak scaling,Computational modeling;Earthquakes;Graphics processing units;Hazards;Tensile stress;Three-dimensional displays,earthquake engineering;finite difference methods;geophysics computing;graphics processing units;mainframes,AWP;Blue Waters system;Cray XK7 Titan;GPU-based finite-difference code;NCSA;ORNL;PSHA;communication reduction;earthquake simulation;key strain tensor calculations;memory locality;petascale heterogeneous supercomputers;physics-based seismic hazard analysis;probabilistic seismic hazard analysis;statewide hazard model;workflow-managed systems,,3,,34,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Investigating applications portability with the uintah DAG-based runtime system on petascale supercomputers,Q. Meng; A. Humphrey; J. Schmidt; M. Berzins,"Sci. Comput. & Imaging Inst., Univ. of Utah, Salt Lake City, UT, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Present trends in high performance computing present formidable challenges for applications code using multicore nodes possibly with accelerators and/or co-processors and reduced memory while still attaining scalability. Software frameworks that execute machine-independent applications code using a runtime system that shields users from architectural complexities offer a possible solution. The Uintah framework for example, solves a broad class of large-scale problems on structured adaptive grids using fluid-flow solvers coupled with particle-based solids methods. Uintah executes directed acyclic graphs of computational tasks with a scalable asynchronous and dynamic runtime system for CPU cores and/or accelerators/co-processors on a node. Uintah's clear separation between application and runtime code has led to scalability increases of 1000x without significant changes to application code. This methodology is tested on three leading Top500 machines; OLCF Titan, TACC Stampede and ALCF Mira using three diverse and challenging applications problems. This investigation of scalability with regard to the different processors and communications performance leads to the overall conclusion that the adaptive DAG-based approach provides a very powerful abstraction for solving challenging multi-scale multi-physics engineering problems on some of the largest and most powerful computers available today.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877529,Blue Gene/Q;GPU;Uintah;Xeon Phi;adaptive;co-processor;heterogeneous systems;hybrid parallelism;parallel;scalability,Computational modeling;Data warehouses;Graphics processing units;Mathematical model;Runtime;Scalability,coprocessors;directed graphs;mainframes;multiprocessing systems;parallel machines;parallel processing;software portability,ALCF Mira;CPU cores;OLCF Titan;TACC Stampede;Top500 machines;Uintah DAG-based runtime system;application code;application portability;computational tasks;coprocessors;directed acyclic graphs;dynamic runtime system;fluid-flow solvers;high performance computing;machine-independent application code;multicore nodes;multiscale multiphysics engineering problems;particle-based solid methods;petascale supercomputers;runtime system;scalable asynchronous system;software frameworks;structured adaptive grids,,0,,65,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
AUGEM: Automatically generate high performance Dense Linear Algebra kernels on x86 CPUs,Q. Wang; X. Zhang; Y. Zhang; Q. Yi,"Inst. of Software, Beijing, China","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Basic Liner algebra subprograms (BLAS) is a fundamental library in scientific computing. In this paper, we present a template-based optimization framework, AUGEM, which can automatically generate fully optimized assembly code for several dense linear algebra (DLA) kernels, such as GEMM, GEMV, AXPY and DOT, on varying multi-core CPUs without requiring any manual interference from developers. In particular, based on domain-specific knowledge about algorithms of the DLA kernels, we use a collection of parameterized code templates to formulate a number of commonly occurring instruction sequences within the optimized low-level C code of these DLA kernels. Then, our framework uses a specialized low-level C optimizer to identify instruction sequences that match the pre-defined code templates and thereby translates them into extremely efficient SSE/AVX instructions. The DLA kernels generated by our templatebased approach surpass the implementations of Intel MKL and AMD ACML BLAS libraries, on both Intel Sandy Bridge and AMD Piledriver processors.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503219,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877458,DLA code optimization;auto-tuning;code generation,Abstracts;Arrays;Generators;Kernel;Registers;Resource management;Seals,linear algebra;multiprocessing systems;optimising compilers;parallel processing;program assemblers,AMD ACML BLAS libraries;AMD Piledriver processors;AUGEM;AXPY;BLAS;DLA kernels;DOT;GEMM;GEMV;Intel MKL;Intel Sandy Bridge;SSE-AVX instructions;basic liner algebra subprograms;domain-specific knowledge;fully optimized assembly code generation;high performance dense linear algebra kernel automatic generation;instruction sequences;multicore CPUs;parameterized code template collection;scientific computing;specialized low-level C code optimizer;template-based optimization framework;x86 CPUs,,6,,21,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Table of contents,,,"2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,9,The following topics are dealt with: high performance computing; networking; storage management; data analysis; fault tolerant computing; GPU programming; load balancing; MPI; memory hierarchy; memory resilience; parallel performance tools; performance assessments; cloud computing; pattern clustering; inter-node communication; resource management; scheduling; energy management; IO tuning; data movement; sorting; and graph algorithms.,2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877432,,,cloud computing;computer networks;data analysis;fault tolerant computing;graph theory;graphics processing units;input-output programs;matrix algebra;parallel processing;pattern clustering;resource allocation;scheduling;sorting;storage management,GPU programming;IO tuning;MPI;cloud computing;data analysis;data movement;energy management;fault tolerant computing;graph algorithms;graphics processing unit;high performance computing;input-output tuning;inter-node communication;load balancing;memory hierarchy;memory resilience;message passing interface;networking;parallel performance tools;pattern clustering;performance assessments;resource management;scheduling;sorting;storage management,,0,,,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
The origin of mass,P. Boyle; M. I. Buchoff; N. Christ; T. Izubuchi; Chulwoo Jung; T. C. Luu; R. Mawhinney; C. Schroeder; R. Soltz; P. Vranas; J. Wasem,"U. of Edinburgh, Edinburgh, UK","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,10,"The origin of mass is one of the deepest mysteries in science. Neutrons and protons, which account for almost all visible mass in the Universe, emerged from a primordial plasma through a cataclysmic phase transition microseconds after the Big Bang. However, most mass in the Universe is invisible. The existence of dark matter, which interacts with our world so weakly that it is essentially undetectable, has been established from its galactic-scale gravitational effects. Here we describe results from the first truly physical calculations of the cosmic phase transition and a groundbreaking first-principles investigation into composite dark matter, studies impossible with previous state-of-the-art methods and resources. By inventing a powerful new algorithm, ‰ÛÏDSDR,‰Û and implementing it effectively for contemporary supercomputers, we attain excellent strong scaling, perfect weak scaling to the LLNL BlueGene/Q two million cores, sustained speed of 7.2 petaflops, and time-to-solution speedup of more than 200 over the previous state-of-the-art.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2504561,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877437,SC13 proceedings,Lattices;Mesons;Neutrons;Numerical models;Physics;Plasmas;Protons,astrophysical plasma;cosmology;gravitation;mass;neutrons;phase transformations;protons;quantum chromodynamics;quark-gluon plasma,Big Bang;DSDR algorithm;LLNL BlueGene/Q;Universe;cataclysmic phase transition;composite dark matter;cosmic phase transition;first-principles investigation;galactic-scale gravitational effects;mass origin;neutrons;perfect weak scaling;physical calculations;primordial plasma;protons;quantum chromodynamics;quark-gluon plasma;supercomputers;time-to-solution speedup;visible mass,,0,,36,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Channel reservation protocol for over-subscribed channels and destinations,G. Michelogiannakis; N. Jiang; D. Becker; W. J. Dally,"Stanford Univ., Stanford, CA, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Channels in system-wide networks tend to be over-subscribed due to the cost of bandwidth and increasing traffic demands. To make matters worse, workloads can overstress specific destinations, creating hotspots. Lossless networks offer attractive advantages compared to lossy networks but suffer from tree saturation. This led to the development of explicit congestion notification (ECN). However, ECN is very sensitive to its configuration parameters and acts only after congestion forms. We propose channel reservation protocol (CRP) to enable sources to reserve bandwidth in multiple resources in advance of packet transmission and with a single request, but without idling resources like circuit switching. CRP prevents congestion from ever occurring and thus reacts instantly to traffic changes, whereas ECN requires 300,000 cycles to stabilize in our experiments. Furthermore, ECN may not prevent congestion formed by short-lived flows generated by a large combination of source-destination pairs.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503213,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877485,Congestion control;congestion notification;large-scale networks;reservation protocol;tree saturation,Abstracts;Bandwidth;Clocks,computer networks;protocols;telecommunication congestion control;telecommunication switching;telecommunication traffic,CRP;ECN;channel reservation protocol;circuit switching;explicit congestion notification;hotspots;lossless networks;lossy networks;over-subscribed channels;packet transmission;short-lived flows;source-destination pairs;system-wide networks;traffic changes;tree saturation;workloads,,2,,46,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Algorithms for high-throughput disk-to-disk sorting,H. Sundar; D. Malhotra; K. W. Schulz,"Univ. of Texas at Austin, Austin, TX, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,10,"In this paper, we present a new out-of-core sort algorithm, designed for problems that are too large to fit into the aggregate RAM available on modern supercomputers. We analyze the performance including the cost of IO and demonstrate the fastest (to the best of our knowledge) reported throughput using the canonical sortBenchmark on a general-purpose, production HPC resource running Lustre. By clever use of available storage and a formulation of asynchronous data transfer mechanisms, we are able to almost completely hide the computation (sorting) behind the IO latency. This latency hiding enables us to achieve comparable execution times, including the additional temporary IO required, between a large sort problem (5TB) run as a single, in-RAM sort and our out-of-core approach using 1/10th the amount of RAM. In our largest run, sorting 100TB of records using 1792 hosts, we achieved an end-to-end throughput of 1.24TB/min using our general-purpose sorter, improving on the current Daytona record holder by 65%.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503259,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877526,Out-of-Core Algorithms;Parallel Algorithms;Sorting;asynchronous methods;distributed-memory parallelism;hypercube;quicksort;samplesort;shared-memory parallelism,Abstracts;Algorithm design and analysis;Computer architecture;Radio access networks,parallel algorithms;parallel memories;sorting,5TB;IO latency;asynchronous data transfer mechanisms;canonical sortBenchmark;general-purpose sorter;high-throughput disk-to-disk sorting;in-RAM sort;large sort problem;latency hiding;modern supercomputers;out-of-core sort algorithm;production HPC resource running Lustre,,1,,24,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Practical nonvolatile multilevel-cell phase change memory,D. H. Yoon; J. Chang; R. S. Schreiber; N. P. Jouppi,"IBM T.J. Watson Res. Center, Yorktown Heights, NY, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Multilevel-cell (MLC) phase change memory (PCM) may provide both high capacity main memory and faster-than-Flash persistent storage. But slow growth in cell resistance with time, resistance drift, can cause transient errors in MLC-PCM. Drift errors increase with time, and prior work suggests refresh before the cell loses data. The need for refresh makes MLC-PCM volatile, taking away a key advantage. Based on the observation that most drift errors occur in a particular state in four-level-cell PCM, we propose to change from four levels to three levels, eliminating the most vulnerable state. This simple change lowers cell drift error rates by many orders of magnitude: three-level-cell PCM can retain data without power for more than ten years. With optimized encoding/decoding and a wearout tolerance mechanism, we can narrow the capacity gap between three-level and four-level cells. These techniques together enable low-cost, high-performance, genuinely nonvolatile MLC-PCM.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503221,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877454,Memory;Multilevel Cell;Nonvolatility;Phase Change,Computer architecture;Encoding;Error analysis;Error correction codes;Microprocessors;Phase change materials;Resistance,phase change memories;random-access storage,cell drift error rates;cell resistance;faster-than-flash persistent storage;four-level-cell PCM;high capacity main memory;nonvolatile MLC-PCM;nonvolatile multilevel-cell phase change memory;optimized encoding-decoding;resistance drift;transient errors;volatile MLC PCM;wearout tolerance mechanism,,7,,40,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Deterministic scale-free pipeline parallelism with hyperqueues,H. Vandierendonck; K. Chronaki; D. S. Nikolopoulos,"Queen's Univ. Belfast, Belfast, UK","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Ubiquitous parallel computing aims to make parallel programming accessible to a wide variety of programming areas using deterministic and scale-free programming models built on a task abstraction. However, it remains hard to reconcile these attributes with pipeline parallelism, where the number of pipeline stages is typically hard-coded in the program and defines the degree of parallelism. This paper introduces hyperqueues, a programming abstraction that enables the construction of deterministic and scale-free pipeline parallel programs. Hyperqueues extend the concept of Cilk++ hyperobjects to provide thread-local views on a shared data structure. While hyperobjects are organized around private local views, hyperqueues require shared concurrent views on the underlying data structure. We define the semantics of hyperqueues and describe their implementation in a work-stealing scheduler. We demonstrate scalable performance on pipeline-parallel PARSEC benchmarks and find that hyperqueues provide comparable or up to 30% better performance than POSIX threads and Intel's Threading Building Blocks. The latter are highly tuned to the number of available processing cores, while programs using hyperqueues are scale-free.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503233,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877465,,Computational modeling;Data structures;Parallel processing;Pipelines;Programming;Runtime;Synchronization,Unix;data structures;deterministic algorithms;multi-threading;parallel programming;pipeline processing;program diagnostics;queueing theory;ubiquitous computing,Cilk++ hyperobjects;Intel Threading Building Blocks;POSIX threads;data structure;deterministic models;deterministic scale-free pipeline parallelism;hyperqueues;parallel programming;pipeline-parallel PARSEC benchmarks;private local views;programming abstraction;scale-free pipeline parallel programs;scale-free programming models;shared data structure;task abstraction;thread-local views;ubiquitous parallel computing;work-stealing scheduler,,0,,35,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Scalable virtual machine deployment using VM image caches,K. Razavi; T. Kielmann,"Dept. of Comput. Sci., VU Univ. Amsterdam, Amsterdam, Netherlands","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"In IaaS clouds, VM startup times are frequently perceived as slow, negatively impacting both dynamic scaling of web applications and the startup of high-performance computing applications consisting of many VM nodes. A significant part of the startup time is due to the large transfers of VM image content from a storage node to the actual compute nodes, even when copy-on-write schemes are used. We have observed that only a tiny part of the VM image is needed for the VM to be able to start up. Based on this observation, we propose using small caches for VM images to overcome the VM startup bottlenecks. We have implemented such caches as an extension to KVM/QEMU. Our evaluation with up to 64 VMs shows that using our caches reduces the time needed for simultaneous VM startups to the one of a single VM.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503274,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877498,Infrastructure-as-a-Service;Scalability,Booting;Delays;Linux;Scalability;Servers;Virtual machining,cache storage;cloud computing;parallel processing;virtual machines,IaaS clouds;KVM;QEMU;VM image caches;VM nodes;VM startup times;copy-on-write schemes;dynamic Web application scaling;high-performance computing;scalable virtual machine deployment,,7,,28,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
A new routing scheme for jellyfish and its performance with HPC workloads,X. Yuan; S. Mahapatra; W. Nienaber; S. Pakin; M. Lang,"Dept. of Comput. Sci., Florida State Univ., Tallahassee, FL, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,11,"The jellyfish topology where switches are connected using a random graph has recently been proposed for large scale data-center networks. It has been shown to offer higher bisection bandwidth and better permutation throughput than the corresponding fat-tree topology with a similar cost. In this work, we propose a new routing scheme for jellyfish that out-performs existing schemes by more effectively exploiting the path diversity, and comprehensively compare the performance of jellyfish and fat-tree topologies with HPC workloads. The results indicate that both jellyfish and fat-tree topologies offer comparable high performance for HPC workloads on systems that can be realized by 3-level fat-trees using the current technology and the corresponding jellyfish topologies with similar costs. Fat-trees are more effective for smaller systems while jellyfish is more scalable.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503229,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877469,HPC workload;Interconnects;fat-tree;jellyfish;performance,Abstracts;Laboratories;Logic gates;Measurement;Switches;Throughput,computer centres;computer networks;parallel processing;telecommunication network routing;telecommunication network topology;trees (mathematics),3-level fat-trees;HPC workloads;bisection bandwidth;fat-tree topology;jellyfish routing scheme;jellyfish topology;large scale data-center networks;path diversity;permutation throughput;random graph;switches,,4,,15,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Exploiting application dynamism and cloud elasticity for continuous dataflows,A. Kumbhare; Y. Simmhan; V. K. Prasanna,"Univ. of Southern California, Los Angeles, CA, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Contemporary continuous data flow systems use elastic scaling on distributed cloud resources to handle variable data rates and to meet applications' needs while attempting to maximize resource utilization. However, virtualized clouds present an added challenge due to the variability in resource performance - over time and space - thereby impacting the application's QoS. Elastic use of cloud resources and their allocation to continuous dataflow tasks need to adapt to such infrastructure dynamism. In this paper, we develop the concept of ‰ÛÏdynamic dataflows‰Û as an extension to continuous dataflows that utilizes alternate tasks and allows additional control over the dataflow's cost and QoS. We formalize an optimization problem to perform both deployment and runtime cloud resource management for such dataflows, and define an objective function that allows trade-off between the application's value against resource cost. We present two novel heuristics, local and global, based on the variable sized bin packing heuristics to solve this NP-hard problem. We evaluate the heuristics against a static allocation policy for a dataflow with different data rate profiles that is simulated using VM performance traces from a private cloud data center. The results show that the heuristics are effective in intelligently utilizing cloud elasticity to mitigate the effect of both input data rate and cloud resource performance variabilities on QoS.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503240,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877490,Dataflows;clouds;data velocity;optimization;resource management;runtime adaptation,Cloud computing;Data models;Optimization;Ports (Computers);Quality of service;Runtime;Throughput,bin packing;cloud computing;computer centres;optimisation;quality of service;resource allocation;virtual machines;virtualisation,NP-hard problem;VM performance;application QoS;application dynamism;cloud elasticity;cloud resource performance variabilities;continuous dataflow systems;data rate profiles;distributed cloud resources;dynamic dataflows;elastic scaling;infrastructure dynamism;optimization problem;private cloud data center;resource utilization;run-time cloud resource management;static allocation policy;variable data rates;variable sized bin packing heuristics;virtualized clouds,,9,,48,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
SIDR: Structure-aware intelligent data routing in hadoop,J. Buck; N. Watkins; G. Levin; A. Crume; K. Ioannidou; S. Brandt; C. Maltzahn; N. Polyzotis; A. Torres,"Dept. of Comput. Sci., Univ. of California-Santa Cruz, Santa Cruz, CA, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"The MapReduce framework is being extended for domains quite different from the web applications for which it was designed, including the processing of big structured data, e.g., scientific and financial data. Previous work using MapReduce to process scientific data ignores existing structure when assigning intermediate data and scheduling tasks. In this paper, we present a method for incorporating knowledge of the structure of scientific data and executing query into the MapReduce communication model. Built in SciHadoop, a version of the Hadoop MapReduce framework for scientific data, SIDR intelligently partitions and routes intermediate data, allowing it to: remove Hadoop's global barrier and execute Reduce tasks prior to all Map tasks completing; minimize intermediate key skew; and produce early, correct results. SIDR executes queries up to 2.5 times faster than Hadoop and 37% faster than SciHadoop; produces initial results with only 6% of the query completed; and produces dense, contiguous output.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503241,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877506,Hadoop;MapReduce;Scientific Data,Abstracts;Shape,Internet;distributed processing;scheduling;scientific information systems,MapReduce framework;SIDR;SciHadoop;Web applications;big structured data;financial data;intermediate data;intermediate key skew;scheduling tasks;scientific data;structure-aware intelligent data routing,,1,,39,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Taming parallel I/O complexity with auto-tuning,B. Behzad; H. V. T. Luu; J. Huchette; S. Byna; Prabhat; R. Aydt; Q. Koziol; M. Snir,"Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"We present an auto-tuning system for optimizing I/O performance of HDF5 applications and demonstrate its value across platforms, applications, and at scale. The system uses a genetic algorithm to search a large space of tunable parameters and to identify effective settings at all layers of the parallel I/O stack. The parameter settings are applied transparently by the auto-tuning system via dynamically intercepted HDF5 calls. To validate our auto-tuning system, we applied it to three I/O benchmarks (VPIC, VORPAL, and GCRM) that replicate the I/O activity of their respective applications. We tested the system with different weak-scaling configurations (128, 2048, and 4096 CPU cores) that generate 30 GB to 1 TB of data, and executed these configurations on diverse HPC platforms (Cray XE6, IBM BG/P, and Dell Cluster). In all cases, the auto-tuning framework identified tunable parameters that substantially improved write performance over default system settings. We consistently demonstrate I/O write speedups between 2ÌÑ and 100ÌÑ for test configurations.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503278,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877501,Auto-Tuning;Parallel I/O;Parallel file systems;Performance Optimization,Abstracts;Buffer storage;Choppers (circuits);Kernel;Laboratories;Sociology;Statistics,computational complexity;genetic algorithms;input-output programs;parallel processing;program testing,HDF5 applications;IO write speedups;auto-tuning system;default system settings;diverse HPC platforms;dynamically intercepted HDF5 calls;genetic algorithm;parallel IO complexity;parallel IO stack;test configurations;tunable parameters;weak-scaling configurations,,13,,35,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
COCA: Online distributed resource management for cost minimization and carbon neutrality in data centers,S. Ren; Y. He,"Florida Int. Univ., Miami, FL, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Due to the enormous energy consumption and associated environmental concerns, data centers have been increasingly pressured to reduce long-term net carbon footprint to zero, i.e., carbon neutrality. In this paper, we propose an online algorithm, called COCA (optimizing for COst minimization and CArbon neutrality), for minimizing data center operational cost while satisfying carbon neutrality without long-term future information. Unlike the existing research, COCA enables distributed server-level resource management: each server autonomously adjusts its processing speed and optimally decides the amount of workloads to process. We prove that COCA achieves a close-to-minimum operational cost (incorporating both electricity and delay costs) compared to the optimal algorithm with future information, while bounding the potential violation of carbon neutrality. We also perform trace-based simulation studies to complement the analysis, and the results show that COCA reduces cost by more than 25% (compared to state of the art) while resulting in a smaller carbon footprint.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503248,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877472,Carbon neutrality;Data center;Load distribution;Resource management;Scheduling;Stochastic control,Abstracts;Carbon;Computational modeling;Resource management;Servers,computer centres;cost reduction;distributed algorithms;green computing;minimisation;power aware computing,COCA cost reduction;cost minimization and carbon neutrality;data center operational cost minimization;data centers;distributed server-level resource management;energy consumption;long-term net carbon footprint reduction;online COCA algorithm;online distributed resource management;trace-based simulation,,7,,36,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Introduction,W. Gropp; S. Matsuoka,"University of Illinois at Urbana-Champaign, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,2,"HPC is everywhere - everyday. That's the tag line of this year's conference on high performance computing, networking, storage, and analysis (SC), and this volume, containing the accepted technical papers and Gordon Bell prize finalists, captures the best current research in all aspects of HPC.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877433,,,,,,0,,,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
An early performance evaluation of many integrated core architecture based sgi rackable computing system,S. Saini; H. Jin; D. Jespersen; H. Feng; J. Djomehri; W. Arasin; R. Hood; P. Mehrotra; R. Biswas,"NASA Ames Res. Center, Moffett Field, CA, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Intel recently introduced the Xeon Phi coprocessor based on the Many Integrated Core architecture featuring 60 cores with a peak performance of 1.0 Tflop/s. NASA has deployed a 128-node SGI Rackable system where each node has two Intel Xeon E2670 8-core Sandy Bridge processors along with two Xeon Phi 5110P coprocessors. We have conducted an early performance evaluation of the Xeon Phi. We used microbenchmarks to measure the latency and bandwidth of memory and interconnect, I/O rates, and the performance of OpenMP directives and MPI functions. We also used OpenMP and MPI versions of the NAS Parallel Benchmarks along with two production CFD applications to test four programming modes: offload, processor native, coprocessor native and symmetric (processor plus coprocessor). In this paper we present preliminary results based on our performance evaluation of various aspects of a Phi-based system.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503272,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877527,CFD applications;Intel MIC architecture;Intel Sandy Bridge processor;Intel Xeon Phi;benchmarking;performance evaluation,Bandwidth;Benchmark testing;Bridges;Coprocessors;Program processors;Programming,application program interfaces;computer architecture;coprocessors;message passing;performance evaluation;shared memory systems,CFD applications;I/O rates;Intel Xeon E2670 8-core Sandy Bridge processors;MPI functions;NAS parallel benchmarks;OpenMP directive performance;SGI rackable computing system;Xeon Phi 5110P coprocessors;many integrated core architecture;memory bandwidth;performance evaluation,,6,,33,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Feng Shui of supercomputer memory positional effects in DRAM and SRAM faults,V. Sridharan; J. Stearley; N. DeBardeleben; S. Blanchard; S. Gurumurthi,"Adv. Micro Devices, Inc., Boxborough, MA, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,11,"Several recent publications confirm that faults are common in high-performance computing systems. Therefore, further attention to the faults experienced by such computing systems is warranted. In this paper, we present a study of DRAM and SRAM faults in large high-performance computing systems. Our goal is to understand the factors that influence faults in production settings. We examine the impact of aging on DRAM, finding a marked shift from permanent to transient faults in the first two years of DRAM lifetime. We examine the impact of DRAM vendor, finding that fault rates vary by more than 4x among vendors. We examine the physical location of faults in a DRAM device and in a data center; contrary to prior studies, we find no correlations with either. Finally, we study the impact of altitude and rack placement on SRAM faults, finding that, as expected, altitude has a substantial impact on SRAM faults, and that top of rack placement correlates with 20% higher fault rate.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877455,,Abstracts;Energy measurement;Laboratories;Random access memory;Terminology;Transient analysis,DRAM chips;SRAM chips;fault diagnosis;mainframes;parallel machines;parallel processing,DRAM device;DRAM faults;DRAM lifetime;DRAM vendor;Feng Shui;SRAM faults;data center;high-performance computing systems;physical fault location;positional effects;supercomputer memory;transient faults,,1,,26,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
A ‰Û÷cool‰Ûª way of improving the reliability of HPC machines,O. Sarood; E. Meneses; L. V. Kale,"Dept. of Comput. Sci., Univ. of Illinois at Urbana-Champaign, Champaign, IL, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"Soaring energy consumption, accompanied by declining reliability, together loom as the biggest hurdles for the next generation of supercomputers. Recent reports have expressed concern that reliability at exascale level could degrade to the point where failures become a norm rather than an exception. HPC researchers are focusing on improving existing fault tolerance protocols to address these concerns. Research on improving hardware reliability, i.e., machine component reliability, has also been making progress independently. In this paper, we try to bridge this gap and explore the potential of combining both software and hardware aspects towards improving reliability of HPC machines. Fault rates are known to double for every 10å¡C rise in core temperature. We leverage this notion to experimentally demonstrate the potential of restraining core temperatures and load balancing to achieve two-fold benefits: improving reliability of parallel machines and reducing total execution time required by applications. Our experimental results show that we can improve the reliability of a machine by a factor of 2.3 and reduce the execution time by 12%. In addition, our scheme can also reduce machine energy consumption by as much as 25%. For a 350K socket machine, regular checkpoint/restart fails to make progress (less than 1% efficiency), whereas our validated model predicts an efficiency of 20% by improving the machine reliability by a factor of up to 2.29.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503228,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877491,Actionable modeling;Checkpointing restart;Energy minimization;Fault tolerance;Load balancing;Temperature capping;Temperature thresholds;Thermal control,Fault tolerance;Fault tolerant systems;Load management;Process control;Temperature control;Temperature distribution,energy consumption;fault tolerant computing;multiprocessing systems;parallel machines;parallel processing;power aware computing;protocols;reliability;resource allocation,HPC machine reliability;core temperatures;exascale level;fault rates;fault tolerance protocols;hardware reliability;load balancing;machine component reliability;machine energy consumption reduction;parallel machines;regular checkpoint-restart;soaring energy consumption;socket machine;supercomputers,,6,,39,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Using cross-layer adaptations for dynamic data management in large scale coupled scientific workflows,T. Jin; F. Zhang; Q. Sun; H. Bui; M. Parashar; H. Yu; S. Klasky; N. Podhorszki; H. Abbasi,"Rutgers Discovery Inf. Inst., Rutgers Univ., Piscataway, NJ, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"As system scales and application complexity grow, managing and processing simulation data has become a significant challenge. While recent approaches based on data staging and in-situ/in-transit data processing are promising, dynamic data volumes and distributions,such as those occurring in AMR-based simulations, make the efficient use of these techniques challenging. In this paper we propose cross-layer adaptations that address these challenges and respond at runtime to dynamic data management requirements. Specifically we explore (1) adaptations of the spatial resolution at which the data is processed, (2) dynamic placement and scheduling of data processing kernels, and (3) dynamic allocation of in-transit resources. We also exploit co-ordinated approaches that dynamically combine these adaptations at the different layers. We evaluate the performance of our adaptive cross-layer management approach on the Intrepid IBM-BlueGene/P and Titan Cray-XK7 systems using Chombo-based AMR applications, and demonstrate its effectiveness in improving overall time-to-solution and increasing resource efficiency.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503301,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877507,Cross-layer adaptation;coupled simulation workflows;data management;in-situ/in-transit;staging,Adaptation models;Analytical models;Data models;Data processing;Dynamic scheduling;Middleware;Runtime,computational complexity;data analysis;performance evaluation;scientific information systems,AMR-based simulations;Chombo-based AMR applications;IBM-BlueGene-P system;Titan Cray-XK7 system;adaptive cross-layer management approach;application complexity;cross-layer adaptations;data processing kernels;data staging;dynamic data distributions;dynamic data management;dynamic data volumes;dynamic in-transit resource allocation;in-transit data processing;large scale coupled scientific workflows;simulation data management;simulation data processing;system scales,,7,,27,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Supercomputing with commodity CPUs: Are mobile SoCs ready for HPC?,N. Rajovic; P. M. Carpenter; I. Gelado; N. Puzovic; A. Ramirez; M. Valero,"Barcelona Supercomput. Center, Barcelona, Spain","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"In the late 1990s, powerful economic forces led to the adoption of commodity desktop processors in high-performance computing. This transformation has been so effective that the June 2013 TOP500 list is still dominated by x86. In 2013, the largest commodity market in computing is not PCs or servers, but mobile computing, comprising smartphones and tablets, most of which are built with ARM-based SoCs. This leads to the suggestion that once mobile SoCs deliver sufficient performance, mobile SoCs can help reduce the cost of HPC. This paper addresses this question in detail. We analyze the trend in mobile SoC performance, comparing it with the similar trend in the 1990s. We also present our experience evaluating performance and efficiency of mobile SoCs, deploying a cluster and evaluating the network and scalability of production applications. In summary, we give a first answer as to whether mobile SoCs are ready for HPC.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503281,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877473,,Graphics processing units;Microprocessors;Mobile communication;Servers;System-on-chip;Vectors,mobile computing;multiprocessing systems;parallel processing;system-on-chip,ARM-based SoCs;HPC;commodity CPUs;commodity desktop processors;cost reduction;high-performance computing;mobile SoC performance;mobile computing;supercomputing,,21,,46,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Taking a quantum leap in time to solution for simulations of high-Tc superconductors,P. Staar; T. A. Maier; R. Solca; G. Fourestey; M. S. Summers; T. C. Schulthess,"ITP, ETH Zurich, Zurich, Switzerland","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,11,"We present a new quantum cluster algorithm to simulate models of high-Tc superconductors. This algorithm extends current methods with continuous lattice self-energies, thereby removing artificial long-range correlations. This cures the fermionic sign problem in the underlying quantum Monte Carlo solver for large clusters and realistic values of the Coulomb interaction in the entire temperature range of interest. We find that the new algorithm improves time-to-solution by nine orders of magnitude compared to current, state of the art quantum cluster simulations. An efficient implementation is given, which ports to multi-core as well as hybrid CPU-GPU systems. Running on 18,600 nodes on ORNL's Titan supercomputer enables us to compute a converged value of T<sub>c</sub>/t = 0.053å±0.0014 for a 28 site cluster in the 2D Hubbard model with U/t = 7 at 10% hole doping. Typical simulations on Titan sustain between 9.2 and 15.4 petaflops (double precision measured over full run), depending on configuration and parameters used.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503282,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877434,,Abstracts;Art;Gold;Subspace constraints,Hubbard model;Monte Carlo methods;fermion systems;high-temperature superconductors;materials science computing;multiprocessing systems,2D Hubbard model;Coulomb interaction;ORNL Titan supercomputer;artificial long-range correlations;continuous lattice self-energies;fermionic sign problem;high-Tc superconductor simulations;hole doping;hybrid CPU-GPU systems;petaflops;quantum Monte Carlo solver;quantum cluster algorithm;quantum cluster simulations;quantum leap;time-to-solution,,0,,30,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Exploring DRAM organizations for energy-efficient and resilient exascale memories,B. Giridhar; M. Cieslak; D. Duggal; R. Dreslinski; Hsing Min Chen; R. Patti; B. Hold; C. Chakrabarti; T. Mudge; D. Blaauw,"Univ. of Michigan, Ann Arbor, MI, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,12,"The power target for exascale supercomputing is 20MW, with about 30% budgeted for the memory subsystem. Commodity DRAMs will not satisfy this requirement. Additionally, the large number of memory chips (>10M) required will result in crippling failure rates. Although specialized DRAM memories have been reorganized to reduce power through 3D-stacking or row buffer resizing, their implications on fault tolerance have not been considered. We show that addressing reliability and energy is a co-optimization problem involving tradeoffs between error correction cost, access energy and refresh power-reducing the physical page size to decrease access energy increases the energy/area overhead of error resilience. Additionally, power can be reduced by optimizing bitline lengths. The proposed 3D-stacked memory uses a page size of 4kb and consumes 5.1pJ/bit based on simulations with NEK5000 benchmarks. Scaling to 100PB, the memory consumes 4.7MW at 100PB/s which, while well within the total power budget (20MW), is also error-resilient.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503215,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877456,,Abstracts;Bandwidth;Error correction codes;Pins;Random access memory;Three-dimensional displays;Through-silicon vias,DRAM chips;buffer storage;error correction;fault tolerant computing;mainframes;parallel machines;power aware computing;system recovery,3D-stacked memory;3D-stacking;DRAM memories;DRAM organizations;NEK5000 benchmarks;access energy;bitline lengths;commodity DRAM;cooptimization problem;crippling failure rates;energy-efficient resilient exascale memories;error correction cost;error resilience;exascale supercomputing;fault tolerance;memory chips;memory subsystem;power 20 MW;power 4.7 MW;row buffer resizing,,14,,45,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
Exploring the future of out-of-core computing with compute-local non-volatile memory,M. Jung; E. H. Wilson; W. Choi; J. Shalf; H. M. Aktulga; C. Yang; E. Saule; U. V. Catalyurek; M. Kandemir,"Dept. of Electr. Eng., Univ. of Texas at Dallas, Dallas, TX, USA","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,11,"Drawing parallels to the rise of general purpose graphical processing units (GPGPUs) as accelerators for specific high-performance computing (HPC) workloads, there is a rise in the use of non-volatile memory (NVM) as accelerators for I/O-intensive scientific applications. However, existing works have explored use of NVM within dedicated I/O nodes, which are distant from the compute nodes that actually need such acceleration. As NVM bandwidth begins to out-pace point-to-point network capacity, we argue for the need to break from the archetype of completely separated storage. Therefore, in this work we investigate co-location of NVM and compute by varying I/O interfaces, file systems, types of NVM, and both current and future SSD architectures, uncovering numerous bottlenecks implicit in these various levels in the I/O stack. We present novel hardware and software solutions, including the new Unified File System (UFS), to enable fuller utilization of the new compute-local NVM storage. Our experimental evaluation, which employs a real-world Out-of-Core (OoC) HPC application, demonstrates throughput increases in excess of an order of magnitude over current approaches.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2503261,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877508,,Abstracts;Computer architecture;Industries;Nonvolatile memory;Phase change materials;Prototypes;Random access memory,graphics processing units;multiprocessing systems;parallel processing;random-access storage;storage management,GPGPUs;HPC workloads;I/O interfaces;I/O-intensive scientific applications;NVM bandwidth;OoC HPC application;SSD architectures;UFS;accelerators;compute nodes;compute-local NVM storage;compute-local nonvolatile memory;dedicated I/O nodes;file systems;general purpose graphical processing units;high-performance computing;out-of-core computing;out-pace point-to-point network capacity;real-world out-of-core HPC application;unified file system,,2,,48,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications
20 Petaflops simulation of proteins suspensions in crowding conditions,M. Bernaschi; M. Bisson; M. Fatica; S. Melchionna,"Ist. Applicazioni Calcolo, Consiglio Naz. delle Ric., Rome, Italy","2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)",20140814,2013,,,1,11,"We present performance results for the simulation of proteins suspensions in crowding conditions obtained with MUPHY, a computational platform for multi-scale simulations of real-life biofluidic problems. Previous versions of MU-PHY have been used in the past for the simulation of blood flow through the human coronary arteries and DNA translocation across nanopores. The simulation exhibits excellent scalability up to 18, 000 K20X Nvidia GPUs and achieves almost 20 Petaflops of aggregate sustained performance with a peak performance of 27.5 Petaflops for the most intensive computing component. Those figures demonstrate once again the flexibility of MUPHY in simulating biofluidic phenomena, exploiting at their best the features of the architecture in use. Preliminary results were obtained in the present case on a completely different platform, the IBM Blue Gene/Q. The combination of novel mathematical models, computational algorithms, hardware technology, code tuning and parallelization techniques required to achieve these results are presented.",2167-4329;21674329,Electronic:978-1-4503-2378-9; POD:978-1-4799-3520-8,10.1145/2503210.2504563,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877435,,Biological system modeling;Computational modeling;Graphics processing units;Hydrodynamics;Proteins;Solvents,DNA;biology computing;graphics processing units;haemodynamics;parallel processing;proteins,DNA translocation;IBM Blue Gene/Q;K20X Nvidia GPUs;MUPHY;aggregate sustained performance;blood flow simulation;code parallelization techniques;code tuning techniques;computational algorithms;computational platform;crowding conditions;hardware technology;human coronary arteries;mathematical models;multiscale simulations;nanopores;petaflops simulation;protein suspensions;real-life biofluidic problems,,0,,25,,,17-22 Nov. 2013,,IEEE,IEEE Conference Publications